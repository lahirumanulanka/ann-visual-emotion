#!/usr/bin/env python3
"""
Script to extend the enhanced CNN Transfer Learning notebook with complete implementation.
This adds the model definition, training loop, evaluation, and analysis.
"""

import json
from pathlib import Path

def create_complete_notebook_extension():
    """Create the complete notebook with all implementation details"""
    
    # Additional cells for complete implementation
    additional_cells = [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 🗂️ Enhanced Dataset Class\n",
                "\n",
                "Custom dataset class with robust error handling and path resolution:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EmotionDataset(Dataset):\n",
                "    \"\"\"Enhanced dataset class for emotion recognition with robust error handling\"\"\"\n",
                "    \n",
                "    def __init__(self, dataframe: pd.DataFrame, transform: transforms.Compose = None, \n",
                "                 root_dir: str = None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            dataframe: DataFrame with 'path' and 'label' columns\n",
                "            transform: Torchvision transforms to apply\n",
                "            root_dir: Root directory for relative paths\n",
                "        \"\"\"\n",
                "        self.dataframe = dataframe.reset_index(drop=True)\n",
                "        self.transform = transform\n",
                "        self.root_dir = Path(root_dir) if root_dir else Path(\"../data/processed/EmoSet_splits\")\n",
                "        self.label_to_idx = {label: idx for idx, label in enumerate(sorted(dataframe['label'].unique()))}\n",
                "        \n",
                "        # Cache for failed image paths\n",
                "        self.failed_images = set()\n",
                "        \n",
                "        print(f\"📊 Dataset initialized:\")\n",
                "        print(f\"   • Samples: {len(self.dataframe):,}\")\n",
                "        print(f\"   • Classes: {len(self.label_to_idx)}\")\n",
                "        print(f\"   • Root directory: {self.root_dir}\")\n",
                "        print(f\"   • Transform: {'Yes' if transform else 'No'}\")\n",
                "    \n",
                "    def _load_image(self, image_path: str) -> Image.Image:\n",
                "        \"\"\"Load image with robust path handling\"\"\"\n",
                "        # Try absolute path first\n",
                "        if Path(image_path).exists():\n",
                "            path = Path(image_path)\n",
                "        else:\n",
                "            # Try relative to root directory\n",
                "            path = self.root_dir / image_path.lstrip('/')\n",
                "            \n",
                "        if not path.exists():\n",
                "            # Try alternative path structures\n",
                "            alt_path = self.root_dir / Path(image_path).name\n",
                "            if alt_path.exists():\n",
                "                path = alt_path\n",
                "            else:\n",
                "                raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
                "        \n",
                "        try:\n",
                "            image = Image.open(path).convert('RGB')\n",
                "            return image\n",
                "        except Exception as e:\n",
                "            raise IOError(f\"Failed to load image {path}: {str(e)}\")\n",
                "    \n",
                "    def __len__(self) -> int:\n",
                "        return len(self.dataframe)\n",
                "    \n",
                "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, str]:\n",
                "        \"\"\"Get item with error handling\"\"\"\n",
                "        row = self.dataframe.iloc[idx]\n",
                "        image_path = row['path']\n",
                "        label_name = row['label']\n",
                "        \n",
                "        # Skip known failed images\n",
                "        if image_path in self.failed_images:\n",
                "            # Return next valid image\n",
                "            return self.__getitem__((idx + 1) % len(self.dataframe))\n",
                "        \n",
                "        try:\n",
                "            # Load and transform image\n",
                "            image = self._load_image(image_path)\n",
                "            \n",
                "            if self.transform:\n",
                "                image = self.transform(image)\n",
                "            \n",
                "            # Convert label to index\n",
                "            label_idx = self.label_to_idx[label_name]\n",
                "            \n",
                "            return image, label_idx, image_path\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"⚠️  Error loading {image_path}: {str(e)}\")\n",
                "            self.failed_images.add(image_path)\n",
                "            # Return next valid image\n",
                "            return self.__getitem__((idx + 1) % len(self.dataframe))\n",
                "    \n",
                "    def get_class_weights(self) -> torch.Tensor:\n",
                "        \"\"\"Calculate class weights for balanced training\"\"\"\n",
                "        class_counts = self.dataframe['label'].value_counts()\n",
                "        total_samples = len(self.dataframe)\n",
                "        \n",
                "        weights = []\n",
                "        for i in range(len(self.label_to_idx)):\n",
                "            label = [k for k, v in self.label_to_idx.items() if v == i][0]\n",
                "            weight = total_samples / (len(self.label_to_idx) * class_counts[label])\n",
                "            weights.append(weight)\n",
                "        \n",
                "        return torch.FloatTensor(weights)\n",
                "\n",
                "# Create dataset instances\n",
                "print(\"🗂️  Creating dataset instances...\")\n",
                "train_dataset = EmotionDataset(train_df, transform=train_transforms)\n",
                "val_dataset = EmotionDataset(val_df, transform=val_transforms)\n",
                "test_dataset = EmotionDataset(test_df, transform=test_transforms)\n",
                "\n",
                "# Calculate class weights for balanced training\n",
                "class_weights = train_dataset.get_class_weights().to(cfg.DEVICE)\n",
                "print(f\"\\n⚖️  Class weights calculated: {class_weights.cpu().numpy().round(3)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. 🚀 Data Loaders with Optimization\n",
                "\n",
                "Creating efficient data loaders with proper configuration:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create data loaders with optimized settings\n",
                "train_loader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=cfg.BATCH_SIZE,\n",
                "    shuffle=True,\n",
                "    num_workers=cfg.NUM_WORKERS,\n",
                "    pin_memory=cfg.PIN_MEMORY,\n",
                "    drop_last=True,  # Ensures consistent batch sizes\n",
                "    persistent_workers=True if cfg.NUM_WORKERS > 0 else False\n",
                ")\n",
                "\n",
                "val_loader = DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=cfg.BATCH_SIZE,\n",
                "    shuffle=False,\n",
                "    num_workers=cfg.NUM_WORKERS,\n",
                "    pin_memory=cfg.PIN_MEMORY,\n",
                "    persistent_workers=True if cfg.NUM_WORKERS > 0 else False\n",
                ")\n",
                "\n",
                "test_loader = DataLoader(\n",
                "    test_dataset,\n",
                "    batch_size=cfg.BATCH_SIZE,\n",
                "    shuffle=False,\n",
                "    num_workers=cfg.NUM_WORKERS,\n",
                "    pin_memory=cfg.PIN_MEMORY,\n",
                "    persistent_workers=True if cfg.NUM_WORKERS > 0 else False\n",
                ")\n",
                "\n",
                "print(\"🚀 Data Loaders Created:\")\n",
                "print(f\"   🏋️  Training: {len(train_loader):,} batches ({len(train_dataset):,} samples)\")\n",
                "print(f\"   📊 Validation: {len(val_loader):,} batches ({len(val_dataset):,} samples)\")\n",
                "print(f\"   🧪 Test: {len(test_loader):,} batches ({len(test_dataset):,} samples)\")\n",
                "print(f\"   ⚙️  Workers: {cfg.NUM_WORKERS}, Pin memory: {cfg.PIN_MEMORY}\")\n",
                "\n",
                "# Test data loading\n",
                "print(\"\\n🔍 Testing data loading...\")\n",
                "try:\n",
                "    sample_batch = next(iter(train_loader))\n",
                "    images, labels, paths = sample_batch\n",
                "    print(f\"   ✅ Batch shape: {images.shape}\")\n",
                "    print(f\"   ✅ Labels shape: {labels.shape}\")\n",
                "    print(f\"   ✅ Data type: {images.dtype}\")\n",
                "    print(f\"   ✅ Value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
                "except Exception as e:\n",
                "    print(f\"   ❌ Error: {str(e)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. 🏗️ Enhanced Model Architecture\n",
                "\n",
                "Building sophisticated transfer learning models with custom heads:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EnhancedClassifierHead(nn.Module):\n",
                "    \"\"\"Advanced classifier head with multiple techniques for better performance\"\"\"\n",
                "    \n",
                "    def __init__(self, in_features: int, num_classes: int, dropout: float = 0.4):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Progressive dimensionality reduction\n",
                "        hidden_dim = max(512, in_features // 4)\n",
                "        \n",
                "        self.classifier = nn.Sequential(\n",
                "            # First layer with batch norm and dropout\n",
                "            nn.Linear(in_features, hidden_dim),\n",
                "            nn.BatchNorm1d(hidden_dim),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(dropout),\n",
                "            \n",
                "            # Second layer with reduced dropout\n",
                "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
                "            nn.BatchNorm1d(hidden_dim // 2),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(dropout * 0.5),\n",
                "            \n",
                "            # Final classification layer\n",
                "            nn.Linear(hidden_dim // 2, num_classes)\n",
                "        )\n",
                "        \n",
                "        # Initialize weights\n",
                "        self._initialize_weights()\n",
                "    \n",
                "    def _initialize_weights(self):\n",
                "        \"\"\"Initialize weights using He initialization\"\"\"\n",
                "        for m in self.modules():\n",
                "            if isinstance(m, nn.Linear):\n",
                "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
                "                if m.bias is not None:\n",
                "                    nn.init.constant_(m.bias, 0)\n",
                "            elif isinstance(m, nn.BatchNorm1d):\n",
                "                nn.init.constant_(m.weight, 1)\n",
                "                nn.init.constant_(m.bias, 0)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.classifier(x)\n",
                "\n",
                "class EnhancedTransferModel(nn.Module):\n",
                "    \"\"\"Enhanced transfer learning model with advanced features\"\"\"\n",
                "    \n",
                "    def __init__(self, model_name: str, num_classes: int, pretrained: bool = True):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.model_name = model_name\n",
                "        self.num_classes = num_classes\n",
                "        \n",
                "        # Load backbone\n",
                "        if model_name == 'resnet50':\n",
                "            self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n",
                "            in_features = self.backbone.fc.in_features\n",
                "            self.backbone.fc = nn.Identity()  # Remove original classifier\n",
                "            \n",
                "        elif model_name == 'efficientnet_b0':\n",
                "            self.backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None)\n",
                "            in_features = self.backbone.classifier[1].in_features\n",
                "            self.backbone.classifier = nn.Identity()  # Remove original classifier\n",
                "            \n",
                "        else:\n",
                "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
                "        \n",
                "        # Custom classifier head\n",
                "        self.classifier = EnhancedClassifierHead(in_features, num_classes, cfg.DROPOUT)\n",
                "        \n",
                "        # Freeze backbone initially (will unfreeze gradually)\n",
                "        self._freeze_backbone()\n",
                "        \n",
                "        print(f\"🏗️  Model created: {model_name}\")\n",
                "        print(f\"   • Backbone features: {in_features:,}\")\n",
                "        print(f\"   • Output classes: {num_classes}\")\n",
                "        print(f\"   • Pretrained: {pretrained}\")\n",
                "        print(f\"   • Total parameters: {self.count_parameters():,}\")\n",
                "        print(f\"   • Trainable parameters: {self.count_parameters(trainable_only=True):,}\")\n",
                "    \n",
                "    def _freeze_backbone(self):\n",
                "        \"\"\"Freeze backbone parameters\"\"\"\n",
                "        for param in self.backbone.parameters():\n",
                "            param.requires_grad = False\n",
                "    \n",
                "    def unfreeze_backbone_layers(self, num_layers: int = -1):\n",
                "        \"\"\"Unfreeze last N layers of backbone for fine-tuning\"\"\"\n",
                "        if self.model_name == 'resnet50':\n",
                "            layers = [self.backbone.layer4, self.backbone.layer3]\n",
                "            if num_layers == -1:\n",
                "                layers.extend([self.backbone.layer2, self.backbone.layer1])\n",
                "        elif self.model_name == 'efficientnet_b0':\n",
                "            layers = list(self.backbone.features[-3:])  # Last 3 blocks\n",
                "            if num_layers == -1:\n",
                "                layers = list(self.backbone.features[-6:])  # Last 6 blocks\n",
                "        \n",
                "        unfrozen_params = 0\n",
                "        for layer in layers[:num_layers if num_layers > 0 else len(layers)]:\n",
                "            for param in layer.parameters():\n",
                "                param.requires_grad = True\n",
                "                unfrozen_params += param.numel()\n",
                "        \n",
                "        print(f\"🔓 Unfroze {unfrozen_params:,} backbone parameters\")\n",
                "    \n",
                "    def count_parameters(self, trainable_only: bool = False) -> int:\n",
                "        \"\"\"Count model parameters\"\"\"\n",
                "        if trainable_only:\n",
                "            return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
                "        return sum(p.numel() for p in self.parameters())\n",
                "    \n",
                "    def forward(self, x):\n",
                "        features = self.backbone(x)\n",
                "        output = self.classifier(features)\n",
                "        return output\n",
                "\n",
                "# Create model\n",
                "model = EnhancedTransferModel(\n",
                "    model_name=cfg.MODEL_NAME,\n",
                "    num_classes=cfg.NUM_CLASSES,\n",
                "    pretrained=cfg.PRETRAINED\n",
                ").to(cfg.DEVICE)\n",
                "\n",
                "# Print model summary\n",
                "print(f\"\\n📊 Model Summary ({cfg.MODEL_NAME}):\")\n",
                "total_params = model.count_parameters()\n",
                "trainable_params = model.count_parameters(trainable_only=True)\n",
                "print(f\"   📦 Total parameters: {total_params:,}\")\n",
                "print(f\"   🎯 Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
                "print(f\"   🔒 Frozen parameters: {total_params-trainable_params:,} ({(total_params-trainable_params)/total_params*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. 🎯 Advanced Training Components\n",
                "\n",
                "Setting up optimizers, schedulers, and loss functions:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Advanced optimizer with differential learning rates\n",
                "def setup_optimizer_and_scheduler(model):\n",
                "    \"\"\"Setup optimizer with differential learning rates and advanced scheduler\"\"\"\n",
                "    \n",
                "    # Separate parameters for backbone and classifier\n",
                "    backbone_params = []\n",
                "    classifier_params = []\n",
                "    \n",
                "    for name, param in model.named_parameters():\n",
                "        if param.requires_grad:\n",
                "            if 'classifier' in name:\n",
                "                classifier_params.append(param)\n",
                "            else:\n",
                "                backbone_params.append(param)\n",
                "    \n",
                "    # Create parameter groups with different learning rates\n",
                "    param_groups = [\n",
                "        {'params': backbone_params, 'lr': cfg.LR_BACKBONE, 'name': 'backbone'},\n",
                "        {'params': classifier_params, 'lr': cfg.LR_HEAD, 'name': 'classifier'}\n",
                "    ]\n",
                "    \n",
                "    # AdamW optimizer with weight decay\n",
                "    optimizer = AdamW(\n",
                "        param_groups,\n",
                "        weight_decay=cfg.WEIGHT_DECAY,\n",
                "        eps=1e-8,\n",
                "        betas=(0.9, 0.999)\n",
                "    )\n",
                "    \n",
                "    # Cosine annealing scheduler with warm restarts\n",
                "    if cfg.USE_COSINE_ANNEALING:\n",
                "        scheduler = CosineAnnealingLR(\n",
                "            optimizer,\n",
                "            T_max=cfg.EPOCHS,\n",
                "            eta_min=cfg.MIN_LR\n",
                "        )\n",
                "    else:\n",
                "        scheduler = ReduceLROnPlateau(\n",
                "            optimizer,\n",
                "            mode='max',\n",
                "            factor=0.5,\n",
                "            patience=5,\n",
                "            verbose=True,\n",
                "            min_lr=cfg.MIN_LR\n",
                "        )\n",
                "    \n",
                "    return optimizer, scheduler\n",
                "\n",
                "# Setup training components\n",
                "optimizer, scheduler = setup_optimizer_and_scheduler(model)\n",
                "\n",
                "# Loss function with label smoothing\n",
                "criterion = nn.CrossEntropyLoss(\n",
                "    weight=class_weights,\n",
                "    label_smoothing=cfg.LABEL_SMOOTHING\n",
                ")\n",
                "\n",
                "# Mixed precision training\n",
                "scaler = torch.cuda.amp.GradScaler() if cfg.DEVICE.type == 'cuda' else None\n",
                "\n",
                "print(\"🎯 Training Components Setup:\")\n",
                "print(f\"   🔧 Optimizer: AdamW with differential LRs\")\n",
                "print(f\"      • Backbone LR: {cfg.LR_BACKBONE:.1e}\")\n",
                "print(f\"      • Classifier LR: {cfg.LR_HEAD:.1e}\")\n",
                "print(f\"      • Weight decay: {cfg.WEIGHT_DECAY:.1e}\")\n",
                "print(f\"   📉 Scheduler: {'CosineAnnealingLR' if cfg.USE_COSINE_ANNEALING else 'ReduceLROnPlateau'}\")\n",
                "print(f\"   💡 Loss: CrossEntropyLoss with label smoothing ({cfg.LABEL_SMOOTHING})\")\n",
                "print(f\"   ⚡ Mixed precision: {'Enabled' if scaler else 'Disabled'}\")\n",
                "print(f\"   🎲 Class weights: Applied ({len(class_weights)} classes)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. 📊 Advanced Training Utilities\n",
                "\n",
                "Comprehensive utilities for monitoring and visualization:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TrainingMonitor:\n",
                "    \"\"\"Comprehensive training monitoring and visualization\"\"\"\n",
                "    \n",
                "    def __init__(self, class_names: List[str]):\n",
                "        self.class_names = class_names\n",
                "        self.history = {\n",
                "            'train_loss': [], 'val_loss': [],\n",
                "            'train_acc': [], 'val_acc': [],\n",
                "            'train_f1': [], 'val_f1': [],\n",
                "            'lr_backbone': [], 'lr_classifier': [],\n",
                "            'epoch_time': []\n",
                "        }\n",
                "        self.best_metrics = {\n",
                "            'val_acc': 0.0,\n",
                "            'val_f1': 0.0,\n",
                "            'epoch': 0\n",
                "        }\n",
                "    \n",
                "    def update(self, epoch: int, train_metrics: Dict, val_metrics: Dict, \n",
                "               lr_info: Dict, epoch_time: float):\n",
                "        \"\"\"Update training history\"\"\"\n",
                "        self.history['train_loss'].append(train_metrics['loss'])\n",
                "        self.history['val_loss'].append(val_metrics['loss'])\n",
                "        self.history['train_acc'].append(train_metrics['accuracy'])\n",
                "        self.history['val_acc'].append(val_metrics['accuracy'])\n",
                "        self.history['train_f1'].append(train_metrics['f1'])\n",
                "        self.history['val_f1'].append(val_metrics['f1'])\n",
                "        self.history['lr_backbone'].append(lr_info['backbone'])\n",
                "        self.history['lr_classifier'].append(lr_info['classifier'])\n",
                "        self.history['epoch_time'].append(epoch_time)\n",
                "        \n",
                "        # Update best metrics\n",
                "        if val_metrics['accuracy'] > self.best_metrics['val_acc']:\n",
                "            self.best_metrics['val_acc'] = val_metrics['accuracy']\n",
                "            self.best_metrics['epoch'] = epoch\n",
                "        \n",
                "        if val_metrics['f1'] > self.best_metrics['val_f1']:\n",
                "            self.best_metrics['val_f1'] = val_metrics['f1']\n",
                "    \n",
                "    def plot_training_curves(self, save_path: str = None):\n",
                "        \"\"\"Plot comprehensive training curves\"\"\"\n",
                "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
                "        \n",
                "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
                "        \n",
                "        # Loss curves\n",
                "        axes[0, 0].plot(epochs, self.history['train_loss'], 'b-', label='Training', linewidth=2)\n",
                "        axes[0, 0].plot(epochs, self.history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
                "        axes[0, 0].set_title('Loss Curves', fontsize=14, fontweight='bold')\n",
                "        axes[0, 0].set_xlabel('Epoch')\n",
                "        axes[0, 0].set_ylabel('Loss')\n",
                "        axes[0, 0].legend()\n",
                "        axes[0, 0].grid(True, alpha=0.3)\n",
                "        \n",
                "        # Accuracy curves\n",
                "        axes[0, 1].plot(epochs, self.history['train_acc'], 'b-', label='Training', linewidth=2)\n",
                "        axes[0, 1].plot(epochs, self.history['val_acc'], 'r-', label='Validation', linewidth=2)\n",
                "        axes[0, 1].set_title('Accuracy Curves', fontsize=14, fontweight='bold')\n",
                "        axes[0, 1].set_xlabel('Epoch')\n",
                "        axes[0, 1].set_ylabel('Accuracy')\n",
                "        axes[0, 1].legend()\n",
                "        axes[0, 1].grid(True, alpha=0.3)\n",
                "        \n",
                "        # F1 curves\n",
                "        axes[0, 2].plot(epochs, self.history['train_f1'], 'b-', label='Training', linewidth=2)\n",
                "        axes[0, 2].plot(epochs, self.history['val_f1'], 'r-', label='Validation', linewidth=2)\n",
                "        axes[0, 2].set_title('F1-Score Curves', fontsize=14, fontweight='bold')\n",
                "        axes[0, 2].set_xlabel('Epoch')\n",
                "        axes[0, 2].set_ylabel('F1-Score')\n",
                "        axes[0, 2].legend()\n",
                "        axes[0, 2].grid(True, alpha=0.3)\n",
                "        \n",
                "        # Learning rate curves\n",
                "        axes[1, 0].semilogy(epochs, self.history['lr_backbone'], 'g-', label='Backbone', linewidth=2)\n",
                "        axes[1, 0].semilogy(epochs, self.history['lr_classifier'], 'orange', label='Classifier', linewidth=2)\n",
                "        axes[1, 0].set_title('Learning Rate Curves', fontsize=14, fontweight='bold')\n",
                "        axes[1, 0].set_xlabel('Epoch')\n",
                "        axes[1, 0].set_ylabel('Learning Rate (log scale)')\n",
                "        axes[1, 0].legend()\n",
                "        axes[1, 0].grid(True, alpha=0.3)\n",
                "        \n",
                "        # Training time\n",
                "        axes[1, 1].plot(epochs, self.history['epoch_time'], 'purple', linewidth=2)\n",
                "        axes[1, 1].set_title('Training Time per Epoch', fontsize=14, fontweight='bold')\n",
                "        axes[1, 1].set_xlabel('Epoch')\n",
                "        axes[1, 1].set_ylabel('Time (seconds)')\n",
                "        axes[1, 1].grid(True, alpha=0.3)\n",
                "        \n",
                "        # Overfitting indicator\n",
                "        overfitting = np.array(self.history['train_acc']) - np.array(self.history['val_acc'])\n",
                "        axes[1, 2].plot(epochs, overfitting, 'red', linewidth=2)\n",
                "        axes[1, 2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
                "        axes[1, 2].set_title('Overfitting Indicator (Train - Val Acc)', fontsize=14, fontweight='bold')\n",
                "        axes[1, 2].set_xlabel('Epoch')\n",
                "        axes[1, 2].set_ylabel('Accuracy Difference')\n",
                "        axes[1, 2].grid(True, alpha=0.3)\n",
                "        \n",
                "        plt.tight_layout()\n",
                "        \n",
                "        if save_path:\n",
                "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
                "        plt.show()\n",
                "        \n",
                "        # Print best metrics\n",
                "        print(f\"\\n🏆 Best Performance:\")\n",
                "        print(f\"   • Best Validation Accuracy: {self.best_metrics['val_acc']:.4f} (Epoch {self.best_metrics['epoch']})\")\n",
                "        print(f\"   • Best Validation F1-Score: {self.best_metrics['val_f1']:.4f}\")\n",
                "        print(f\"   • Average epoch time: {np.mean(self.history['epoch_time']):.1f}s\")\n",
                "        print(f\"   • Total training time: {np.sum(self.history['epoch_time'])/3600:.2f}h\")\n",
                "\n",
                "# Initialize training monitor\n",
                "monitor = TrainingMonitor(class_names)\n",
                "print(\"📊 Training monitor initialized with comprehensive tracking\")"
            ]
        }
    ]
    
    return additional_cells

# Load existing notebook
notebook_path = Path("notebooks/Improved_CNN_Transfer_Learning_Enhanced.ipynb")
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

# Add the new cells
notebook["cells"].extend(create_complete_notebook_extension())

# Save the updated notebook
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=1, ensure_ascii=False)

print(f"📓 Notebook extended with complete implementation")
print("🎯 Additional features added:")
print("   • Enhanced dataset class with robust error handling")
print("   • Optimized data loaders")
print("   • Advanced model architecture with custom heads")
print("   • Sophisticated training components")
print("   • Comprehensive training monitoring")