<div align="center">

# ANN Visual Emotion Recognition

End-to-end computer vision pipeline for multi-class human emotion recognition from images: data acquisition & curation (EmoSet + YOLO emotion style sources), preprocessing (face detection, cropping, grayscale / color conversions), dataset splitting & balancing, model training with CNN transfer learning (PyTorch / ResNet50), model explainability (SHAP / LIME foundations), export to ONNX, realtime inference API (FastAPI), and multi-platform client applications (Flutter mobile + lightweight web demo + realtime local app). 

</div>

---

## 🔍 High-Level Overview

This repository brings together every stage of the ML lifecycle:

| Stage | Components |
|-------|------------|
| Data Ingestion & Cleaning | `data/raw`, `scripts/regenerate_emoset_splits.py`, `merge_yolo_emotion.py` |
| Preprocessing / Transformation | `crop_faces.py`, `grayscale_dataset.py`, `split_train_val.py`, `update_emotion_labels.py` |
| Dataset Management | Processed splits in `data/processed/EmoSet_splits/` (CSV + label maps + stats) |
| Exploration | Notebooks `01_eda.ipynb`, `02_feature_engineering_balancing.ipynb` |
| Synthetic Data / Augmentation | `03_synthetic_gen_ai_generation.ipynb` (GenAI generation placeholder) |
| Modeling | `CNN_with_Transfer_Learning.ipynb`, PyTorch ResNet50 fine-tuning (checkpoint in `models/best_model.pth`) |
| Serving | `realtime_app/` (FastAPI inference service) |
| Deployment Targets | ONNX export (`scripts/export_onnx.py`) + Flutter app (`app/emotion_detector/`) + simple web demo (`realtime_app/templates/index.html`) |
| Explainability | Planned via SHAP / LIME (dependencies included) – doc stubs in `docs/xai_comparison.md` |
| Experiment Tracking | MLflow directory `experiments/mlruns/` (structure present) |
| Configuration | `pyproject.toml`, (placeholder Hydra configs under `configs/`) |
| Automation | DVC pipeline stub `dvc.yaml` (ready for future data/model tracking) |

---

## 📁 Repository Structure (Curated)

```
ann-visual-emotion/
├── app/emotion_detector/        # Flutter mobile application (multi entrypoints in lib/)
│   ├── assets/                  # Bundled ONNX model + label map for on-device inference
│   └── lib/                     # Dart sources (main.dart variants for experimentation)
├── configs/                     # (Placeholders) for Hydra: data/model/train/eval configs
├── data/
│   ├── raw/                     # Original datasets (EmoSet, FullDataEmoSet, etc.)
│   └── processed/               # Generated splits (EmoSet_splits with CSV + stats)
├── deliverables/                # Presentation / packaging / media artifacts
├── docker/                      # Dockerfiles for training + API (`Dockerfile.train`, `Dockerfile.api`)
├── docs/                        # EDA, model design, genAI, XAI documentation (some WIP)
├── experiments/                 # Experiment outputs; includes timestamped runs + MLflow
├── models/                      # Trained artifacts (best_model.pth, label_map.json, model.onnx)
├── mobile_exports/              # Export target for ONNX when running export script
├── notebooks/                   # Jupyter notebooks for EDA, balancing, synthetic data, training
├── realtime_app/                # FastAPI realtime inference service (face detection + prediction)
├── scripts/                     # Data engineering, preprocessing, export & maintenance scripts
├── src/                         # (Current) genai prompt space (empty placeholder)
├── templates/                   # (Global) HTML assets if needed
├── webapp/                      # Placeholder for a separate web client (package.json present)
├── pyproject.toml               # Python project definition & dependencies
├── dvc.yaml                     # (Stub) for future DVC stages
└── README.md
```

---

## 🧪 Emotion Classes

Current active model (`models/label_map.json`):

```
angry (0), fearful (1), happy (2), neutral (3), sad (4), surprised (5)
```

Historical / richer dataset (EDA) tracked 8 classes: Amusement, Anger, Awe, Contentment, Disgust, Excitement, Fear, Sadness. The production model distilled / remapped to 6 final classes (e.g. excitement→happy / amusement→happy, disgust removed due to severe sparsity, awe/contentment merged into neutral or positive class). Mapping decisions can be formalized in `docs/model_design.md` (TODO).

---

## 📊 Dataset & EDA Summary

Primary processed split: `data/processed/EmoSet_splits/` generated by `scripts/regenerate_emoset_splits.py`.

Key points (from `docs/eda.md`):
* Severe class imbalance (minority Disgust ~27 images originally; dropped / merged downstream).
* Mitigation strategies considered: class-weighted loss, oversampling, synthetic generation (see `03_synthetic_gen_ai_generation.ipynb`).
* Standard preprocessing: resize 224×224, normalization (ImageNet mean/std), augmentation (flip, rotate, color jitter) when training.

Supplemental transformation scripts:
* Face-centric crops: `crop_faces.py` (largest face, optional grayscale, stats + label_map output)
* Grayscale conversion (two variants): `grayscale_dataset.py` & legacy `split_dataset.py`
* Stratified splitting utilities: `split_train_val.py`, `regenerate_emoset_splits.py`
* Label harmonization: `update_emotion_labels.py`
* Merging external YOLO-style datasets: `merge_yolo_emotion.py`

---

## 🧬 Notebooks (Workflow Narrative)

| Notebook | Purpose |
|----------|---------|
| `01_eda.ipynb` | Exploratory data analysis: distribution plots, imbalance visualization, sanity checks |
| `02_feature_engineering_balancing.ipynb` | Experimenting with resampling, augmentation, potential embeddings |
| `03_synthetic_gen_ai_generation.ipynb` | Prototype for generating synthetic faces/emotions via GenAI (WIP) |
| `CNN_with_Transfer_Learning.ipynb` | Core training & fine-tuning of ResNet-based classifier (metrics, export) |

Note: Summaries show cells present but unexecuted in repo snapshot—executed outputs may have been cleared before commit.

---

## 🧠 Model Architecture & Training

* Backbone: `torchvision.models.resnet50` with pretrained ImageNet weights (`ResNet50_Weights.IMAGENET1K_V2`).
* Head: Replaced final FC layer with `Linear(in_features, num_classes)`.
* Loss: (Inferred) CrossEntropy – (Consider adding class weighting if re-training).
* Checkpoint: `models/best_model.pth` (robust loader handles various internal formats).
* ONNX Export: `scripts/export_onnx.py` (dynamic batch axis, opset configurable, default 17).
* Inference normalization pipeline matches training (Resize 224, ToTensor, Normalize ImageNet stats) – defined in `realtime_app/model.py`.
* Explainability: Dependencies (`shap`, `lime`) included; comparison doc placeholder `docs/xai_comparison.md`.

Potential future improvements:
* Replace ResNet50 with more parameter-efficient backbones (EfficientNetV2, ConvNeXt-Tiny) for mobile.
* Distillation to lightweight student model for Flutter / Web (quantization + INT8 calibration).
* Expand label taxonomy if acquiring more balanced data.

---

## 🚀 Inference & Serving

### FastAPI Realtime Service (`realtime_app/`)
Endpoints:
* `GET /health` – liveness check
* `POST /predict` – body: `{image_base64: str, detect_face: bool}` returns top label, ranked probabilities, optional face bounding box.
* `GET /` – serves a minimal HTML demo (`realtime_app/templates/index.html`).

Pipeline inside `predict`:
1. Decode Base64 → PIL Image.
2. (Optional) Haar cascade face detection (`realtime_app/face.py`).
3. Crop (with slight expansion) → normalization → ResNet forward.
4. Softmax ranking → JSON response.

Singleton pattern caches model after first load (`realtime_app/model.py::get_model`).

### Docker
* `docker/Dockerfile.api` – build minimal FastAPI inference image.
* `docker/Dockerfile.train` – environment for reproducible training (PyTorch, augmentation libs). (Details to be documented)

### Flutter Mobile App (`app/emotion_detector/`)
Contains multiple experimental entrypoints (`lib/main_*.dart`) indicating iterative development (e.g. ONNX vs simpler prototypes). Assets include `model.onnx` + `label_map.json`. Intended to run on-device inference via a Dart/FFI or platform plugin for ONNX Runtime (exact integration code not yet inspected—future README section can be expanded once inference wrapper is added).

### Web / Other
* `webapp/` placeholder Package.json suggests potential separate frontend build (no sources yet).

---

## 🛠️ Scripts Cheat-Sheet

| Script | Function |
|--------|----------|
| `regenerate_emoset_splits.py` | Build stratified train/val/test splits with label map + stats |
| `update_emotion_labels.py` | Rename / reconcile class labels across CSV + directory structure |
| `crop_faces.py` | Detect & crop faces, optional grayscale, produce metadata + stats |
| `grayscale_dataset.py` | Convert dataset color mode (GRAY/RGB) recursively |
| `split_train_val.py` | Simple two-way train/val split for class-folder dataset |
| `merge_yolo_emotion.py` | Merge YOLO-format annotation splits & reorganize into class folders |
| `export_onnx.py` | Convert PyTorch checkpoint → ONNX for mobile/web deployment |
| `benchmark_api.py` | Placeholder for API performance testing |
| `split_dataset.py` | (Legacy / alternate grayscale utility) |
| `prepare_data.py` | (Placeholder – hook for DVC or composite preprocessing pipeline) |

---

## 📦 Dependencies

Defined in `pyproject.toml`:
* Core: torch, torchvision, numpy, pandas, scikit-learn, opencv-python, pillow
* Serving: fastapi, uvicorn, pydantic
* Explainability: shap, lime
* Experiment Tracking: mlflow
* Augmentation: albumentations
* Config: hydra-core (configs pending population)
* Optional: google-cloud-storage (artifact/data sync)

Tooling: `ruff` (lint/format), `pytest` (tests), DVC stub.

---

## ▶️ Quickstart

### 1. Environment
```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -e .[gcs]  # add [gcs] if you need GCS uploads
```

### 2. Run Realtime API
```bash
uvicorn realtime_app.main:app --reload --port 8000
```
Open: http://localhost:8000 (web demo) or POST `/predict` with a base64 image.

### 3. Export ONNX (optional)
```bash
python scripts/export_onnx.py --checkpoint models/best_model.pth --output mobile_exports/model.onnx
```

### 4. (Future) Train / Re-Train
Populate `configs/model.yaml`, `configs/train.yaml`, then implement a `train.py` (not yet committed). DVC + Hydra can orchestrate reproducibility. For now training logic lives primarily in notebooks.

---

## ✅ Testing & Linting

Tasks (VS Code tasks provided):
```bash
ruff check .        # Lint
ruff format .       # Format
pytest -q           # Run tests (currently limited test coverage)
```

---

## 📐 Design Decisions (Highlights)
* Adopted ResNet50 for a balance of accuracy and familiarity; future: lighter architecture.
* Consolidated sparse emotion classes to stabilize training and improve per-class reliability.
* Face detection is opportunistic (Haar) – fast & CPU-friendly. Could be upgraded to RetinaFace / MediaPipe.
* Modular preprocessing scripts enable iterative dataset refinement without locking into a single pipeline.
* ONNX export ensures a single canonical inference graph across API + mobile.

---

## 🔮 Roadmap / TODO
| Area | Next Steps |
|------|------------|
| Configs | Fill `configs/*.yaml` with Hydra-ready schemas (data, model, train, eval) |
| Training Script | Add `src/train.py` with CLI + MLflow logging + checkpoint mgmt |
| XAI | Populate `docs/xai_comparison.md` with SHAP vs LIME visual examples on misclassified samples |
| GenAI | Integrate synthetic image generation & selection heuristics (quality filtering) |
| Mobile | Document ONNX Runtime integration layer in Flutter (plugin / FFI) |
| Benchmark | Implement `scripts/benchmark_api.py` (latency, throughput, warm start vs cold) |
| Evaluation | Add confusion matrix, per-class precision/recall in `docs/model_design.md` |
| Automation | Flesh out `dvc.yaml` stages (prepare -> train -> eval -> export) |
| Testing | Create unit tests for preprocessing scripts + API response schema |
| Security | Add input validation & size limits on `/predict` | 

---

## 📄 Licensing & Attribution
* License: See `LICENSE` (project root).
* Datasets: Ensure original EmoSet / YOLO-style emotion dataset licenses permit redistribution (not bundled here beyond derived stats and structure).

---

## 🙌 Acknowledgements
* PyTorch & TorchVision maintainers.
* OpenCV Haar cascade resources.
* SHAP / LIME authors for explainability tooling.
* Community datasets enabling emotion recognition research.

---

## 🗨️ Questions / Contributions
Issues & pull requests welcome. Please open an issue describing enhancements (model compression, new classes, improved detectors) or bugs (checkpoint load edge cases, API failure modes).


