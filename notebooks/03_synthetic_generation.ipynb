{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b perfect_model https://github.com/lahirumanulanka/ann-visual-emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28cb46",
   "metadata": {},
   "source": [
    "# Synthetic Emotion Dataset Expansion (Original ≈80,190 → Target 100,000)\n",
    "\n",
    "This notebook generates additional face emotion images using a Hugging Face diffusion model to reach a balanced 100k sample dataset of 224×224 grayscale faces. It:\n",
    "\n",
    "1. Loads existing dataset structure and counts.\n",
    "2. Plans required per-class synthetic counts.\n",
    "3. Generates realistic face images conditioned by emotion prompts.\n",
    "4. Filters for face presence, quality, and near-duplicate removal.\n",
    "5. Converts to grayscale 224×224 and stores under `data/processed/FullDataEmoSet_gen/<label>/`.\n",
    "6. Merges original + synthetic metadata, balances, and performs stratified splits with synthetic fraction caps.\n",
    "7. Writes `train.csv`, `val.csv`, `test.csv`, `status.json` (extended), and synthetic metadata logs.\n",
    "8. Produces diagnostics, plots, and reproducibility artifacts.\n",
    "\n",
    "Safety / Ethics: These are synthetic faces (not tied to real users). Use only for model training & research. Avoid unintended misuse. Adjust prompts to maintain demographic diversity and neutrality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Config & Dependencies\n",
    "import sys, os, math, json, time, hashlib, platform, shutil, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Core generation target\n",
    "TARGET_TOTAL = 100_000\n",
    "IMG_SIZE = (224, 224)  # (width, height)\n",
    "GRAYSCALE_MODE = 'L'\n",
    "STRICT_ENFORCE_OUTPUT = True  # Extra post-processing pass to guarantee all images are 224x224 grayscale\n",
    "\n",
    "# Roots (relative to this notebook)\n",
    "RAW_ROOT = Path('../data/raw/FullDataEmoSet').resolve()\n",
    "ORIG_SPLITS_DIR = Path('../data/processed/EmoSet_splits').resolve()  # existing splits (if needed)\n",
    "OUT_GEN_ROOT = Path('../data/processed/FullDataEmoSet_gen').resolve()  # synthetic images per class\n",
    "CSV_DIR = Path('../data/processed/EmoSet_splits_gen').resolve()       # new combined splits output\n",
    "META_DIR = CSV_DIR / 'meta'\n",
    "DEST_IMG_ROOT = CSV_DIR / 'raw_all'  # final curated (original + synthetic) copy for training reference\n",
    "\n",
    "# Controls\n",
    "RUN_WRITE = True  # master switch for any filesystem writes\n",
    "MAX_PER_CLASS_SYNTH_FRACTION = 0.60  # cap synthetic fraction per class in TRAIN split\n",
    "MAX_WORKERS = 1  # placeholder if future parallelization is added\n",
    "BATCH_GENERATION = 1  # diffusion batch size (1 keeps memory manageable)\n",
    "MODEL_ID = 'runwayml/stable-diffusion-v1-5'\n",
    "INFERENCE_STEPS = 30\n",
    "GUIDANCE_SCALE = 7.5\n",
    "BASE_SEED = 20250924\n",
    "DEVICE = 'cuda' if (os.environ.get('FORCE_CPU','0')!='1' and __import__('torch').cuda.is_available()) else 'cpu'\n",
    "ALLOW_HARDLINK = True  # attempt os.link instead of copy to save space (POSIX)\n",
    "\n",
    "# Quality thresholds\n",
    "FACE_DETECT_SINGLE_ONLY = True\n",
    "BLUR_VAR_MIN = 60.0  # Laplacian variance threshold\n",
    "HASH_DIST_MAX = 4    # max allowed Hamming distance for near-duplicate rejection (pHash)\n",
    "PHASH_SIZE = 16      # phash size (affects sensitivity)\n",
    "PHASH_SAMPLE_RESIZE = (128,128)\n",
    "RETRY_DEFICIT_MAX_PASSES = 2\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_PCT, VAL_PCT, TEST_PCT = 0.7, 0.15, 0.15\n",
    "assert abs(TRAIN_PCT + VAL_PCT + TEST_PCT - 1.0) < 1e-6\n",
    "\n",
    "# Create dirs when writing\n",
    "if RUN_WRITE:\n",
    "    for d in [OUT_GEN_ROOT, CSV_DIR, META_DIR, DEST_IMG_ROOT]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\\nRAW_ROOT={RAW_ROOT}\\nOUT_GEN_ROOT={OUT_GEN_ROOT}\\nCSV_DIR={CSV_DIR}\")\n",
    "\n",
    "# Install-time note (libraries expected via pyproject; add here if missing)\n",
    "try:\n",
    "    import torch, numpy as np, pandas as pd, cv2, matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    import imagehash\n",
    "    from tqdm import tqdm\n",
    "    from diffusers import StableDiffusionPipeline\n",
    "except Exception as e:\n",
    "    print('If import fails, ensure diffusers, accelerate, safetensors, opencv-python, imagehash installed.')\n",
    "    raise\n",
    "\n",
    "random.seed(BASE_SEED)\n",
    "np.random.seed(BASE_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed(BASE_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bace4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Existing Dataset Metadata\n",
    "IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}\n",
    "assert RAW_ROOT.exists(), f\"RAW_ROOT missing: {RAW_ROOT}\"\n",
    "\n",
    "class_dirs = [p for p in RAW_ROOT.iterdir() if p.is_dir()]\n",
    "labels = sorted([p.name for p in class_dirs])\n",
    "print('Labels:', labels)\n",
    "\n",
    "records = []\n",
    "for lab in labels:\n",
    "    for p in (RAW_ROOT / lab).rglob('*'):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            records.append({'path': str(p.resolve()), 'label': lab, 'origin': 'original'})\n",
    "\n",
    "df_original = pd.DataFrame(records)\n",
    "class_counts_original = df_original['label'].value_counts().sort_index()\n",
    "total_original = int(class_counts_original.sum())\n",
    "print('Original per-class counts:\\n', class_counts_original)\n",
    "print('Total original:', total_original)\n",
    "\n",
    "# Optional assertion (can relax if dataset changed)\n",
    "if total_original != 80190:\n",
    "    print(f\"[WARN] Expected 80190 originals, found {total_original}. Proceeding anyway.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute Generation Plan (Target 100,000)\n",
    "remaining_needed = max(0, TARGET_TOTAL - len(df_original))\n",
    "print(f\"Synthetic images needed to reach target: {remaining_needed}\")\n",
    "\n",
    "num_classes = len(labels)\n",
    "base_target_per_class = math.ceil(TARGET_TOTAL / num_classes)\n",
    "\n",
    "plan_rows = []\n",
    "needed_per_class: Dict[str,int] = {}\n",
    "for lab in labels:\n",
    "    current = int(class_counts_original.get(lab, 0))\n",
    "    target = max(current, base_target_per_class)\n",
    "    need = max(0, target - current)\n",
    "    needed_per_class[lab] = need\n",
    "    plan_rows.append({\n",
    "        'label': lab,\n",
    "        'current': current,\n",
    "        'per_class_target': target,\n",
    "        'needed': need,\n",
    "    })\n",
    "\n",
    "plan_df = pd.DataFrame(plan_rows).sort_values('label')\n",
    "print(plan_df)\n",
    "print('Planned synthetic total (sum needed):', int(plan_df['needed'].sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prompt Templates & Emotion-to-Prompt Mapping\n",
    "emotion_prompts = {\n",
    "    'happy': \"portrait photo of a person smiling, expressive happy face, natural skin texture, neutral background, well lit\",\n",
    "    'sad': \"portrait photo of a person with a sad expression, subtle frown, gentle lighting, neutral background\",\n",
    "    'angry': \"portrait photo of a person showing anger, tense eyebrows, intense gaze, dramatic lighting, neutral background\",\n",
    "    'fear': \"portrait photo of a person showing fear, widened eyes, slightly open mouth, cinematic soft lighting, neutral background\",\n",
    "    'surprise': \"portrait photo of a person surprised, raised eyebrows, open mouth, sharp focus, neutral background\",\n",
    "    'disgust': \"portrait photo of a person showing disgust, wrinkled nose, expressive face, soft studio light, neutral background\",\n",
    "    'neutral': \"portrait photo of a person with a neutral calm expression, even soft lighting, neutral background\",\n",
    "}\n",
    "\n",
    "# Optional stylistic suffixes to encourage diversity\n",
    "diversity_suffixes = [\n",
    "    \"ultra detailed, photorealistic\",\n",
    "    \"soft diffused light\",\n",
    "    \"high detail skin texture\",\n",
    "    \"professional portrait\",\n",
    "    \"dslr, crisp details\",\n",
    "]\n",
    "\n",
    "def build_prompt(label: str, variant_idx: int) -> str:\n",
    "    base = emotion_prompts.get(label, f\"portrait photo of a person showing {label} expression, neutral background\")\n",
    "    suffix = diversity_suffixes[variant_idx % len(diversity_suffixes)]\n",
    "    return base + \", \" + suffix\n",
    "\n",
    "# Hash of prompt dict for reproducibility\n",
    "prompt_dict_hash = hashlib.sha256(json.dumps(emotion_prompts, sort_keys=True).encode()).hexdigest()[:16]\n",
    "print('Prompt dict hash:', prompt_dict_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize Diffusion Pipeline (HuggingFace)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16 if (DEVICE=='cuda') else torch.float32,\n",
    "    safety_checker=None,  # optionally keep None; add safety checker if required\n",
    ")\n",
    "if DEVICE == 'cuda':\n",
    "    pipe = pipe.to('cuda')\n",
    "    pipe.enable_attention_slicing()\n",
    "else:\n",
    "    pipe = pipe.to('cpu')\n",
    "\n",
    "print(f\"Loaded model {MODEL_ID} on {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-9. Generation Loop + Grayscale + Face Filter + Dedup\n",
    "\n",
    "# Prepare face detector (Haar cascade)\n",
    "import cv2\n",
    "haar_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(haar_path)\n",
    "assert not face_cascade.empty(), 'Failed to load Haar cascade.'\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "phash_sets: Dict[str, List[imagehash.ImageHash]] = defaultdict(list)\n",
    "\n",
    "synthetic_rows = []\n",
    "start_time = time.time()\n",
    "\n",
    "global_attempt = 0\n",
    "for lab in labels:\n",
    "    need = needed_per_class.get(lab, 0)\n",
    "    if need == 0:\n",
    "        continue\n",
    "    out_dir = OUT_GEN_ROOT / lab\n",
    "    if RUN_WRITE:\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    created = 0\n",
    "    variant_idx = 0\n",
    "    pbar = tqdm(total=need, desc=f'Gen {lab}')\n",
    "    while created < need:\n",
    "        prompt = build_prompt(lab, variant_idx)\n",
    "        seed = BASE_SEED + global_attempt\n",
    "        generator = torch.Generator(device=DEVICE)\n",
    "        generator = generator.manual_seed(seed)\n",
    "        try:\n",
    "            with torch.autocast('cuda', enabled=(DEVICE=='cuda')):\n",
    "                result = pipe(prompt, num_inference_steps=INFERENCE_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator)\n",
    "            img: Image.Image = result.images[0]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] Generation failed {lab}: {e}\")\n",
    "            global_attempt += 1\n",
    "            variant_idx += 1\n",
    "            continue\n",
    "\n",
    "        # Convert to grayscale & resize\n",
    "        img = img.convert('L')\n",
    "        if img.size != IMG_SIZE:\n",
    "            img = img.resize(IMG_SIZE, Image.BILINEAR)\n",
    "\n",
    "        # Face detection\n",
    "        cv_img = np.array(img)\n",
    "        faces = face_cascade.detectMultiScale(cv_img, scaleFactor=1.1, minNeighbors=4, minSize=(40,40))\n",
    "        face_detected = len(faces) > 0\n",
    "        if FACE_DETECT_SINGLE_ONLY and len(faces) != 1:\n",
    "            global_attempt += 1\n",
    "            variant_idx += 1\n",
    "            continue\n",
    "\n",
    "        # Blur metric (Laplacian variance)\n",
    "        lap_var = float(cv2.Laplacian(cv_img, cv2.CV_64F).var())\n",
    "        if lap_var < BLUR_VAR_MIN:\n",
    "            global_attempt += 1\n",
    "            variant_idx += 1\n",
    "            continue\n",
    "\n",
    "        # Dedup via pHash\n",
    "        ph = imagehash.phash(img, hash_size=PHASH_SIZE)\n",
    "        is_dup = False\n",
    "        for existing in phash_sets[lab]:\n",
    "            if (ph - existing) <= HASH_DIST_MAX:\n",
    "                is_dup = True\n",
    "                break\n",
    "        if is_dup:\n",
    "            global_attempt += 1\n",
    "            variant_idx += 1\n",
    "            continue\n",
    "\n",
    "        # Accept image\n",
    "        filename = f\"{lab}_syn_{created:06d}_seed{seed}.png\"\n",
    "        save_path = out_dir / filename\n",
    "        if RUN_WRITE:\n",
    "            try:\n",
    "                img.save(save_path)\n",
    "            except Exception as e:\n",
    "                print('Save failed:', e)\n",
    "                global_attempt += 1\n",
    "                variant_idx += 1\n",
    "                continue\n",
    "\n",
    "        phash_sets[lab].append(ph)\n",
    "        synthetic_rows.append({\n",
    "            'path': str(save_path.resolve()),\n",
    "            'label': lab,\n",
    "            'origin': 'synthetic',\n",
    "            'prompt': prompt,\n",
    "            'seed': seed,\n",
    "            'lap_var': lap_var,\n",
    "            'phash': str(ph),\n",
    "            'attempt': global_attempt,\n",
    "            'face_detected': face_detected,\n",
    "        })\n",
    "        created += 1\n",
    "        pbar.update(1)\n",
    "        global_attempt += 1\n",
    "        variant_idx += 1\n",
    "    pbar.close()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Generation loop elapsed {elapsed/60:.2f} min\")\n",
    "\n",
    "synthetic_meta_df = pd.DataFrame(synthetic_rows)\n",
    "print('Synthetic created per class:\\n', synthetic_meta_df['label'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-12. Metadata Logging + Combine + Adjust\n",
    "if RUN_WRITE and not synthetic_meta_df.empty:\n",
    "    synthetic_meta_df.to_csv(META_DIR / 'synthetic_meta.csv', index=False)\n",
    "    with open(META_DIR / 'synthetic_meta.jsonl', 'w') as f:\n",
    "        for rec in synthetic_rows:\n",
    "            f.write(json.dumps(rec) + '\\n')\n",
    "\n",
    "# Combine\n",
    "if len(synthetic_meta_df):\n",
    "    df_synth = synthetic_meta_df[['path','label','origin']]\n",
    "else:\n",
    "    df_synth = pd.DataFrame(columns=['path','label','origin'])\n",
    "\n",
    "df_all = pd.concat([df_original, df_synth], ignore_index=True)\n",
    "\n",
    "# Per-class summary\n",
    "summary_rows = []\n",
    "for lab in labels:\n",
    "    orig_n = int((df_original['label']==lab).sum())\n",
    "    syn_n = int((df_synth['label']==lab).sum())\n",
    "    tot = orig_n + syn_n\n",
    "    frac_syn = syn_n / tot if tot>0 else 0.0\n",
    "    summary_rows.append({'label': lab, 'original': orig_n, 'synthetic': syn_n, 'total': tot, 'synthetic_fraction': round(frac_syn,4)})\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(summary_df)\n",
    "\n",
    "# Identify deficits (should normally be zero if plan succeeded)\n",
    "expected_per_class = math.ceil(TARGET_TOTAL / len(labels))\n",
    "deficit_labels = [r.label for r in summary_df.itertuples() if r.total < expected_per_class]\n",
    "if deficit_labels:\n",
    "    print('[INFO] Deficit labels pending second pass:', deficit_labels)\n",
    "else:\n",
    "    print('No deficits detected after first pass.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce grayscale + 224x224 on all synthetic images (safety pass)\n",
    "if STRICT_ENFORCE_OUTPUT and RUN_WRITE:\n",
    "    repaired = 0\n",
    "    for rec in synthetic_rows:\n",
    "        p = Path(rec['path'])\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        try:\n",
    "            img = Image.open(p)\n",
    "            changed = False\n",
    "            if img.mode != GRAYSCALE_MODE:\n",
    "                img = img.convert(GRAYSCALE_MODE)\n",
    "                changed = True\n",
    "            if img.size != IMG_SIZE:\n",
    "                img = img.resize(IMG_SIZE, Image.BILINEAR)\n",
    "                changed = True\n",
    "            if changed:\n",
    "                img.save(p)\n",
    "                repaired += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(f\"STRICT_ENFORCE_OUTPUT applied. Repaired {repaired} synthetic files to {GRAYSCALE_MODE} {IMG_SIZE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72476f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Stratified Split with Synthetic Ratio Constraints\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Basic stratified split first on full df_all\n",
    "train_df, temp_df = train_test_split(df_all, test_size=(1-TRAIN_PCT), stratify=df_all['label'], random_state=BASE_SEED)\n",
    "val_rel = VAL_PCT / (VAL_PCT + TEST_PCT)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=(1-val_rel), stratify=temp_df['label'], random_state=BASE_SEED)\n",
    "\n",
    "# Enforce synthetic fraction cap in TRAIN by per-class adjustment\n",
    "adjusted_rows = []\n",
    "for lab in labels:\n",
    "    subset = train_df[train_df.label==lab]\n",
    "    orig_sub = subset[subset.origin=='original']\n",
    "    synth_sub = subset[subset.origin=='synthetic']\n",
    "    total_lab = len(subset)\n",
    "    if total_lab == 0:\n",
    "        continue\n",
    "    max_syn = int(MAX_PER_CLASS_SYNTH_FRACTION * total_lab)\n",
    "    if len(synth_sub) > max_syn:\n",
    "        # keep all originals, sample allowed number of synthetics\n",
    "        keep_synth = synth_sub.sample(n=max_syn, random_state=BASE_SEED)\n",
    "        adjusted = pd.concat([orig_sub, keep_synth])\n",
    "    else:\n",
    "        adjusted = subset\n",
    "    adjusted_rows.append(adjusted)\n",
    "train_df_adjusted = pd.concat(adjusted_rows) if adjusted_rows else train_df\n",
    "\n",
    "print({'train': len(train_df_adjusted), 'val': len(val_df), 'test': len(test_df)})\n",
    "print('Train synthetic fraction overall:', round((train_df_adjusted.origin=='synthetic').mean(),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14-15. Write Balanced Images & CSVs + Extended status.json\n",
    "\n",
    "def to_container_path(abs_path: Path) -> str:\n",
    "    # Standardize relative path inside processed dataset root\n",
    "    # Using pattern similar to prior splits: /data/processed/EmoSet_splits_gen/raw_all/<label>/<file>\n",
    "    rel = abs_path.relative_to(DEST_IMG_ROOT)\n",
    "    return f\"/data/processed/EmoSet_splits_gen/raw_all/{rel.as_posix()}\"\n",
    "\n",
    "if RUN_WRITE:\n",
    "    for lab in labels:\n",
    "        (DEST_IMG_ROOT / lab).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def copy_into_root(df_):\n",
    "        copied = 0\n",
    "        for rec in df_.itertuples():\n",
    "            src = Path(rec.path)\n",
    "            lab = rec.label\n",
    "            dst = DEST_IMG_ROOT / lab / src.name\n",
    "            if not dst.exists():\n",
    "                try:\n",
    "                    if ALLOW_HARDLINK:\n",
    "                        os.link(src, dst)\n",
    "                    else:\n",
    "                        shutil.copy2(src, dst)\n",
    "                    copied += 1\n",
    "                except Exception:\n",
    "                    continue\n",
    "        return copied\n",
    "\n",
    "    copied_n = copy_into_root(train_df_adjusted) + copy_into_root(val_df) + copy_into_root(test_df)\n",
    "    print('Copied/linked files:', copied_n)\n",
    "\n",
    "    def remap(df_):\n",
    "        df_ = df_.copy()\n",
    "        df_['path'] = df_['path'].apply(lambda p: to_container_path(DEST_IMG_ROOT / Path(p).parent.name / Path(p).name))\n",
    "        return df_\n",
    "\n",
    "    train_out = remap(train_df_adjusted)\n",
    "    val_out = remap(val_df)\n",
    "    test_out = remap(test_df)\n",
    "\n",
    "    train_out.to_csv(CSV_DIR / 'train.csv', index=False)\n",
    "    val_out.to_csv(CSV_DIR / 'val.csv', index=False)\n",
    "    test_out.to_csv(CSV_DIR / 'test.csv', index=False)\n",
    "    print('Wrote split CSVs')\n",
    "\n",
    "    # Extended status.json\n",
    "    per_class_extended = {}\n",
    "    for r in summary_df.itertuples():\n",
    "        per_class_extended[r.label] = {\n",
    "            'original': int(r.original),\n",
    "            'synthetic': int(r.synthetic),\n",
    "            'total': int(r.total),\n",
    "            'synthetic_fraction': float(r.synthetic_fraction)\n",
    "        }\n",
    "\n",
    "    total_images = int(summary_df['total'].sum())\n",
    "    total_synth = int(summary_df['synthetic'].sum())\n",
    "    status_json = {\n",
    "        'total_images': total_images,\n",
    "        'total_original': int(summary_df['original'].sum()),\n",
    "        'total_synthetic': total_synth,\n",
    "        'synthetic_fraction': float(total_synth/total_images if total_images else 0.0),\n",
    "        'per_class': per_class_extended,\n",
    "        'splits_fraction': {'train': TRAIN_PCT, 'val': VAL_PCT, 'test': TEST_PCT},\n",
    "        'image_size': {'width': IMG_SIZE[0], 'height': IMG_SIZE[1], 'mode': GRAYSCALE_MODE},\n",
    "        'seed': BASE_SEED,\n",
    "        'generation_model_id': MODEL_ID,\n",
    "        'prompts_version_hash': prompt_dict_hash,\n",
    "        'quality_thresholds': {\n",
    "            'blur_var_min': BLUR_VAR_MIN,\n",
    "            'hash_dist_max': HASH_DIST_MAX,\n",
    "            'phash_size': PHASH_SIZE,\n",
    "        },\n",
    "        'created_timestamp': datetime.now(timezone.utc).isoformat().replace('+00:00','Z'),\n",
    "    }\n",
    "    with open(CSV_DIR / 'status.json','w') as f:\n",
    "        json.dump(status_json, f, indent=2)\n",
    "    print('Wrote status.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fcd722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16-17. Diagnostics & Balance Report + Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "TOL_PCT = 2.0\n",
    "\n",
    "def diag(counts: pd.Series) -> dict:\n",
    "    counts = counts.sort_index()\n",
    "    target = counts.mean()\n",
    "    diffs = (counts - target) / target * 100\n",
    "    return {\n",
    "        'counts': counts.to_dict(),\n",
    "        'min': int(counts.min()),\n",
    "        'max': int(counts.max()),\n",
    "        'mean': float(target),\n",
    "        'std': float(counts.std(ddof=0)),\n",
    "        'max_dev_pct': float(diffs.abs().max()),\n",
    "        'balanced_within_tol_pct': bool(diffs.abs().max() <= TOL_PCT),\n",
    "        'tolerance_pct': TOL_PCT,\n",
    "    }\n",
    "\n",
    "print('Overall total diag:')\n",
    "print(diag(df_all['label'].value_counts()))\n",
    "print('\\nOriginal-only diag:')\n",
    "print(diag(df_original['label'].value_counts()))\n",
    "\n",
    "splits_named = {\n",
    "    'train': train_df_adjusted,\n",
    "    'val': val_df,\n",
    "    'test': test_df,\n",
    "}\n",
    "for name, sdf in splits_named.items():\n",
    "    print(f\"\\nSplit {name} diag:\")\n",
    "    print(diag(sdf['label'].value_counts()))\n",
    "\n",
    "# Plot counts per class stacked (original vs synthetic)\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "summary_df.set_index('label')[['original','synthetic']].plot(kind='bar', stacked=True, ax=ax, color=['#4B8BBE','#FFD43B'])\n",
    "ax.set_title('Original vs Synthetic Counts per Class')\n",
    "ax.set_ylabel('Count')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Synthetic fraction bar\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "summary_df.plot(x='label', y='synthetic_fraction', kind='bar', ax=ax, color='#FF8C42')\n",
    "ax.set_title('Synthetic Fraction per Class (Total Dataset)')\n",
    "ax.set_ylabel('Fraction')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Heatmap synthetic fraction per split\n",
    "heat_data = []\n",
    "for lab in labels:\n",
    "    row = []\n",
    "    for name, sdf in splits_named.items():\n",
    "        sub = sdf[sdf.label==lab]\n",
    "        if len(sub)==0:\n",
    "            row.append(0.0)\n",
    "        else:\n",
    "            row.append((sub.origin=='synthetic').mean())\n",
    "    heat_data.append(row)\n",
    "heat_df = pd.DataFrame(heat_data, index=labels, columns=['train','val','test'])\n",
    "plt.figure(figsize=(6, max(4, len(labels)*0.4)))\n",
    "sns.heatmap(heat_df, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Synthetic Fraction per Class per Split')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Sample Visualization (Original vs Synthetic)\n",
    "import random\n",
    "\n",
    "N_SHOW = 5  # per class\n",
    "random.seed(BASE_SEED)\n",
    "\n",
    "fig_rows = []\n",
    "for lab in labels:\n",
    "    orig_paths = df_original[df_original.label==lab]['path'].tolist()\n",
    "    synth_paths = df_synth[df_synth.label==lab]['path'].tolist() if 'df_synth' in globals() else []\n",
    "    random.shuffle(orig_paths)\n",
    "    random.shuffle(synth_paths)\n",
    "    show_orig = orig_paths[:N_SHOW]\n",
    "    show_syn = synth_paths[:N_SHOW]\n",
    "    # Build grid row per class\n",
    "    fig_rows.append((lab, show_orig, show_syn))\n",
    "\n",
    "n_classes = len(fig_rows)\n",
    "cols = N_SHOW\n",
    "fig, axes = plt.subplots(n_classes*2, cols, figsize=(cols*2.2, n_classes*2.2*2))\n",
    "for r, (lab, orig_list, syn_list) in enumerate(fig_rows):\n",
    "    for c in range(cols):\n",
    "        ax_o = axes[r*2][c]\n",
    "        if c < len(orig_list):\n",
    "            try:\n",
    "                img = Image.open(orig_list[c]).convert('L')\n",
    "                ax_o.imshow(img, cmap='gray')\n",
    "            except Exception:\n",
    "                ax_o.text(0.5,0.5,'err',ha='center')\n",
    "        ax_o.set_xticks([]); ax_o.set_yticks([])\n",
    "        if c==0:\n",
    "            ax_o.set_ylabel(f'{lab}\\n(orig)', rotation=0, labelpad=40, va='center')\n",
    "\n",
    "        ax_s = axes[r*2+1][c]\n",
    "        if c < len(syn_list):\n",
    "            try:\n",
    "                img = Image.open(syn_list[c]).convert('L')\n",
    "                ax_s.imshow(img, cmap='gray')\n",
    "            except Exception:\n",
    "                ax_s.text(0.5,0.5,'err',ha='center')\n",
    "        ax_s.set_xticks([]); ax_s.set_yticks([])\n",
    "        if c==0:\n",
    "            ax_s.set_ylabel('syn', rotation=0, labelpad=20, va='center')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Grayscale Integrity & Shape Assertions\n",
    "sample_check_paths = df_all.sample(min(50, len(df_all)), random_state=BASE_SEED)['path'].tolist()\n",
    "for p in sample_check_paths[:20]:\n",
    "    try:\n",
    "        img = Image.open(p)\n",
    "        assert img.mode == 'L', f'Not grayscale: {p} mode={img.mode}'\n",
    "        assert img.size == IMG_SIZE, f'Bad size: {p} size={img.size}'\n",
    "    except Exception as e:\n",
    "        print('[WARN] Integrity check issue:', e)\n",
    "print('Basic grayscale + size assertions done.')\n",
    "\n",
    "# Mean/std comparison\n",
    "orig_stats_imgs = df_original.sample(min(500, len(df_original)), random_state=BASE_SEED)['path'].tolist()\n",
    "synth_stats_imgs = df_synth.sample(min(500, len(df_synth)), random_state=BASE_SEED)['path'].tolist() if len(df_synth) else []\n",
    "\n",
    "def gather_stats(paths):\n",
    "    vals = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            arr = np.array(Image.open(p).convert('L'), dtype=np.float32) / 255.0\n",
    "            vals.append(arr)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not vals:\n",
    "        return {'mean': None, 'std': None}\n",
    "    stack = np.stack(vals)\n",
    "    return {'mean': float(stack.mean()), 'std': float(stack.std())}\n",
    "\n",
    "orig_stats = gather_stats(orig_stats_imgs)\n",
    "synth_stats = gather_stats(synth_stats_imgs)\n",
    "print('Original grayscale mean/std:', orig_stats)\n",
    "print('Synthetic grayscale mean/std:', synth_stats)\n",
    "\n",
    "# 20. Regeneration Retry for Deficit Classes (simplified placeholder)\n",
    "if deficit_labels:\n",
    "    print('[Retry] Attempting second-pass generation for deficits...')\n",
    "    # For brevity, not re-implementing full second loop here.\n",
    "    # Could call a function similar to main loop with modified prompts.\n",
    "    pass\n",
    "\n",
    "# 21. Reproducibility & Environment Capture\n",
    "env_report = {\n",
    "    'python_version': sys.version,\n",
    "    'platform': platform.platform(),\n",
    "    'torch_version': torch.__version__,\n",
    "    'diffusers_version': __import__('diffusers').__version__,\n",
    "    'numpy_version': np.__version__,\n",
    "    'model_id': MODEL_ID,\n",
    "    'base_seed': BASE_SEED,\n",
    "    'prompt_dict_hash': prompt_dict_hash,\n",
    "    'generation_params': {\n",
    "        'steps': INFERENCE_STEPS,\n",
    "        'guidance_scale': GUIDANCE_SCALE,\n",
    "    },\n",
    "}\n",
    "if RUN_WRITE:\n",
    "    with open(CSV_DIR / 'env_report.json','w') as f:\n",
    "        json.dump(env_report, f, indent=2)\n",
    "\n",
    "# 22. Disk Usage & Summary Stats\n",
    "import humanize\n",
    "\n",
    "def dir_size(path: Path) -> int:\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for fn in files:\n",
    "            try:\n",
    "                total += (Path(root)/fn).stat().st_size\n",
    "            except Exception:\n",
    "                pass\n",
    "    return total\n",
    "\n",
    "orig_size = dir_size(RAW_ROOT)\n",
    "synthetic_size = dir_size(OUT_GEN_ROOT) if OUT_GEN_ROOT.exists() else 0\n",
    "final_size = dir_size(DEST_IMG_ROOT) if DEST_IMG_ROOT.exists() else 0\n",
    "\n",
    "print('\\nDisk Usage:')\n",
    "print('Original raw root:', humanize.naturalsize(orig_size))\n",
    "print('Synthetic root:', humanize.naturalsize(synthetic_size))\n",
    "print('Final curated root:', humanize.naturalsize(final_size))\n",
    "\n",
    "print('\\nSUMMARY:')\n",
    "print(f\"Total images original: {len(df_original)}\")\n",
    "print(f\"Total images synthetic: {len(df_synth)}\")\n",
    "print(f\"Total combined: {len(df_all)}\")\n",
    "print('Train/Val/Test sizes:', len(train_df_adjusted), len(val_df), len(test_df))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
