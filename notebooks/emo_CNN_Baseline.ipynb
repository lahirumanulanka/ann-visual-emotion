{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahirumanulanka/ann-visual-emotion/blob/created_new_dataset/notebooks/emo_CNN_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiWayyMtEiFP",
        "outputId": "7b7a38cc-e1c0-4b76-fb28-89c5f7795a5c"
      },
      "id": "BiWayyMtEiFP",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ===========================\n",
        "# # Cell 0 — (Colab) installs\n",
        "# # ===========================\n",
        "# # Assumes torch/torchvision already OK in your runtime.\n",
        "# # If not, install a matched triplet first (we discussed earlier).\n",
        "# !pip -q install timm==1.0.9 facenet-pytorch opencv-python-headless"
      ],
      "metadata": {
        "id": "CwbTV_VLEvjF"
      },
      "id": "CwbTV_VLEvjF",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Smart installer: pick one build: 'cu121', 'cu126', or 'cpu'\n",
        "# # Using standard PyTorch installation command for Colab to ensure compatibility\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# # Verify installation\n",
        "# import torch, torchvision, torchaudio\n",
        "# print(\"torch:\", torch.__version__)\n",
        "# print(\"torchvision:\", torchvision.__version__)\n",
        "# print(\"torchaudio:\", torchaudio.__version__)\n",
        "# print(\"CUDA available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "77mB6a1D08zr"
      },
      "id": "77mB6a1D08zr",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 1 — Imports & setup\n",
        "# =========================\n",
        "import os, json, math, time, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "plt.rcParams[\"figure.dpi\"] = 120\n",
        "\n",
        "# reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buRATqkmyF66",
        "outputId": "414f9fc5-6abf-4795-e4e2-d27b6105a704"
      },
      "id": "buRATqkmyF66",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Change this if your folder lives elsewhere (e.g., in a Shared Drive)\n",
        "PROJECT_ROOTS = [\n",
        "    \"/content/drive/MyDrive/ann-visual-emotion\",                          # common\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/ann-visual-emotion\",          # sometimes used\n",
        "    \"/content/drive/Shared drives/YourTeamDrive/ann-visual-emotion\",      # shared drive (edit name)\n",
        "]\n",
        "\n",
        "import os\n",
        "for p in PROJECT_ROOTS:\n",
        "    if os.path.isdir(p):\n",
        "        PROJECT_ROOT = p\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        \"Couldn't find 'ann-visual-emotion' in the usual locations.\\n\"\n",
        "        \"Please create it or update PROJECT_ROOTS with the correct path.\"\n",
        "    )\n",
        "\n",
        "print(\"Using PROJECT_ROOT:\", PROJECT_ROOT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIIAfHGD2Yd5",
        "outputId": "18d9cbc7-a4c6-41ce-8e2c-a6b8a7c77f09"
      },
      "id": "fIIAfHGD2Yd5",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PROJECT_ROOT: /content/drive/MyDrive/ann-visual-emotion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# Cell 2 — Configuration\n",
        "# ========================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ---- Base project dir on Google Drive (set by Cell 1.5) ----\n",
        "assert 'PROJECT_ROOT' in globals(), \"Run the Drive mount cell first to set PROJECT_ROOT.\"\n",
        "\n",
        "# ---- Common subfolders (edit if your structure differs) ----\n",
        "RAW_DIR        = Path(PROJECT_ROOT) / \"data\" / \"raw\" / \"EmoSet\"\n",
        "SPLIT_DIR      = Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"EmoSet_splits\"\n",
        "\n",
        "# ---- Your ORIGINAL dataset (used if RUN_FACE_PREP=True) ----\n",
        "IMAGES_ROOT_RAW = str(RAW_DIR)                                 # folder containing original images\n",
        "TRAIN_CSV_RAW   = str(SPLIT_DIR / \"train.csv\")\n",
        "VAL_CSV_RAW     = str(SPLIT_DIR / \"val.csv\")\n",
        "TEST_CSV_RAW    = str(SPLIT_DIR / \"test.csv\")                  # set to None if you don't have it\n",
        "\n",
        "LABEL_MAP_JSON  = str(SPLIT_DIR / \"label_map.json\")            # optional\n",
        "\n",
        "# ---- Face-cropped dataset output (created if RUN_FACE_PREP=True) ----\n",
        "FACES_ROOT      = str(Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"EmoSet_faces224\")\n",
        "FACES_SPLITS    = str(Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"EmoSet_faces_splits\")\n",
        "\n",
        "# Create output dirs if missing\n",
        "Path(FACES_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "Path(FACES_SPLITS).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Whether to run the face detection/cropping step now ----\n",
        "RUN_FACE_PREP   = True   # set False to train directly on RAW images\n",
        "\n",
        "# ---- Training will read from these (set automatically if RUN_FACE_PREP=True) ----\n",
        "IMAGES_ROOT     = FACES_ROOT if RUN_FACE_PREP else IMAGES_ROOT_RAW\n",
        "TRAIN_CSV       = None  # set after face prep or set to raw if skipping\n",
        "VAL_CSV         = None\n",
        "TEST_CSV        = None\n",
        "\n",
        "# ---- Data & model ----\n",
        "IMG_MODE        = \"rgb\"      # after face-cropping we save RGB crops; if training on raw gray 48x48, set 'gray'\n",
        "IMG_SIZE        = 224        # ResNet default\n",
        "BATCH           = 64\n",
        "NUM_WORKERS     = 4\n",
        "\n",
        "# ---- Optimization ----\n",
        "EPOCHS          = 30\n",
        "WARMUP_EPOCHS   = 2\n",
        "BASE_LR         = 3e-4\n",
        "WEIGHT_DECAY    = 1e-4\n",
        "LABEL_SMOOTHING = 0.05\n",
        "\n",
        "# ---- Imbalance / loss ----\n",
        "BALANCE_MODE    = \"sampler\"  # 'sampler' or 'none'\n",
        "LOSS_MODE       = \"ce\"       # 'ce' or 'focal'\n",
        "FOCAL_GAMMA     = 2.0\n",
        "\n",
        "# ---- Augmentations ----\n",
        "USE_MIXUP       = True\n",
        "MIXUP_ALPHA     = 0.2\n",
        "USE_CUTMIX      = False\n",
        "CUTMIX_ALPHA    = 0.0\n",
        "\n",
        "# ---- Early stopping ----\n",
        "PATIENCE        = 5\n",
        "\n",
        "# ---- Outputs (also on Drive so they persist) ----\n",
        "OUT_DIR         = str(Path(PROJECT_ROOT) / \"outputs\" / \"emo_training\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Sanity prints ----\n",
        "print(\"IMAGES_ROOT_RAW:\", IMAGES_ROOT_RAW)\n",
        "print(\"TRAIN/VAL/TEST CSV:\", TRAIN_CSV_RAW, VAL_CSV_RAW, TEST_CSV_RAW)\n",
        "print(\"FACES_ROOT:\", FACES_ROOT)\n",
        "print(\"FACES_SPLITS:\", FACES_SPLITS)\n",
        "print(\"OUT_DIR:\", OUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8q-nxQjyHx-",
        "outputId": "5b0db77f-25b9-4613-8df3-2002f275eb16"
      },
      "id": "I8q-nxQjyHx-",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMAGES_ROOT_RAW: /content/drive/MyDrive/ann-visual-emotion/data/raw/EmoSet\n",
            "TRAIN/VAL/TEST CSV: /content/drive/MyDrive/ann-visual-emotion/data/processed/EmoSet_splits/train.csv /content/drive/MyDrive/ann-visual-emotion/data/processed/EmoSet_splits/val.csv /content/drive/MyDrive/ann-visual-emotion/data/processed/EmoSet_splits/test.csv\n",
            "FACES_ROOT: /content/drive/MyDrive/ann-visual-emotion/data/processed/EmoSet_faces224\n",
            "FACES_SPLITS: /content/drive/MyDrive/ann-visual-emotion/data/processed/EmoSet_faces_splits\n",
            "OUT_DIR: /content/drive/MyDrive/ann-visual-emotion/outputs/emo_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 3 — Label map & small helper funcs\n",
        "# =========================================\n",
        "# Definitions moved to Cell 5 to resolve NameError"
      ],
      "metadata": {
        "id": "OBmPUZiPyLk5"
      },
      "id": "OBmPUZiPyLk5",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# Cell 4 — OPTIONAL: Face detection & crop with MTCNN\n",
        "# (Runs once. Produces new CSVs that point to face crops)\n",
        "# Skips face-cropping for files starting with 'syn_'\n",
        "# ======================================================\n",
        "if RUN_FACE_PREP:\n",
        "    from facenet_pytorch import MTCNN\n",
        "    from PIL import Image\n",
        "\n",
        "    FACE_SIZE     = 224\n",
        "    MARGIN_FRAC   = 0.20     # padding around face bbox\n",
        "    MIN_FACE_SIZE = 24\n",
        "    CONF_THRESH   = 0.90\n",
        "\n",
        "    os.makedirs(FACES_ROOT, exist_ok=True)\n",
        "    os.makedirs(FACES_SPLITS, exist_ok=True)\n",
        "\n",
        "    mtcnn = MTCNN(\n",
        "        image_size=FACE_SIZE, margin=0, min_face_size=MIN_FACE_SIZE,\n",
        "        thresholds=[0.6, 0.7, 0.7], post_process=False,\n",
        "        device=device, keep_all=True\n",
        "    )\n",
        "\n",
        "    def add_margin(box, w, h, frac=0.2):\n",
        "        x1, y1, x2, y2 = box\n",
        "        bw, bh = x2 - x1, y2 - y1\n",
        "        m = frac * max(bw, bh)\n",
        "        nx1 = max(0, int(x1 - m))\n",
        "        ny1 = max(0, int(y1 - m))\n",
        "        nx2 = min(w, int(x2 + m))\n",
        "        ny2 = min(h, int(y2 + m))\n",
        "        return [nx1, ny1, nx2, ny2]\n",
        "\n",
        "    def largest_face_box(boxes, probs):\n",
        "        if boxes is None or len(boxes) == 0: return None, None\n",
        "        areas = (boxes[:,2]-boxes[:,0]) * (boxes[:,3]-boxes[:,1])\n",
        "        score = areas * probs\n",
        "        idx = int(np.argmax(score))\n",
        "        return boxes[idx], float(probs[idx])\n",
        "\n",
        "    def save_face_crop(pil_img, box, out_path, size=224):\n",
        "        w, h = pil_img.size\n",
        "        x1, y1, x2, y2 = [int(v) for v in box]\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(w, x2), min(h, y2)\n",
        "        crop = pil_img.crop((x1, y1, x2, y2))\n",
        "        # square pad to avoid aspect distortion\n",
        "        cw, ch = crop.size\n",
        "        side = max(cw, ch)\n",
        "        canvas = Image.new(\"RGB\", (side, side), (0,0,0))\n",
        "        canvas.paste(crop, ((side - cw)//2, (side - ch)//2))\n",
        "        canvas = canvas.resize((size, size), Image.BILINEAR)\n",
        "        Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        canvas.save(out_path, format=\"JPEG\", quality=95, subsampling=0)\n",
        "        return out_path\n",
        "\n",
        "    def process_split(csv_path, images_root, faces_root, out_split_dir,\n",
        "                      label_col=\"label\", path_col=\"image\",\n",
        "                      face_size=224, margin_frac=0.2, conf_thresh=0.90,\n",
        "                      fallback_to_full=True, split_name=\"train\"):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        out_rows = []\n",
        "        misses = 0\n",
        "\n",
        "        for i, row in df.iterrows():\n",
        "            # path column handling\n",
        "            rel_path = row[path_col] if path_col in df.columns else row.get(\"image_path\", None)\n",
        "            if rel_path is None:\n",
        "                raise ValueError(\"CSV must have column 'image' or 'image_path'\")\n",
        "\n",
        "            src = resolve_path(rel_path, images_root)\n",
        "            label = str(row[label_col]) if label_col in df.columns else str(row.get(\"label_idx\"))\n",
        "\n",
        "            base = Path(rel_path).name\n",
        "            # enforce .jpg output\n",
        "            out_path = str(Path(faces_root) / split_name / label / base)\n",
        "            out_path = out_path.rsplit(\".\", 1)[0] + \".jpg\"\n",
        "\n",
        "            # ---- SKIP CROPPING for syn_ files: just resize to FACE_SIZE ----\n",
        "            if base.startswith(\"syn_\"):\n",
        "                try:\n",
        "                    pil = Image.open(src).convert(\"RGB\")\n",
        "                    pil = pil.resize((face_size, face_size), Image.BILINEAR)\n",
        "                    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "                    pil.save(out_path, format=\"JPEG\", quality=95)\n",
        "                    out_rows.append({\"image\": out_path, \"label\": label})\n",
        "                except Exception as e:\n",
        "                    print(f\"[{split_name}] bad syn_ image {src}: {e}\")\n",
        "                    misses += 1\n",
        "                continue\n",
        "\n",
        "            # ---- Normal face-cropping for non-syn_ files ----\n",
        "            try:\n",
        "                pil = Image.open(src).convert(\"RGB\")\n",
        "            except Exception as e:\n",
        "                print(f\"[{split_name}] bad image {src}: {e}\")\n",
        "                if fallback_to_full:\n",
        "                    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "                    # save a black placeholder of correct size to keep row\n",
        "                    Image.new(\"RGB\", (face_size, face_size), (0,0,0)).save(out_path, format=\"JPEG\", quality=95)\n",
        "                    out_rows.append({\"image\": out_path, \"label\": label})\n",
        "                else:\n",
        "                    misses += 1\n",
        "                continue\n",
        "\n",
        "            # detect faces\n",
        "            boxes, probs = mtcnn.detect(pil)\n",
        "            box, score = largest_face_box(boxes, probs) if boxes is not None else (None, None)\n",
        "\n",
        "            if box is not None and score is not None and score >= conf_thresh:\n",
        "                w, h = pil.size\n",
        "                mbox = add_margin(box, w, h, frac=margin_frac)\n",
        "                save_face_crop(pil, mbox, out_path, size=face_size)\n",
        "                out_rows.append({\"image\": out_path, \"label\": label})\n",
        "            else:\n",
        "                if fallback_to_full:\n",
        "                    pil = pil.resize((face_size, face_size), Image.BILINEAR)\n",
        "                    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "                    pil.save(out_path, format=\"JPEG\", quality=95)\n",
        "                    out_rows.append({\"image\": out_path, \"label\": label})\n",
        "                else:\n",
        "                    misses += 1\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(f\"[{split_name}] processed {i+1}/{len(df)}\")\n",
        "\n",
        "        out_df = pd.DataFrame(out_rows)\n",
        "        out_csv = str(Path(out_split_dir) / f\"{split_name}.csv\")\n",
        "        out_df.to_csv(out_csv, index=False)\n",
        "        print(f\"[{split_name}] wrote {len(out_df)} rows → {out_csv}; misses = {misses}\")\n",
        "        return out_csv\n",
        "\n",
        "    # ---- Run face extraction for each split ----\n",
        "    TRAIN_CSV = process_split(TRAIN_CSV_RAW, IMAGES_ROOT_RAW, FACES_ROOT, FACES_SPLITS,\n",
        "                              split_name=\"train\", face_size=FACE_SIZE,\n",
        "                              margin_frac=MARGIN_FRAC, conf_thresh=CONF_THRESH)\n",
        "\n",
        "    VAL_CSV   = process_split(VAL_CSV_RAW,   IMAGES_ROOT_RAW, FACES_ROOT, FACES_SPLITS,\n",
        "                              split_name=\"val\", face_size=FACE_SIZE,\n",
        "                              margin_frac=MARGIN_FRAC, conf_thresh=CONF_THRESH)\n",
        "\n",
        "    TEST_CSV  = None\n",
        "    if TEST_CSV_RAW and Path(TEST_CSV_RAW).exists():\n",
        "        TEST_CSV = process_split(TEST_CSV_RAW, IMAGES_ROOT_RAW, FACES_ROOT, FACES_SPLITS,\n",
        "                                 split_name=\"test\", face_size=FACE_SIZE,\n",
        "                                 margin_frac=MARGIN_FRAC, conf_thresh=CONF_THRESH)\n",
        "\n",
        "    # After cropping we train on the new face dataset\n",
        "    IMAGES_ROOT = FACES_ROOT\n",
        "    IMG_MODE    = \"rgb\"\n",
        "else:\n",
        "    # training straight on raw images\n",
        "    TRAIN_CSV, VAL_CSV, TEST_CSV = TRAIN_CSV_RAW, VAL_CSV_RAW, TEST_CSV_RAW"
      ],
      "metadata": {
        "id": "IE5Gbz-JyQEf"
      },
      "id": "IE5Gbz-JyQEf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================\n",
        "# Cell 5 — Dataset & DataLoaders\n",
        "# (fixes transform order: ToTensor BEFORE Normalize/RandomErasing)\n",
        "# ==================================\n",
        "TRAIN_CSV = process_split(TRAIN_CSV_RAW, str(RAW_DIR), FACES_ROOT, FACES_SPLITS, \"train\")\n",
        "VAL_CSV   = process_split(VAL_CSV_RAW,   str(RAW_DIR), FACES_ROOT, FACES_SPLITS, \"val\")\n",
        "TEST_CSV  = process_split(TEST_CSV_RAW,  str(RAW_DIR), FACES_ROOT, FACES_SPLITS, \"test\") if (TEST_CSV_RAW and Path(TEST_CSV_RAW).exists()) else None\n",
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "str2idx, idx2str = load_label_map(TRAIN_CSV, LABEL_MAP_JSON if not RUN_FACE_PREP else None)\n",
        "num_classes = len(idx2str)\n",
        "print(\"Classes:\", idx2str)\n",
        "\n",
        "class EmotionCSVDataset(Dataset):\n",
        "    def __init__(self, csv_path, images_root, str2idx, img_mode=\"rgb\", img_size=224, train=True):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.images_root = images_root\n",
        "        self.str2idx = str2idx\n",
        "        self.img_mode = img_mode\n",
        "        self.train = train\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # path column\n",
        "        self.path_col = \"image\" if \"image\" in self.df.columns else (\"image_path\" if \"image_path\" in self.df.columns else None)\n",
        "        if self.path_col is None:\n",
        "            raise ValueError(\"CSV must contain 'image' or 'image_path'\")\n",
        "\n",
        "        # label to idx\n",
        "        if \"label\" not in self.df.columns:\n",
        "            raise ValueError(\"CSV must contain 'label' column\")\n",
        "        if self.df[\"label\"].dtype == object:\n",
        "            self.df[\"label_idx\"] = self.df[\"label\"].map(self.str2idx).astype(int)\n",
        "        else:\n",
        "            self.df[\"label_idx\"] = self.df[\"label\"].astype(int)\n",
        "\n",
        "        # --- IMPORTANT: ToTensor BEFORE Normalize/RandomErasing ---\n",
        "        normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "\n",
        "        if train:\n",
        "            self.tf = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                (transforms.Grayscale(num_output_channels=3) if img_mode==\"gray\" else transforms.Lambda(lambda x: x)),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomApply([transforms.ColorJitter(0.2,0.2,0.2,0.1)], p=0.5),\n",
        "                transforms.ToTensor(),                # <-- convert to tensor first\n",
        "                transforms.RandomErasing(             # <-- now OK (expects tensor)\n",
        "                    p=0.3, scale=(0.02, 0.12), ratio=(0.3, 3.3)\n",
        "                ),\n",
        "                normalize,                            # <-- Normalize expects tensor\n",
        "            ])\n",
        "        else:\n",
        "            self.tf = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                (transforms.Grayscale(num_output_channels=3) if img_mode==\"gray\" else transforms.Lambda(lambda x: x)),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        img_path = resolve_path(row[self.path_col], self.images_root)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        y = int(row[\"label_idx\"])\n",
        "        return self.tf(img), y\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self.df[\"label_idx\"].to_numpy()\n",
        "\n",
        "train_ds = EmotionCSVDataset(TRAIN_CSV, IMAGES_ROOT, str2idx, IMG_MODE, IMG_SIZE, train=True)\n",
        "val_ds   = EmotionCSVDataset(VAL_CSV,   IMAGES_ROOT, str2idx, IMG_MODE, IMG_SIZE, train=False)\n",
        "test_ds  = EmotionCSVDataset(TEST_CSV,  IMAGES_ROOT, str2idx, IMG_MODE, IMG_SIZE, train=False) if (TEST_CSV and Path(TEST_CSV).exists()) else None\n",
        "\n",
        "counts = np.bincount(train_ds.labels, minlength=num_classes)\n",
        "print(\"Train counts per class:\", counts)\n",
        "\n",
        "if BALANCE_MODE == \"sampler\":\n",
        "    inv = 1.0 / np.clip(counts, 1, None)\n",
        "    sample_weights = inv[train_ds.labels]\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "else:\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "val_loader  = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True) if test_ds else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "dbmsFlgfySLC",
        "outputId": "f07f03d5-df06-4a2c-8364-05f4a9f56d80"
      },
      "id": "dbmsFlgfySLC",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'process_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-81284346.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# (fixes transform order: ToTensor BEFORE Normalize/RandomErasing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ==================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mTRAIN_CSV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_CSV_RAW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFACES_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFACES_SPLITS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mVAL_CSV\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mprocess_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_CSV_RAW\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFACES_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFACES_SPLITS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mTEST_CSV\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mprocess_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_CSV_RAW\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFACES_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFACES_SPLITS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTEST_CSV_RAW\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_CSV_RAW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'process_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 6 — Model & losses\n",
        "# =========================\n",
        "weights = ResNet18_Weights.IMAGENET1K_V1\n",
        "model = resnet18(weights=weights)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(model.fc.in_features, num_classes)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Class weights if not using sampler\n",
        "class_weights = None\n",
        "if LOSS_MODE == \"ce\" and BALANCE_MODE != \"sampler\":\n",
        "    inv = 1.0 / np.clip(counts, 1, None)\n",
        "    class_weights = torch.tensor(inv / inv.sum() * num_classes, dtype=torch.float32, device=device)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
        "    def forward(self, logits, targets):\n",
        "        ce = self.ce(logits, targets)\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
        "\n",
        "criterion = FocalLoss(gamma=FOCAL_GAMMA, weight=class_weights) if LOSS_MODE==\"focal\" \\\n",
        "            else nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < WARMUP_EPOCHS:\n",
        "        return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
        "    progress = (epoch - WARMUP_EPOCHS) / float(max(1, EPOCHS - WARMUP_EPOCHS))\n",
        "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)"
      ],
      "metadata": {
        "id": "cWXmD-hbydIH"
      },
      "id": "cWXmD-hbydIH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================\n",
        "# Cell 7 — MixUp / CutMix helpers\n",
        "# ==================================\n",
        "def rand_bbox(W, H, lam):\n",
        "    cut_rat = math.sqrt(1.0 - lam)\n",
        "    cw, ch = int(W * cut_rat), int(H * cut_rat)\n",
        "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
        "    x1 = np.clip(cx - cw // 2, 0, W); x2 = np.clip(cx + cw // 2, 0, W)\n",
        "    y1 = np.clip(cy - ch // 2, 0, H); y2 = np.clip(cy + ch // 2, 0, H)\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ],
      "metadata": {
        "id": "s0OyEejpyg_j"
      },
      "id": "s0OyEejpyg_j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================\n",
        "# Cell 8 — Train / Evaluate loops\n",
        "# ==================================\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "            ys.append(y.cpu().numpy()); ps.append(pred.cpu().numpy())\n",
        "    y_true = np.concatenate(ys); y_pred = np.concatenate(ps)\n",
        "    return accuracy_score(y_true, y_pred), f1_score(y_true, y_pred, average='macro', zero_division=0), y_true, y_pred\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=5):\n",
        "        self.patience = patience; self.best = -1; self.bad_epochs = 0\n",
        "        self.best_path = Path(OUT_DIR) / \"best_model.pth\"\n",
        "    def step(self, score, model):\n",
        "        improved = score > self.best\n",
        "        if improved:\n",
        "            self.best = score; self.bad_epochs = 0\n",
        "            torch.save({\"state_dict\": model.state_dict(), \"best_macro_f1\": score}, self.best_path)\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "        return improved\n",
        "\n",
        "def confusion_matrix_fig(y_true, y_pred, title, labels):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
        "    fig, ax = plt.subplots(figsize=(5,5))\n",
        "    im = ax.imshow(cm, cmap='viridis')\n",
        "    ax.set_title(title); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
        "    ax.set_xticks(range(len(labels))); ax.set_yticks(range(len(labels)))\n",
        "    ax.set_xticklabels(range(len(labels))); ax.set_yticklabels(range(len(labels)))\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    return fig, cm\n",
        "\n",
        "history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": [], \"val_f1\": []}\n",
        "early = EarlyStopper(PATIENCE)\n",
        "\n",
        "def plot_live():\n",
        "    clear_output(wait=True)\n",
        "    fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "    ax[0].plot(history[\"train_loss\"], label=\"train loss\")\n",
        "    ax[0].plot(history[\"val_loss\"], label=\"val loss\")\n",
        "    ax[0].set_title(\"Loss\"); ax[0].legend()\n",
        "    ax[1].plot(history[\"val_acc\"], label=\"val acc\")\n",
        "    ax[1].plot(history[\"val_f1\"], label=\"val macro-F1\")\n",
        "    ax[1].set_title(\"Validation\"); ax[1].legend()\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def train_one_epoch():\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        use_mix = USE_MIXUP and MIXUP_ALPHA > 0\n",
        "        use_cut = USE_CUTMIX and CUTMIX_ALPHA > 0\n",
        "\n",
        "        if use_mix or use_cut:\n",
        "            indices = torch.randperm(x.size(0)).to(device)\n",
        "            y_a, y_b = y, y[indices]\n",
        "\n",
        "            if use_mix:\n",
        "                lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "                x_mixed = lam * x + (1 - lam) * x[indices]\n",
        "                logits = model(x_mixed)\n",
        "                loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
        "            else:\n",
        "                lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "                W, H = x.size(3), x.size(2)\n",
        "                x1,y1,x2,y2 = rand_bbox(W,H,lam)\n",
        "                x_cut = x.clone()\n",
        "                x_cut[:, :, y1:y2, x1:x2] = x[indices, :, y1:y2, x1:x2]\n",
        "                lam = 1 - ((x2-x1)*(y2-y1)/(W*H))\n",
        "                logits = model(x_cut)\n",
        "                loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
        "        else:\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        running += loss.item() * x.size(0)\n",
        "    return running / len(train_ds)"
      ],
      "metadata": {
        "id": "NpGffPodyowP"
      },
      "id": "NpGffPodyowP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Cell 9 — Run training loop\n",
        "# ===========================\n",
        "best_epoch = -1\n",
        "for epoch in range(EPOCHS):\n",
        "    t0 = time.time()\n",
        "    train_loss = train_one_epoch()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_running = 0.0; ys=[]; ps=[]\n",
        "        for x,y in val_loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            val_running += loss.item() * x.size(0)\n",
        "            ys.append(y.cpu().numpy()); ps.append(torch.argmax(logits,1).cpu().numpy())\n",
        "        y_true = np.concatenate(ys); y_pred = np.concatenate(ps)\n",
        "        val_loss = val_running / len(val_ds)\n",
        "        val_acc  = accuracy_score(y_true, y_pred)\n",
        "        val_f1   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "    history[\"val_f1\"].append(val_f1)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if early.step(val_f1, model): best_epoch = epoch\n",
        "    plot_live()\n",
        "    print(f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
        "          f\"train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_acc={val_acc:.4f}  val_macroF1={val_f1:.4f}  \"\n",
        "          f\"lr={optimizer.param_groups[0]['lr']:.2e}  time={time.time()-t0:.1f}s\")\n",
        "\n",
        "    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    per_class = [rep.get(str(i), {}).get(\"f1-score\", 0.0) for i in range(num_classes)]\n",
        "    print(\"Per-class F1:\", np.round(per_class, 3))\n",
        "\n",
        "    if early.bad_epochs >= PATIENCE:\n",
        "        print(f\"Early stopping. Best epoch was {best_epoch+1}.\")\n",
        "        break\n",
        "\n",
        "# Save plots\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(history[\"train_loss\"], label=\"train\"); plt.plot(history[\"val_loss\"], label=\"val\")\n",
        "plt.title(\"Loss\"); plt.xlabel(\"epoch\"); plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"loss_curve.png\"))\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(history[\"val_acc\"], label=\"val acc\"); plt.plot(history[\"val_f1\"], label=\"val macro-F1\")\n",
        "plt.title(\"Validation\"); plt.xlabel(\"epoch\"); plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"val_metrics.png\"))"
      ],
      "metadata": {
        "id": "icSkKdl1y7lo"
      },
      "id": "icSkKdl1y7lo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 10 — Best checkpoint & final reports\n",
        "# =========================================\n",
        "best_ckpt = torch.load(Path(OUT_DIR) / \"best_model.pth\", map_location=device)\n",
        "model.load_state_dict(best_ckpt[\"state_dict\"])\n",
        "\n",
        "# VAL\n",
        "val_acc, val_f1, y_true_v, y_pred_v = evaluate(model, val_loader)\n",
        "print(f\"[VAL]  acc={val_acc:.4f}  macro-F1={val_f1:.4f}\")\n",
        "\n",
        "fig, cm = confusion_matrix_fig(y_true_v, y_pred_v, \"Confusion Matrix (VAL)\", idx2str)\n",
        "plt.savefig(os.path.join(OUT_DIR, \"cm_val.png\")); plt.show()\n",
        "\n",
        "rep_val = classification_report(y_true_v, y_pred_v,\n",
        "                                target_names=[f\"{i}-{n}\" for i,n in enumerate(idx2str)],\n",
        "                                zero_division=0, digits=4)\n",
        "print(rep_val)\n",
        "Path(os.path.join(OUT_DIR, \"val_classification_report.txt\")).write_text(rep_val)\n",
        "pd.DataFrame({\"y_true\": y_true_v, \"y_pred\": y_pred_v}).to_csv(os.path.join(OUT_DIR, \"val_predictions.csv\"), index=False)\n",
        "\n",
        "# TEST (if provided)\n",
        "if test_loader:\n",
        "    test_acc, test_f1, y_true_t, y_pred_t = evaluate(model, test_loader)\n",
        "    print(f\"[TEST] acc={test_acc:.4f}  macro-F1={test_f1:.4f}\")\n",
        "    fig, cm = confusion_matrix_fig(y_true_t, y_pred_t, \"Confusion Matrix (TEST)\", idx2str)\n",
        "    plt.savefig(os.path.join(OUT_DIR, \"cm_test.png\")); plt.show()\n",
        "    rep_test = classification_report(y_true_t, y_pred_t,\n",
        "                                     target_names=[f\"{i}-{n}\" for i,n in enumerate(idx2str)],\n",
        "                                     zero_division=0, digits=4)\n",
        "    print(rep_test)\n",
        "    Path(os.path.join(OUT_DIR, \"test_classification_report.txt\")).write_text(rep_test)\n",
        "    pd.DataFrame({\"y_true\": y_true_t, \"y_pred\": y_pred_t}).to_csv(os.path.join(OUT_DIR, \"test_predictions.csv\"), index=False)\n",
        "\n",
        "print(\"Outputs saved to:\", OUT_DIR)"
      ],
      "metadata": {
        "id": "CE8TSL7Iy9Do"
      },
      "id": "CE8TSL7Iy9Do",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 11 — (Optional) Export to ONNX\n",
        "# =========================================\n",
        "onnx_path = os.path.join(OUT_DIR, \"resnet18_emotions.onnx\")\n",
        "model.eval()\n",
        "dummy = torch.randn(1, 3, IMG_SIZE, IMG_SIZE, device=device)\n",
        "torch.onnx.export(model, dummy, onnx_path,\n",
        "                  input_names=[\"input\"], output_names=[\"logits\"],\n",
        "                  dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
        "                  opset_version=12)\n",
        "print(\"Saved ONNX:\", onnx_path)"
      ],
      "metadata": {
        "id": "btobJbbCy-qP"
      },
      "id": "btobJbbCy-qP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c43c7741"
      },
      "source": [
        "# Install the onnx package\n",
        "!pip install onnx"
      ],
      "id": "c43c7741",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z2pqU6JeC7Nu"
      },
      "id": "Z2pqU6JeC7Nu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}