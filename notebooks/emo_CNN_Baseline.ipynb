{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahirumanulanka/ann-visual-emotion/blob/new-dataset/notebooks/emo_CNN_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install torch torchvision torchaudio scikit-learn matplotlib pillow\n",
        "import os, math, random, copy, numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split\n",
        "from torchvision import transforms, models, datasets\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gyjjI8Bc5tVj"
      },
      "id": "gyjjI8Bc5tVj",
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); print(\"Device:\", device)\n",
        "SEED=42; random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc36o7_3RzNg",
        "outputId": "17a9cbf6-41ff-4ec5-bc80-75d17f3c23f5"
      },
      "id": "qc36o7_3RzNg",
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prefilled with your uploaded CSVs and label_map in /mnt/data\n",
        "TRAIN_CSV = \"/content/ann-visual-emotion/data/processed/EmoSet_splits/train.csv\"\n",
        "VAL_CSV   = \"/content/ann-visual-emotion/data/processed/EmoSet_splits/val.csv\"\n",
        "TEST_CSV  = \"/content/ann-visual-emotion/data/processed/EmoSet_splits/test.csv\"\n",
        "LABEL_MAP = \"/content/ann-visual-emotion/data/processed/EmoSet_splits/label_map.json\"\n",
        "IMAGES_ROOT = \"/content/ann-visual-emotion/data/raw/EmoSet\"\n",
        "\n",
        "MODEL   = \"resnet18\"      # or \"simple_cnn\"\n",
        "EPOCHS  = 30\n",
        "BATCH   = 128\n",
        "LR      = 3e-4\n",
        "IMG_SZ  = 224 if MODEL==\"resnet18\" else 96\n",
        "SAVE_DIR= \"./checkpoints_csv_prefilled\"\n",
        "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "6sEQBfN2R0at"
      },
      "id": "6sEQBfN2R0at",
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_label_map(p):\n",
        "    with open(p,\"r\") as f: lm = json.load(f)\n",
        "    str2idx = dict(lm); idx2str = {v:k for k,v in str2idx.items()}\n",
        "    return str2idx, idx2str\n",
        "\n",
        "def find_col(cols, cands):\n",
        "    cols_lower=[c.lower() for c in cols]\n",
        "    for cand in cands:\n",
        "        if cand in cols_lower: return cols[cols_lower.index(cand)]\n",
        "    raise ValueError(f\"Need one of {cands}, have {cols}\")\n",
        "\n",
        "class CSVDataset(Dataset):\n",
        "    def __init__(self, csv_path, label_map, images_root=\"\", transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.transform=transform; self.images_root=images_root; self.label_map=label_map\n",
        "        self.path_col  = find_col(self.df.columns, [\"path\",\"filepath\",\"image\",\"img\",\"file\"])\n",
        "        self.label_col = find_col(self.df.columns, [\"label\",\"emotion\",\"class\"])\n",
        "        self.df[self.label_col]=self.df[self.label_col].astype(str)\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        rel = str(row[self.path_col]); path = rel if os.path.isabs(rel) else os.path.join(self.images_root, rel)\n",
        "        x = Image.open(path).convert(\"L\")\n",
        "        y = self.label_map[str(row[self.label_col])]\n",
        "        if self.transform: x = self.transform(x)\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "pF0MenOqU2vD"
      },
      "id": "pF0MenOqU2vD",
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transforms(img_size=224, model=\"resnet18\", img_mode=\"L\"):\n",
        "    # Ensure the output is 1 channel if img_mode is \"L\"\n",
        "    if img_mode == \"L\":\n",
        "        train_tf = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=1), # Ensure 1 output channel\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            # ColorJitter is typically for RGB, consider removing or adjusting for grayscale\n",
        "            # transforms.ColorJitter(brightness=0.15, contrast=0.15),\n",
        "            transforms.ToTensor(),\n",
        "            # Normalize for 1 channel\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])\n",
        "        eval_tf = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=1), # Ensure 1 output channel\n",
        "            transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            # Normalize for 1 channel\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])\n",
        "    else: # Assuming RGB\n",
        "        train_tf = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=3), # Keep 3 channels for RGB if needed\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.15, contrast=0.15),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "        eval_tf = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=3), # Keep 3 channels for RGB if needed\n",
        "            transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "    return train_tf, eval_tf\n",
        "\n",
        "# Assuming LABEL_MAP, IMG_SZ, MODEL, IMAGES_ROOT, TRAIN_CSV, VAL_CSV, TEST_CSV, BATCH are defined in previous cells\n",
        "str2idx, idx2str = read_label_map(LABEL_MAP)\n",
        "classes = [k for k,_ in sorted(str2idx.items(), key=lambda kv: kv[1])]\n",
        "\n",
        "# Pass IMG_MODE to build_transforms\n",
        "train_tf, eval_tf = build_transforms(IMG_SZ, MODEL, IMG_MODE)\n",
        "\n",
        "ds_tr = CSVDataset(TRAIN_CSV, str2idx, IMAGES_ROOT, transform=train_tf)\n",
        "ds_va = CSVDataset(VAL_CSV,   str2idx, IMAGES_ROOT, transform=eval_tf)\n",
        "ds_te = CSVDataset(TEST_CSV,  str2idx, IMAGES_ROOT, transform=eval_tf)\n",
        "\n",
        "import torch\n",
        "counts = torch.zeros(len(classes))\n",
        "for y in ds_tr.df[ds_tr.label_col].astype(str):\n",
        "    counts[str2idx[y]] += 1\n",
        "wpc = 1.0/torch.clamp(counts, min=1.0)\n",
        "sample_weights = [wpc[str2idx[y]].item() for y in ds_tr.df[ds_tr.label_col].astype(str)]\n",
        "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "tr_loader = DataLoader(ds_tr, batch_size=BATCH, sampler=sampler, num_workers=4, pin_memory=True)\n",
        "va_loader = DataLoader(ds_va, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "te_loader = DataLoader(ds_te, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "RSLJOWx7R1yU"
      },
      "id": "RSLJOWx7R1yU",
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, in_ch=1, num_classes=7, dropout=0.3):\n",
        "        super().__init__()\n",
        "        c=32\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_ch,c,3,1,1), nn.BatchNorm2d(c), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(c,c*2,3,1,1),   nn.BatchNorm2d(c*2), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(c*2,c*4,3,1,1), nn.BatchNorm2d(c*4), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        )\n",
        "        h=IMG_SZ//8\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(c*4*h*h,256), nn.ReLU(),\n",
        "            nn.Dropout(0.3), nn.Linear(256,len(classes))\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x=self.features(x); x=torch.flatten(x,1); return self.head(x)\n",
        "\n",
        "def build_resnet18(num_classes, in_ch=1, pretrained=True, dropout=0.2):\n",
        "    m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "    if in_ch==1:\n",
        "        w=m.conv1.weight; m.conv1 = nn.Conv2d(1,64,7,2,3,bias=False)\n",
        "        if pretrained:\n",
        "            with torch.no_grad(): m.conv1.weight[:] = w.mean(dim=1, keepdim=True)\n",
        "    m.fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(m.fc.in_features, num_classes))\n",
        "    return m\n",
        "\n",
        "net = build_resnet18(len(classes), in_ch=1, pretrained=True, dropout=0.2) if MODEL==\"resnet18\" else SimpleCNN(in_ch=1, num_classes=len(classes))\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkNuu49VR3dk",
        "outputId": "368941af-f68c-40f9-99af-e07c0ca5dc25"
      },
      "id": "jkNuu49VR3dk",
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=512, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
        "optimizer = optim.AdamW(net.parameters(), lr=LR, weight_decay=1e-4)\n",
        "steps_per_epoch = max(1, int(len(tr_loader.dataset)/BATCH))\n",
        "scheduler = OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch) if MODEL==\"resnet18\" else CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train(); total_loss=total_correct=total=0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
        "            out=model(imgs); loss=criterion(out, labels)\n",
        "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
        "        total_loss += loss.item()*labels.size(0)\n",
        "        total_correct += (out.argmax(1)==labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return total_loss/total, total_correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval(); total_loss=total_correct=total=0; all_p, all_y=[], []\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out=model(imgs); loss=criterion(out, labels)\n",
        "        total_loss += loss.item()*labels.size(0)\n",
        "        total_correct += (out.argmax(1)==labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        all_p.append(out.argmax(1).cpu()); all_y.append(labels.cpu())\n",
        "    all_p=torch.cat(all_p).numpy(); all_y=torch.cat(all_y).numpy()\n",
        "    f1 = __import__('sklearn.metrics').metrics.f1_score(all_y, all_p, average='macro')\n",
        "    return total_loss/total, total_correct/total, f1, all_y, all_p"
      ],
      "metadata": {
        "id": "v_oQFtJzVXen"
      },
      "id": "v_oQFtJzVXen",
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_f1, best_state = -1.0, None; patience, bad = 8, 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(net, tr_loader)\n",
        "    va_loss, va_acc, va_f1, y_true, y_pred = evaluate(net, va_loader)\n",
        "    try: scheduler.step()\n",
        "    except: pass\n",
        "    print(f\"Epoch {ep:02d} | tr_loss {tr_loss:.4f} acc {tr_acc:.3f} | val_loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f}\")\n",
        "    if va_f1>best_f1:\n",
        "        best_f1, bad = va_f1, 0\n",
        "        best_state = copy.deepcopy(net.state_dict())\n",
        "        torch.save({'state_dict': best_state, 'classes': classes}, f\"{SAVE_DIR}/best.pt\")\n",
        "    else:\n",
        "        bad+=1\n",
        "        if bad>=8:\n",
        "            print(\"Early stopping\"); break\n",
        "\n",
        "if best_state is not None: net.load_state_dict(best_state)\n",
        "te_loss, te_acc, te_f1, y_true, y_pred = evaluate(net, te_loader)\n",
        "print(f\"TEST | loss {te_loss:.4f} acc {te_acc:.3f} f1 {te_f1:.3f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "3c18yQi4VbV5",
        "outputId": "c41b8336-4aa8-4af5-d04a-f5fb7fd9c27c"
      },
      "id": "3c18yQi4VbV5",
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | tr_loss 0.7037 acc 0.820 | val_loss 1.0589 acc 0.673 f1 0.661\n",
            "Epoch 02 | tr_loss 0.6958 acc 0.825 | val_loss 1.0557 acc 0.674 f1 0.661\n",
            "Epoch 03 | tr_loss 0.6850 acc 0.831 | val_loss 1.0591 acc 0.671 f1 0.660\n",
            "Epoch 04 | tr_loss 0.6657 acc 0.841 | val_loss 1.0696 acc 0.671 f1 0.661\n",
            "Epoch 05 | tr_loss 0.6593 acc 0.842 | val_loss 1.0730 acc 0.675 f1 0.664\n",
            "Epoch 06 | tr_loss 0.6485 acc 0.850 | val_loss 1.0760 acc 0.675 f1 0.666\n",
            "Epoch 07 | tr_loss 0.6390 acc 0.852 | val_loss 1.0743 acc 0.674 f1 0.661\n",
            "Epoch 08 | tr_loss 0.6302 acc 0.857 | val_loss 1.0852 acc 0.672 f1 0.659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bb9e6576980>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1628, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1651947376.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mva_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4205381524.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
        "fig, ax = plt.subplots(figsize=(6,5))\n",
        "im = ax.imshow(cm, aspect='auto')\n",
        "ax.set_title(\"FER Confusion Matrix (Test)\")\n",
        "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
        "ax.set_xticks(range(len(classes))); ax.set_yticks(range(len(classes)))\n",
        "ax.set_xticklabels(classes, rotation=45, ha='right'); ax.set_yticklabels(classes)\n",
        "for i in range(len(classes)):\n",
        "    for j in range(len(classes)):\n",
        "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "fig.colorbar(im, ax); fig.tight_layout()\n",
        "fig.savefig(f\"{SAVE_DIR}/confusion_matrix.png\", dpi=160)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "VjKw2FG-VdIc",
        "outputId": "b368ccce-5f26-4a89-b948-c98f99bd9fae"
      },
      "id": "VjKw2FG-VdIc",
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OqaFXUXtcGrG"
      },
      "id": "OqaFXUXtcGrG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "493ef694"
      },
      "source": [
        "# Task\n",
        "Improve the accuracy of the image classification model by exploring different hyperparameter settings, data augmentation techniques, model architectures, training schedules, regularization methods, and loss functions."
      ],
      "id": "493ef694"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0919c113"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Experiment with different learning rates, batch sizes, optimizers, and weight decay values.\n"
      ],
      "id": "0919c113"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dff79d7c"
      },
      "source": [
        "**Reasoning**:\n",
        "I will modify the hyperparameters (LR, BATCH, WEIGHT_DECAY) and the optimizer, then run the training loop to observe the impact on performance.\n",
        "\n"
      ],
      "id": "dff79d7c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6c50db6",
        "outputId": "1fd1c1fb-3087-44a7-b2d2-47b71cd4cf0a"
      },
      "source": [
        "# Experiment with different hyperparameters\n",
        "LR = 1e-4  # Reduced learning rate\n",
        "BATCH = 64 # Reduced batch size\n",
        "WEIGHT_DECAY = 5e-4 # Increased weight decay\n",
        "\n",
        "# Try a different optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Rebuild the dataloaders with the new batch size\n",
        "tr_loader = DataLoader(ds_tr, batch_size=BATCH, sampler=sampler, num_workers=4, pin_memory=True)\n",
        "va_loader = DataLoader(ds_va, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "te_loader = DataLoader(ds_te, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Re-define the scheduler with the potentially new steps_per_epoch\n",
        "steps_per_epoch = max(1, int(len(tr_loader.dataset)/BATCH))\n",
        "scheduler = OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch) if MODEL==\"resnet18\" else CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# Rerun the training loop\n",
        "best_f1, best_state = -1.0, None; patience, bad = 8, 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(net, tr_loader)\n",
        "    va_loss, va_acc, va_f1, y_true, y_pred = evaluate(net, va_loader)\n",
        "    try: scheduler.step()\n",
        "    except: pass\n",
        "    print(f\"Epoch {ep:02d} | tr_loss {tr_loss:.4f} acc {tr_acc:.3f} | val_loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f}\")\n",
        "    if va_f1>best_f1:\n",
        "        best_f1, bad = va_f1, 0\n",
        "        best_state = copy.deepcopy(net.state_dict())\n",
        "        torch.save({'state_dict': best_state, 'classes': classes}, f\"{SAVE_DIR}/best.pt\")\n",
        "    else:\n",
        "        bad+=1\n",
        "        if bad>=8:\n",
        "            print(\"Early stopping\"); break\n",
        "\n",
        "if best_state is not None: net.load_state_dict(best_state)\n",
        "te_loss, te_acc, te_f1, y_true, y_pred = evaluate(net, te_loader)\n",
        "print(f\"TEST | loss {te_loss:.4f} acc {te_acc:.3f} f1 {te_f1:.3f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ],
      "id": "b6c50db6",
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | tr_loss 0.6291 acc 0.858 | val_loss 1.0917 acc 0.675 f1 0.665\n",
            "Epoch 02 | tr_loss 0.6233 acc 0.860 | val_loss 1.0909 acc 0.680 f1 0.666\n",
            "Epoch 03 | tr_loss 0.6310 acc 0.857 | val_loss 1.0886 acc 0.677 f1 0.666\n",
            "Epoch 04 | tr_loss 0.6233 acc 0.861 | val_loss 1.0885 acc 0.677 f1 0.662\n",
            "Epoch 05 | tr_loss 0.6218 acc 0.862 | val_loss 1.0832 acc 0.679 f1 0.666\n",
            "Epoch 06 | tr_loss 0.6194 acc 0.863 | val_loss 1.0889 acc 0.676 f1 0.665\n",
            "Epoch 07 | tr_loss 0.6189 acc 0.862 | val_loss 1.0943 acc 0.677 f1 0.658\n",
            "Epoch 08 | tr_loss 0.6194 acc 0.861 | val_loss 1.0843 acc 0.681 f1 0.671\n",
            "Epoch 09 | tr_loss 0.6187 acc 0.862 | val_loss 1.0879 acc 0.680 f1 0.670\n",
            "Epoch 10 | tr_loss 0.6171 acc 0.864 | val_loss 1.0881 acc 0.678 f1 0.668\n",
            "Epoch 11 | tr_loss 0.6212 acc 0.860 | val_loss 1.0952 acc 0.675 f1 0.663\n",
            "Epoch 12 | tr_loss 0.6171 acc 0.862 | val_loss 1.0847 acc 0.681 f1 0.670\n",
            "Epoch 13 | tr_loss 0.6162 acc 0.863 | val_loss 1.0822 acc 0.682 f1 0.669\n",
            "Epoch 14 | tr_loss 0.6185 acc 0.862 | val_loss 1.0854 acc 0.680 f1 0.671\n",
            "Epoch 15 | tr_loss 0.6102 acc 0.867 | val_loss 1.0902 acc 0.682 f1 0.675\n",
            "Epoch 16 | tr_loss 0.6115 acc 0.865 | val_loss 1.0906 acc 0.680 f1 0.669\n",
            "Epoch 17 | tr_loss 0.6125 acc 0.867 | val_loss 1.0929 acc 0.677 f1 0.663\n",
            "Epoch 18 | tr_loss 0.6132 acc 0.866 | val_loss 1.0888 acc 0.678 f1 0.667\n",
            "Epoch 19 | tr_loss 0.6164 acc 0.863 | val_loss 1.0953 acc 0.676 f1 0.666\n",
            "Epoch 20 | tr_loss 0.6121 acc 0.865 | val_loss 1.0901 acc 0.678 f1 0.665\n",
            "Epoch 21 | tr_loss 0.6115 acc 0.865 | val_loss 1.0900 acc 0.677 f1 0.667\n",
            "Epoch 22 | tr_loss 0.6096 acc 0.866 | val_loss 1.0873 acc 0.680 f1 0.669\n",
            "Epoch 23 | tr_loss 0.6049 acc 0.868 | val_loss 1.0910 acc 0.677 f1 0.666\n",
            "Early stopping\n",
            "TEST | loss 1.0973 acc 0.666 f1 0.653\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.59      0.56      0.57       192\n",
            "     disgust       0.74      0.64      0.68        22\n",
            "        fear       0.50      0.50      0.50       204\n",
            "       happy       0.89      0.85      0.87       365\n",
            "     neutral       0.66      0.62      0.64       243\n",
            "         sad       0.51      0.58      0.54       228\n",
            "    surprise       0.76      0.79      0.77       159\n",
            "\n",
            "    accuracy                           0.67      1413\n",
            "   macro avg       0.66      0.65      0.65      1413\n",
            "weighted avg       0.67      0.67      0.67      1413\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef9d03f9"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "### Subtask:\n",
        "Implement more advanced data augmentation techniques beyond the current random horizontal flip and rotation to make the model more robust.\n"
      ],
      "id": "ef9d03f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cb11420"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `build_transforms` function to include more advanced data augmentation techniques suitable for grayscale images if IMG_MODE is \"L\", otherwise for RGB images.\n",
        "\n"
      ],
      "id": "1cb11420"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "799287bb",
        "outputId": "84cce607-ca35-44dd-a76c-8db5a61a8562"
      },
      "source": [
        "def build_transforms(img_size=224, model=\"resnet18\", img_mode=\"L\"):\n",
        "    # Ensure the output is 1 channel if img_mode is \"L\"\n",
        "    if img_mode == \"L\":\n",
        "        train_tf = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=1), # Ensure 1 output channel\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.7,1.0), ratio=(0.75, 1.333333)), # More aggressive cropping\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(20), # Increased rotation range\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)), # Add affine transformations\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2), # Add color jitter for grayscale\n",
        "            transforms.ToTensor(),\n",
        "            # Normalize for 1 channel\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])\n",
        "        eval_tf = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=1), # Ensure 1 output channel\n",
        "            transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            # Normalize for 1 channel\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])\n",
        "    else: # Assuming RGB\n",
        "        train_tf = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=3), # Keep 3 channels for RGB if needed\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.7,1.0), ratio=(0.75, 1.333333)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(20),\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "        eval_tf = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=3), # Keep 3 channels for RGB if needed\n",
        "            transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "    return train_tf, eval_tf\n",
        "\n",
        "# Re-instantiate datasets and dataloaders with the updated transforms\n",
        "# Assuming LABEL_MAP, IMG_SZ, MODEL, IMAGES_ROOT, TRAIN_CSV, VAL_CSV, TEST_CSV, BATCH are defined in previous cells\n",
        "str2idx, idx2str = read_label_map(LABEL_MAP)\n",
        "classes = [k for k,_ in sorted(str2idx.items(), key=lambda kv: kv[1])]\n",
        "\n",
        "# Pass IMG_MODE to build_transforms, assuming IMG_MODE is defined\n",
        "# Define IMG_MODE if it's not defined in previous cells\n",
        "try: IMG_MODE\n",
        "except NameError: IMG_MODE = \"L\" # Default to grayscale if not defined\n",
        "\n",
        "train_tf, eval_tf = build_transforms(IMG_SZ, MODEL, IMG_MODE)\n",
        "\n",
        "ds_tr = CSVDataset(TRAIN_CSV, str2idx, IMAGES_ROOT, transform=train_tf)\n",
        "ds_va = CSVDataset(VAL_CSV,   str2idx, IMAGES_ROOT, transform=eval_tf)\n",
        "ds_te = CSVDataset(TEST_CSV,  str2idx, IMAGES_ROOT, transform=eval_tf)\n",
        "\n",
        "# Rebuild sampler for the training dataset\n",
        "counts = torch.zeros(len(classes))\n",
        "for y in ds_tr.df[ds_tr.label_col].astype(str):\n",
        "    counts[str2idx[y]] += 1\n",
        "wpc = 1.0/torch.clamp(counts, min=1.0)\n",
        "sample_weights = [wpc[str2idx[y]].item() for y in ds_tr.df[ds_tr.label_col].astype(str)]\n",
        "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# Rebuild dataloaders\n",
        "tr_loader = DataLoader(ds_tr, batch_size=BATCH, sampler=sampler, num_workers=4, pin_memory=True)\n",
        "va_loader = DataLoader(ds_va, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "te_loader = DataLoader(ds_te, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Rerun the training loop\n",
        "best_f1, best_state = -1.0, None; patience, bad = 8, 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(net, tr_loader)\n",
        "    va_loss, va_acc, va_f1, y_true, y_pred = evaluate(net, va_loader)\n",
        "    try: scheduler.step()\n",
        "    except: pass\n",
        "    print(f\"Epoch {ep:02d} | tr_loss {tr_loss:.4f} acc {tr_acc:.3f} | val_loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f}\")\n",
        "    if va_f1>best_f1:\n",
        "        best_f1, bad = va_f1, 0\n",
        "        best_state = copy.deepcopy(net.state_dict())\n",
        "        torch.save({'state_dict': best_state, 'classes': classes}, f\"{SAVE_DIR}/best.pt\")\n",
        "    else:\n",
        "        bad+=1\n",
        "        if bad>=8:\n",
        "            print(\"Early stopping\"); break\n",
        "\n",
        "if best_state is not None: net.load_state_dict(best_state)\n",
        "te_loss, te_acc, te_f1, y_true, y_pred = evaluate(net, te_loader)\n",
        "print(f\"TEST | loss {te_loss:.4f} acc {te_acc:.3f} f1 {te_f1:.3f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ],
      "id": "799287bb",
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | tr_loss 0.8909 acc 0.738 | val_loss 1.1005 acc 0.665 f1 0.646\n",
            "Epoch 02 | tr_loss 0.8708 acc 0.745 | val_loss 1.1095 acc 0.663 f1 0.637\n",
            "Epoch 03 | tr_loss 0.8776 acc 0.742 | val_loss 1.1069 acc 0.663 f1 0.639\n",
            "Epoch 04 | tr_loss 0.8693 acc 0.742 | val_loss 1.0896 acc 0.671 f1 0.649\n",
            "Epoch 05 | tr_loss 0.8552 acc 0.750 | val_loss 1.0991 acc 0.668 f1 0.644\n",
            "Epoch 06 | tr_loss 0.8518 acc 0.755 | val_loss 1.0927 acc 0.669 f1 0.651\n",
            "Epoch 07 | tr_loss 0.8616 acc 0.753 | val_loss 1.0889 acc 0.672 f1 0.651\n",
            "Epoch 08 | tr_loss 0.8611 acc 0.748 | val_loss 1.0875 acc 0.674 f1 0.652\n",
            "Epoch 09 | tr_loss 0.8535 acc 0.750 | val_loss 1.0895 acc 0.674 f1 0.652\n",
            "Epoch 10 | tr_loss 0.8437 acc 0.757 | val_loss 1.0856 acc 0.673 f1 0.657\n",
            "Epoch 11 | tr_loss 0.8482 acc 0.753 | val_loss 1.0912 acc 0.673 f1 0.650\n",
            "Epoch 12 | tr_loss 0.8475 acc 0.755 | val_loss 1.0891 acc 0.672 f1 0.650\n",
            "Epoch 13 | tr_loss 0.8445 acc 0.759 | val_loss 1.0936 acc 0.672 f1 0.648\n",
            "Epoch 14 | tr_loss 0.8438 acc 0.755 | val_loss 1.0817 acc 0.676 f1 0.658\n",
            "Epoch 15 | tr_loss 0.8485 acc 0.756 | val_loss 1.0853 acc 0.674 f1 0.653\n",
            "Epoch 16 | tr_loss 0.8402 acc 0.759 | val_loss 1.0845 acc 0.677 f1 0.660\n",
            "Epoch 17 | tr_loss 0.8400 acc 0.759 | val_loss 1.0821 acc 0.674 f1 0.658\n",
            "Epoch 18 | tr_loss 0.8343 acc 0.759 | val_loss 1.0816 acc 0.676 f1 0.656\n",
            "Epoch 19 | tr_loss 0.8461 acc 0.752 | val_loss 1.0804 acc 0.675 f1 0.656\n",
            "Epoch 20 | tr_loss 0.8293 acc 0.765 | val_loss 1.0842 acc 0.677 f1 0.655\n",
            "Epoch 21 | tr_loss 0.8394 acc 0.761 | val_loss 1.0721 acc 0.679 f1 0.664\n",
            "Epoch 22 | tr_loss 0.8415 acc 0.757 | val_loss 1.0778 acc 0.676 f1 0.656\n",
            "Epoch 23 | tr_loss 0.8335 acc 0.762 | val_loss 1.0821 acc 0.675 f1 0.655\n",
            "Epoch 24 | tr_loss 0.8368 acc 0.759 | val_loss 1.0798 acc 0.676 f1 0.663\n",
            "Epoch 25 | tr_loss 0.8337 acc 0.763 | val_loss 1.0840 acc 0.671 f1 0.654\n",
            "Epoch 26 | tr_loss 0.8341 acc 0.763 | val_loss 1.0770 acc 0.679 f1 0.659\n",
            "Epoch 27 | tr_loss 0.8296 acc 0.764 | val_loss 1.0716 acc 0.678 f1 0.661\n",
            "Epoch 28 | tr_loss 0.8253 acc 0.767 | val_loss 1.0763 acc 0.679 f1 0.662\n",
            "Epoch 29 | tr_loss 0.8266 acc 0.764 | val_loss 1.0748 acc 0.678 f1 0.664\n",
            "Epoch 30 | tr_loss 0.8280 acc 0.763 | val_loss 1.0777 acc 0.676 f1 0.657\n",
            "TEST | loss 1.0770 acc 0.665 f1 0.648\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.58      0.59      0.58       192\n",
            "     disgust       0.70      0.64      0.67        22\n",
            "        fear       0.49      0.48      0.49       204\n",
            "       happy       0.90      0.84      0.87       365\n",
            "     neutral       0.60      0.67      0.64       243\n",
            "         sad       0.54      0.49      0.52       228\n",
            "    surprise       0.74      0.82      0.78       159\n",
            "\n",
            "    accuracy                           0.67      1413\n",
            "   macro avg       0.65      0.65      0.65      1413\n",
            "weighted avg       0.67      0.67      0.66      1413\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75eabb66"
      },
      "source": [
        "## Model architecture\n",
        "\n",
        "### Subtask:\n",
        "Explore different model architectures or variations of the current ResNet-18 model, potentially using pre-trained weights differently or trying a larger model.\n"
      ],
      "id": "75eabb66"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32a306ee"
      },
      "source": [
        "**Reasoning**:\n",
        "I will modify the `build_resnet18` function to allow building ResNet34 and ResNet50 models, adapt the input layer for grayscale images, and then instantiate a ResNet34 model and rerun the training loop to evaluate its performance.\n",
        "\n"
      ],
      "id": "32a306ee"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4470421d",
        "outputId": "205177a8-060e-4a01-9ed7-0a0248df78bf"
      },
      "source": [
        "def build_resnet(model_name, num_classes, in_ch=1, pretrained=True, dropout=0.2):\n",
        "    if model_name == \"resnet18\":\n",
        "        m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "    elif model_name == \"resnet34\":\n",
        "        m = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
        "    elif model_name == \"resnet50\":\n",
        "        m = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} not supported\")\n",
        "\n",
        "    if in_ch == 1:\n",
        "        w = m.conv1.weight\n",
        "        m.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        if pretrained:\n",
        "            with torch.no_grad():\n",
        "                m.conv1.weight[:] = w.mean(dim=1, keepdim=True)\n",
        "\n",
        "    m.fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(m.fc.in_features, num_classes))\n",
        "    return m\n",
        "\n",
        "# Use ResNet34\n",
        "MODEL_ARCH = \"resnet34\" # Changed from MODEL\n",
        "net = build_resnet(MODEL_ARCH, len(classes), in_ch=1, pretrained=True, dropout=0.2)\n",
        "net.to(device)\n",
        "\n",
        "# Re-define optimizer and scheduler for the new model parameters\n",
        "optimizer = optim.AdamW(net.parameters(), lr=LR, weight_decay=1e-4) # Using AdamW again with potentially updated LR\n",
        "steps_per_epoch = max(1, int(len(tr_loader.dataset)/BATCH))\n",
        "scheduler = OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch) if MODEL_ARCH.startswith(\"resnet\") else CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# Rerun the training loop\n",
        "best_f1, best_state = -1.0, None; patience, bad = 8, 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(net, tr_loader)\n",
        "    va_loss, va_acc, va_f1, y_true, y_pred = evaluate(net, va_loader)\n",
        "    try: scheduler.step()\n",
        "    except: pass\n",
        "    print(f\"Epoch {ep:02d} | tr_loss {tr_loss:.4f} acc {tr_acc:.3f} | val_loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f}\")\n",
        "    if va_f1>best_f1:\n",
        "        best_f1, bad = va_f1, 0\n",
        "        best_state = copy.deepcopy(net.state_dict())\n",
        "        torch.save({'state_dict': best_state, 'classes': classes}, f\"{SAVE_DIR}/best.pt\")\n",
        "    else:\n",
        "        bad+=1\n",
        "        if bad>=8:\n",
        "            print(\"Early stopping\"); break\n",
        "\n",
        "if best_state is not None: net.load_state_dict(best_state)\n",
        "te_loss, te_acc, te_f1, y_true, y_pred = evaluate(net, te_loader)\n",
        "print(f\"TEST | loss {te_loss:.4f} acc {te_acc:.3f} f1 {te_f1:.3f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ],
      "id": "4470421d",
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 132MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | tr_loss 1.8898 acc 0.247 | val_loss 1.6902 acc 0.380 f1 0.324\n",
            "Epoch 02 | tr_loss 1.6404 acc 0.389 | val_loss 1.4969 acc 0.472 f1 0.399\n",
            "Epoch 03 | tr_loss 1.5046 acc 0.452 | val_loss 1.4156 acc 0.498 f1 0.437\n",
            "Epoch 04 | tr_loss 1.4268 acc 0.486 | val_loss 1.3233 acc 0.540 f1 0.477\n",
            "Epoch 05 | tr_loss 1.3482 acc 0.523 | val_loss 1.2711 acc 0.561 f1 0.506\n",
            "Epoch 06 | tr_loss 1.2937 acc 0.548 | val_loss 1.2453 acc 0.574 f1 0.522\n",
            "Epoch 07 | tr_loss 1.2519 acc 0.569 | val_loss 1.2164 acc 0.582 f1 0.528\n",
            "Epoch 08 | tr_loss 1.2106 acc 0.584 | val_loss 1.1715 acc 0.601 f1 0.553\n",
            "Epoch 09 | tr_loss 1.1833 acc 0.599 | val_loss 1.1383 acc 0.622 f1 0.579\n",
            "Epoch 10 | tr_loss 1.1532 acc 0.612 | val_loss 1.1266 acc 0.627 f1 0.585\n",
            "Epoch 11 | tr_loss 1.1242 acc 0.625 | val_loss 1.1263 acc 0.630 f1 0.585\n",
            "Epoch 12 | tr_loss 1.1033 acc 0.635 | val_loss 1.1084 acc 0.634 f1 0.599\n",
            "Epoch 13 | tr_loss 1.0784 acc 0.644 | val_loss 1.0978 acc 0.642 f1 0.604\n",
            "Epoch 14 | tr_loss 1.0709 acc 0.649 | val_loss 1.0883 acc 0.648 f1 0.611\n",
            "Epoch 15 | tr_loss 1.0455 acc 0.659 | val_loss 1.0845 acc 0.648 f1 0.612\n",
            "Epoch 16 | tr_loss 1.0298 acc 0.667 | val_loss 1.0761 acc 0.653 f1 0.624\n",
            "Epoch 17 | tr_loss 1.0263 acc 0.668 | val_loss 1.0666 acc 0.657 f1 0.629\n",
            "Epoch 18 | tr_loss 1.0058 acc 0.677 | val_loss 1.0649 acc 0.659 f1 0.631\n",
            "Epoch 19 | tr_loss 0.9906 acc 0.686 | val_loss 1.0577 acc 0.660 f1 0.634\n",
            "Epoch 20 | tr_loss 0.9861 acc 0.689 | val_loss 1.0468 acc 0.665 f1 0.643\n",
            "Epoch 21 | tr_loss 0.9737 acc 0.691 | val_loss 1.0447 acc 0.667 f1 0.642\n",
            "Epoch 22 | tr_loss 0.9515 acc 0.702 | val_loss 1.0364 acc 0.670 f1 0.647\n",
            "Epoch 23 | tr_loss 0.9510 acc 0.701 | val_loss 1.0357 acc 0.670 f1 0.645\n",
            "Epoch 24 | tr_loss 0.9470 acc 0.704 | val_loss 1.0376 acc 0.672 f1 0.648\n",
            "Epoch 25 | tr_loss 0.9349 acc 0.709 | val_loss 1.0358 acc 0.670 f1 0.649\n",
            "Epoch 26 | tr_loss 0.9263 acc 0.716 | val_loss 1.0291 acc 0.675 f1 0.655\n",
            "Epoch 27 | tr_loss 0.9089 acc 0.721 | val_loss 1.0311 acc 0.678 f1 0.661\n",
            "Epoch 28 | tr_loss 0.9115 acc 0.721 | val_loss 1.0283 acc 0.678 f1 0.656\n",
            "Epoch 29 | tr_loss 0.8967 acc 0.727 | val_loss 1.0244 acc 0.681 f1 0.661\n",
            "Epoch 30 | tr_loss 0.8931 acc 0.727 | val_loss 1.0236 acc 0.678 f1 0.663\n",
            "TEST | loss 1.0122 acc 0.675 f1 0.657\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.62      0.65      0.63       192\n",
            "     disgust       0.70      0.64      0.67        22\n",
            "        fear       0.52      0.42      0.46       204\n",
            "       happy       0.90      0.86      0.88       365\n",
            "     neutral       0.59      0.65      0.62       243\n",
            "         sad       0.54      0.57      0.56       228\n",
            "    surprise       0.77      0.80      0.78       159\n",
            "\n",
            "    accuracy                           0.68      1413\n",
            "   macro avg       0.66      0.66      0.66      1413\n",
            "weighted avg       0.68      0.68      0.67      1413\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0452349e"
      },
      "source": [
        "## Training schedule\n",
        "\n",
        "### Subtask:\n",
        "Adjust the learning rate scheduler or train for more epochs if the model is still improving.\n"
      ],
      "id": "0452349e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0358f4c"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the training loop to train for a higher number of epochs by changing the EPOCHS variable and rerun the training.\n",
        "\n"
      ],
      "id": "a0358f4c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16f66108",
        "outputId": "3af3a3a2-9864-4eae-9809-4bfaaab9767c"
      },
      "source": [
        "# Increase the number of training epochs\n",
        "EPOCHS = 50\n",
        "\n",
        "# Re-define the scheduler with the potentially new steps_per_epoch and epochs\n",
        "steps_per_epoch = max(1, int(len(tr_loader.dataset)/BATCH))\n",
        "scheduler = OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch) if MODEL_ARCH.startswith(\"resnet\") else CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# Rerun the training loop\n",
        "best_f1, best_state = -1.0, None; patience, bad = 8, 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(net, tr_loader)\n",
        "    va_loss, va_acc, va_f1, y_true, y_pred = evaluate(net, va_loader)\n",
        "    try: scheduler.step()\n",
        "    except: pass\n",
        "    print(f\"Epoch {ep:02d} | tr_loss {tr_loss:.4f} acc {tr_acc:.3f} | val_loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f}\")\n",
        "    if va_f1>best_f1:\n",
        "        best_f1, bad = va_f1, 0\n",
        "        best_state = copy.deepcopy(net.state_dict())\n",
        "        torch.save({'state_dict': best_state, 'classes': classes}, f\"{SAVE_DIR}/best.pt\")\n",
        "    else:\n",
        "        bad+=1\n",
        "        if bad>=8:\n",
        "            print(\"Early stopping\"); break\n",
        "\n",
        "if best_state is not None: net.load_state_dict(best_state)\n",
        "te_loss, te_acc, te_f1, y_true, y_pred = evaluate(net, te_loader)\n",
        "print(f\"TEST | loss {te_loss:.4f} acc {te_acc:.3f} f1 {te_f1:.3f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ],
      "id": "16f66108",
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | tr_loss 0.8820 acc 0.734 | val_loss 1.0161 acc 0.683 f1 0.667\n",
            "Epoch 02 | tr_loss 0.8804 acc 0.735 | val_loss 1.0139 acc 0.683 f1 0.664\n",
            "Epoch 03 | tr_loss 0.8613 acc 0.744 | val_loss 1.0138 acc 0.685 f1 0.666\n",
            "Epoch 04 | tr_loss 0.8545 acc 0.749 | val_loss 1.0178 acc 0.682 f1 0.661\n",
            "Epoch 05 | tr_loss 0.8524 acc 0.747 | val_loss 1.0213 acc 0.680 f1 0.663\n",
            "Epoch 06 | tr_loss 0.8453 acc 0.750 | val_loss 1.0166 acc 0.684 f1 0.668\n",
            "Epoch 07 | tr_loss 0.8332 acc 0.758 | val_loss 1.0124 acc 0.690 f1 0.674\n",
            "Epoch 08 | tr_loss 0.8332 acc 0.755 | val_loss 1.0174 acc 0.687 f1 0.672\n",
            "Epoch 09 | tr_loss 0.8218 acc 0.762 | val_loss 1.0184 acc 0.688 f1 0.672\n",
            "Epoch 10 | tr_loss 0.8132 acc 0.766 | val_loss 1.0147 acc 0.691 f1 0.673\n",
            "Epoch 11 | tr_loss 0.8031 acc 0.772 | val_loss 1.0180 acc 0.691 f1 0.680\n",
            "Epoch 12 | tr_loss 0.8017 acc 0.773 | val_loss 1.0147 acc 0.689 f1 0.678\n",
            "Epoch 13 | tr_loss 0.7932 acc 0.776 | val_loss 1.0135 acc 0.694 f1 0.684\n",
            "Epoch 14 | tr_loss 0.7845 acc 0.780 | val_loss 1.0205 acc 0.690 f1 0.674\n",
            "Epoch 15 | tr_loss 0.7831 acc 0.780 | val_loss 1.0143 acc 0.694 f1 0.680\n",
            "Epoch 16 | tr_loss 0.7724 acc 0.783 | val_loss 1.0133 acc 0.694 f1 0.683\n",
            "Epoch 17 | tr_loss 0.7624 acc 0.790 | val_loss 1.0192 acc 0.698 f1 0.688\n",
            "Epoch 18 | tr_loss 0.7581 acc 0.793 | val_loss 1.0183 acc 0.698 f1 0.691\n",
            "Epoch 19 | tr_loss 0.7554 acc 0.795 | val_loss 1.0203 acc 0.694 f1 0.682\n",
            "Epoch 20 | tr_loss 0.7471 acc 0.796 | val_loss 1.0217 acc 0.692 f1 0.682\n",
            "Epoch 21 | tr_loss 0.7414 acc 0.799 | val_loss 1.0292 acc 0.694 f1 0.680\n",
            "Epoch 22 | tr_loss 0.7292 acc 0.806 | val_loss 1.0361 acc 0.693 f1 0.683\n",
            "Epoch 23 | tr_loss 0.7253 acc 0.808 | val_loss 1.0274 acc 0.695 f1 0.682\n",
            "Epoch 24 | tr_loss 0.7265 acc 0.806 | val_loss 1.0275 acc 0.698 f1 0.691\n",
            "Epoch 25 | tr_loss 0.7041 acc 0.817 | val_loss 1.0291 acc 0.702 f1 0.686\n",
            "Epoch 26 | tr_loss 0.7126 acc 0.812 | val_loss 1.0307 acc 0.699 f1 0.687\n",
            "Epoch 27 | tr_loss 0.6988 acc 0.819 | val_loss 1.0351 acc 0.697 f1 0.682\n",
            "Epoch 28 | tr_loss 0.6972 acc 0.819 | val_loss 1.0291 acc 0.701 f1 0.691\n",
            "Epoch 29 | tr_loss 0.6911 acc 0.822 | val_loss 1.0393 acc 0.697 f1 0.687\n",
            "Epoch 30 | tr_loss 0.6834 acc 0.827 | val_loss 1.0367 acc 0.699 f1 0.689\n",
            "Epoch 31 | tr_loss 0.6740 acc 0.830 | val_loss 1.0453 acc 0.699 f1 0.692\n",
            "Epoch 32 | tr_loss 0.6710 acc 0.832 | val_loss 1.0436 acc 0.698 f1 0.689\n",
            "Epoch 33 | tr_loss 0.6654 acc 0.835 | val_loss 1.0493 acc 0.701 f1 0.689\n",
            "Epoch 34 | tr_loss 0.6604 acc 0.836 | val_loss 1.0588 acc 0.700 f1 0.690\n",
            "Epoch 35 | tr_loss 0.6581 acc 0.838 | val_loss 1.0530 acc 0.702 f1 0.691\n",
            "Epoch 36 | tr_loss 0.6525 acc 0.840 | val_loss 1.0583 acc 0.700 f1 0.691\n",
            "Epoch 37 | tr_loss 0.6460 acc 0.842 | val_loss 1.0582 acc 0.699 f1 0.689\n",
            "Epoch 38 | tr_loss 0.6473 acc 0.846 | val_loss 1.0581 acc 0.700 f1 0.693\n",
            "Epoch 39 | tr_loss 0.6380 acc 0.847 | val_loss 1.0570 acc 0.704 f1 0.693\n",
            "Epoch 40 | tr_loss 0.6253 acc 0.853 | val_loss 1.0554 acc 0.707 f1 0.696\n",
            "Epoch 41 | tr_loss 0.6237 acc 0.855 | val_loss 1.0691 acc 0.701 f1 0.693\n",
            "Epoch 42 | tr_loss 0.6210 acc 0.857 | val_loss 1.0678 acc 0.700 f1 0.688\n",
            "Epoch 43 | tr_loss 0.6139 acc 0.859 | val_loss 1.0730 acc 0.704 f1 0.695\n",
            "Epoch 44 | tr_loss 0.6127 acc 0.859 | val_loss 1.0758 acc 0.702 f1 0.693\n",
            "Epoch 45 | tr_loss 0.6074 acc 0.861 | val_loss 1.0781 acc 0.704 f1 0.694\n",
            "Epoch 46 | tr_loss 0.6069 acc 0.863 | val_loss 1.0814 acc 0.698 f1 0.688\n",
            "Epoch 47 | tr_loss 0.5956 acc 0.867 | val_loss 1.0763 acc 0.701 f1 0.691\n",
            "Epoch 48 | tr_loss 0.5871 acc 0.872 | val_loss 1.0832 acc 0.698 f1 0.690\n",
            "Early stopping\n",
            "TEST | loss 1.0387 acc 0.696 f1 0.686\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.63      0.65      0.64       192\n",
            "     disgust       0.88      0.64      0.74        22\n",
            "        fear       0.60      0.49      0.54       204\n",
            "       happy       0.88      0.88      0.88       365\n",
            "     neutral       0.61      0.67      0.64       243\n",
            "         sad       0.54      0.57      0.56       228\n",
            "    surprise       0.80      0.80      0.80       159\n",
            "\n",
            "    accuracy                           0.70      1413\n",
            "   macro avg       0.71      0.67      0.69      1413\n",
            "weighted avg       0.70      0.70      0.70      1413\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d68d5aaa"
      },
      "source": [
        "## Regularization\n",
        "\n",
        "### Subtask:\n",
        "Add or adjust regularization techniques like dropout or weight decay.\n"
      ],
      "id": "d68d5aaa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0283b15"
      },
      "source": [
        "**Reasoning**:\n",
        "Adjust the dropout rate in the model definition and the weight decay in the optimizer definition, then re-run the training loop.\n",
        "\n"
      ],
      "id": "e0283b15"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa56bf1d",
        "outputId": "faade1ea-2c50-4ab2-8586-7a4b796b4d0f"
      },
      "source": [
        "# 1. Adjust the dropout rate in the build_resnet function\n",
        "def build_resnet(model_name, num_classes, in_ch=1, pretrained=True, dropout=0.4): # Increased dropout to 0.4\n",
        "    if model_name == \"resnet18\":\n",
        "        m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "    elif model_name == \"resnet34\":\n",
        "        m = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
        "    elif model_name == \"resnet50\":\n",
        "        m = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} not supported\")\n",
        "\n",
        "    if in_ch == 1:\n",
        "        w = m.conv1.weight\n",
        "        m.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        if pretrained:\n",
        "            with torch.no_grad():\n",
        "                m.conv1.weight[:] = w.mean(dim=1, keepdim=True)\n",
        "\n",
        "    m.fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(m.fc.in_features, num_classes))\n",
        "    return m\n",
        "\n",
        "# 3. Re-instantiate the model with the adjusted dropout rate.\n",
        "# Assuming MODEL_ARCH is defined from previous steps, if not, default to resnet34\n",
        "try: MODEL_ARCH\n",
        "except NameError: MODEL_ARCH = \"resnet34\"\n",
        "\n",
        "net = build_resnet(MODEL_ARCH, len(classes), in_ch=1, pretrained=True, dropout=0.4) # Use updated dropout\n",
        "net.to(device)\n",
        "\n",
        "\n",
        "# 2. Adjust the weight decay parameter in the optimizer definition.\n",
        "WEIGHT_DECAY = 1e-3 # Adjusted weight decay\n",
        "optimizer = optim.AdamW(net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY) # Use adjusted weight decay\n",
        "\n",
        "# 4. Re-define the optimizer with the adjusted weight decay. (Done in the previous step)\n",
        "\n",
        "# Re-define the scheduler as it's tied to the optimizer and epochs\n",
        "steps_per_epoch = max(1, int(len(tr_loader.dataset)/BATCH))\n",
        "scheduler = OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch) if MODEL_ARCH.startswith(\"resnet\") else CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "\n",
        "# 5. Re-run the training loop and evaluate the model performance on the validation and test sets.\n",
        "best_f1, best_state = -1.0, None; patience, bad = 8, 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(net, tr_loader)\n",
        "    va_loss, va_acc, va_f1, y_true, y_pred = evaluate(net, va_loader)\n",
        "    try: scheduler.step()\n",
        "    except: pass\n",
        "    print(f\"Epoch {ep:02d} | tr_loss {tr_loss:.4f} acc {tr_acc:.3f} | val_loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f}\")\n",
        "    if va_f1>best_f1:\n",
        "        best_f1, bad = va_f1, 0\n",
        "        best_state = copy.deepcopy(net.state_dict())\n",
        "        torch.save({'state_dict': best_state, 'classes': classes}, f\"{SAVE_DIR}/best.pt\")\n",
        "    else:\n",
        "        bad+=1\n",
        "        if bad>=8:\n",
        "            print(\"Early stopping\"); break\n",
        "\n",
        "if best_state is not None: net.load_state_dict(best_state)\n",
        "te_loss, te_acc, te_f1, y_true, y_pred = evaluate(net, te_loader)\n",
        "print(f\"TEST | loss {te_loss:.4f} acc {te_acc:.3f} f1 {te_f1:.3f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))\n"
      ],
      "id": "aa56bf1d",
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | tr_loss 1.9714 acc 0.224 | val_loss 1.7319 acc 0.351 f1 0.281\n",
            "Epoch 02 | tr_loss 1.7342 acc 0.350 | val_loss 1.5330 acc 0.437 f1 0.363\n",
            "Epoch 03 | tr_loss 1.6042 acc 0.413 | val_loss 1.4125 acc 0.485 f1 0.413\n",
            "Epoch 04 | tr_loss 1.5091 acc 0.449 | val_loss 1.3480 acc 0.511 f1 0.440\n",
            "Epoch 05 | tr_loss 1.4364 acc 0.482 | val_loss 1.3002 acc 0.537 f1 0.473\n",
            "Epoch 06 | tr_loss 1.3694 acc 0.511 | val_loss 1.2522 acc 0.559 f1 0.503\n",
            "Epoch 07 | tr_loss 1.3264 acc 0.534 | val_loss 1.2247 acc 0.575 f1 0.521\n",
            "Epoch 08 | tr_loss 1.2921 acc 0.546 | val_loss 1.1849 acc 0.595 f1 0.546\n",
            "Epoch 09 | tr_loss 1.2439 acc 0.567 | val_loss 1.1710 acc 0.603 f1 0.551\n",
            "Epoch 10 | tr_loss 1.2124 acc 0.583 | val_loss 1.1483 acc 0.614 f1 0.566\n",
            "Epoch 11 | tr_loss 1.1981 acc 0.590 | val_loss 1.1393 acc 0.622 f1 0.575\n",
            "Epoch 12 | tr_loss 1.1638 acc 0.605 | val_loss 1.1305 acc 0.629 f1 0.583\n",
            "Epoch 13 | tr_loss 1.1518 acc 0.614 | val_loss 1.1121 acc 0.635 f1 0.591\n",
            "Epoch 14 | tr_loss 1.1347 acc 0.619 | val_loss 1.0980 acc 0.642 f1 0.608\n",
            "Epoch 15 | tr_loss 1.1118 acc 0.632 | val_loss 1.0888 acc 0.646 f1 0.613\n",
            "Epoch 16 | tr_loss 1.0853 acc 0.642 | val_loss 1.0810 acc 0.647 f1 0.612\n",
            "Epoch 17 | tr_loss 1.0710 acc 0.651 | val_loss 1.0674 acc 0.656 f1 0.623\n",
            "Epoch 18 | tr_loss 1.0614 acc 0.652 | val_loss 1.0575 acc 0.661 f1 0.636\n",
            "Epoch 19 | tr_loss 1.0511 acc 0.654 | val_loss 1.0540 acc 0.661 f1 0.635\n",
            "Epoch 20 | tr_loss 1.0349 acc 0.662 | val_loss 1.0532 acc 0.658 f1 0.634\n",
            "Epoch 21 | tr_loss 1.0189 acc 0.671 | val_loss 1.0485 acc 0.662 f1 0.637\n",
            "Epoch 22 | tr_loss 1.0127 acc 0.674 | val_loss 1.0382 acc 0.668 f1 0.650\n",
            "Epoch 23 | tr_loss 1.0066 acc 0.674 | val_loss 1.0354 acc 0.670 f1 0.652\n",
            "Epoch 24 | tr_loss 0.9839 acc 0.686 | val_loss 1.0379 acc 0.669 f1 0.648\n",
            "Epoch 25 | tr_loss 0.9825 acc 0.688 | val_loss 1.0293 acc 0.671 f1 0.654\n",
            "Epoch 26 | tr_loss 0.9788 acc 0.689 | val_loss 1.0305 acc 0.672 f1 0.654\n",
            "Epoch 27 | tr_loss 0.9620 acc 0.698 | val_loss 1.0233 acc 0.677 f1 0.661\n",
            "Epoch 28 | tr_loss 0.9416 acc 0.706 | val_loss 1.0170 acc 0.675 f1 0.664\n",
            "Epoch 29 | tr_loss 0.9456 acc 0.701 | val_loss 1.0191 acc 0.680 f1 0.662\n",
            "Epoch 30 | tr_loss 0.9286 acc 0.713 | val_loss 1.0138 acc 0.681 f1 0.666\n",
            "Epoch 31 | tr_loss 0.9230 acc 0.716 | val_loss 1.0130 acc 0.683 f1 0.668\n",
            "Epoch 32 | tr_loss 0.9160 acc 0.719 | val_loss 1.0136 acc 0.681 f1 0.672\n",
            "Epoch 33 | tr_loss 0.9010 acc 0.726 | val_loss 1.0108 acc 0.688 f1 0.672\n",
            "Epoch 34 | tr_loss 0.9070 acc 0.723 | val_loss 1.0141 acc 0.685 f1 0.672\n",
            "Epoch 35 | tr_loss 0.8879 acc 0.733 | val_loss 1.0203 acc 0.678 f1 0.665\n",
            "Epoch 36 | tr_loss 0.8882 acc 0.734 | val_loss 1.0130 acc 0.685 f1 0.683\n",
            "Epoch 37 | tr_loss 0.8802 acc 0.734 | val_loss 1.0158 acc 0.684 f1 0.677\n",
            "Epoch 38 | tr_loss 0.8742 acc 0.738 | val_loss 1.0148 acc 0.685 f1 0.675\n",
            "Epoch 39 | tr_loss 0.8694 acc 0.739 | val_loss 1.0104 acc 0.687 f1 0.683\n",
            "Epoch 40 | tr_loss 0.8607 acc 0.744 | val_loss 1.0082 acc 0.691 f1 0.685\n",
            "Epoch 41 | tr_loss 0.8391 acc 0.753 | val_loss 1.0109 acc 0.690 f1 0.683\n",
            "Epoch 42 | tr_loss 0.8387 acc 0.753 | val_loss 1.0128 acc 0.687 f1 0.680\n",
            "Epoch 43 | tr_loss 0.8315 acc 0.758 | val_loss 1.0174 acc 0.685 f1 0.674\n",
            "Epoch 44 | tr_loss 0.8264 acc 0.760 | val_loss 1.0133 acc 0.688 f1 0.685\n",
            "Epoch 45 | tr_loss 0.8172 acc 0.766 | val_loss 1.0158 acc 0.685 f1 0.675\n",
            "Epoch 46 | tr_loss 0.8136 acc 0.766 | val_loss 1.0136 acc 0.691 f1 0.686\n",
            "Epoch 47 | tr_loss 0.8006 acc 0.773 | val_loss 1.0200 acc 0.689 f1 0.681\n",
            "Epoch 48 | tr_loss 0.7962 acc 0.774 | val_loss 1.0122 acc 0.693 f1 0.689\n",
            "Epoch 49 | tr_loss 0.7961 acc 0.774 | val_loss 1.0151 acc 0.692 f1 0.684\n",
            "Epoch 50 | tr_loss 0.7833 acc 0.779 | val_loss 1.0231 acc 0.691 f1 0.684\n",
            "TEST | loss 1.0310 acc 0.678 f1 0.657\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.57      0.66      0.62       192\n",
            "     disgust       0.70      0.64      0.67        22\n",
            "        fear       0.55      0.38      0.45       204\n",
            "       happy       0.88      0.88      0.88       365\n",
            "     neutral       0.64      0.66      0.65       243\n",
            "         sad       0.52      0.59      0.55       228\n",
            "    surprise       0.79      0.77      0.78       159\n",
            "\n",
            "    accuracy                           0.68      1413\n",
            "   macro avg       0.67      0.65      0.66      1413\n",
            "weighted avg       0.68      0.68      0.68      1413\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fec7f854"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "### Subtask:\n",
        "Experiment with different loss functions or class weighting strategies.\n"
      ],
      "id": "fec7f854"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dab272b8"
      },
      "source": [
        "**Reasoning**:\n",
        "To experiment with class weighting strategies, I will calculate class weights based on the inverse frequency of each class in the training dataset and use these weights with the CrossEntropyLoss. This will modify the existing loss function rather than selecting a completely different one. I will then modify the training loop to use this weighted loss function and re-run the training and evaluation steps.\n",
        "\n"
      ],
      "id": "dab272b8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e67d7923",
        "outputId": "5607409a-12d8-4dc3-dde5-d52935093ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        }
      },
      "source": [
        "# 1. Calculate class weights based on inverse frequency\n",
        "# The counts and wpc are already calculated in a previous cell for the WeightedRandomSampler\n",
        "# Use the inverse of the counts to get class weights\n",
        "class_weights = 1.0 / torch.clamp(counts, min=1.0)\n",
        "class_weights = class_weights / class_weights.sum() # Normalize weights to sum to 1\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# 2. Define the weighted CrossEntropyLoss\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)\n",
        "\n",
        "# 3. Modify the training loop to use the new weighted loss function (no code change needed here,\n",
        "# as the criterion variable is already used in train_one_epoch)\n",
        "\n",
        "# 4. Re-run the training loop and evaluate the model performance on the validation and test sets.\n",
        "# The model, optimizer, and scheduler from the previous step are used.\n",
        "best_f1, best_state = -1.0, None; patience, bad = 8, 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(net, tr_loader)\n",
        "    va_loss, va_acc, va_f1, y_true, y_pred = evaluate(net, va_loader)\n",
        "    try: scheduler.step()\n",
        "    except: pass\n",
        "    print(f\"Epoch {ep:02d} | tr_loss {tr_loss:.4f} acc {tr_acc:.3f} | val_loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f}\")\n",
        "    if va_f1>best_f1:\n",
        "        best_f1, bad = va_f1, 0\n",
        "        best_state = copy.deepcopy(net.state_dict())\n",
        "        torch.save({'state_dict': best_state, 'classes': classes}, f\"{SAVE_DIR}/best.pt\")\n",
        "    else:\n",
        "        bad+=1\n",
        "        if bad>=8:\n",
        "            print(\"Early stopping\"); break\n",
        "\n",
        "if best_state is not None: net.load_state_dict(best_state)\n",
        "te_loss, te_acc, te_f1, y_true, y_pred = evaluate(net, te_loader)\n",
        "print(f\"TEST | loss {te_loss:.4f} acc {te_acc:.3f} f1 {te_f1:.3f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ],
      "id": "e67d7923",
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | tr_loss 0.4984 acc 0.769 | val_loss 1.3821 acc 0.677 f1 0.648\n",
            "Epoch 02 | tr_loss 0.4684 acc 0.770 | val_loss 1.3411 acc 0.682 f1 0.653\n",
            "Epoch 03 | tr_loss 0.4599 acc 0.768 | val_loss 1.3166 acc 0.685 f1 0.656\n",
            "Epoch 04 | tr_loss 0.4523 acc 0.769 | val_loss 1.3081 acc 0.684 f1 0.655\n",
            "Epoch 05 | tr_loss 0.4451 acc 0.772 | val_loss 1.3116 acc 0.683 f1 0.656\n",
            "Epoch 06 | tr_loss 0.4612 acc 0.771 | val_loss 1.3130 acc 0.676 f1 0.642\n",
            "Epoch 07 | tr_loss 0.4456 acc 0.775 | val_loss 1.3133 acc 0.682 f1 0.654\n",
            "Epoch 08 | tr_loss 0.4401 acc 0.776 | val_loss 1.3135 acc 0.679 f1 0.651\n",
            "Epoch 09 | tr_loss 0.4436 acc 0.780 | val_loss 1.3129 acc 0.681 f1 0.648\n",
            "Epoch 10 | tr_loss 0.4422 acc 0.780 | val_loss 1.3036 acc 0.687 f1 0.664\n",
            "Epoch 11 | tr_loss 0.4432 acc 0.785 | val_loss 1.3138 acc 0.683 f1 0.663\n",
            "Epoch 12 | tr_loss 0.4334 acc 0.787 | val_loss 1.3044 acc 0.691 f1 0.666\n",
            "Epoch 13 | tr_loss 0.4324 acc 0.789 | val_loss 1.3041 acc 0.689 f1 0.666\n",
            "Epoch 14 | tr_loss 0.4306 acc 0.797 | val_loss 1.3061 acc 0.687 f1 0.666\n",
            "Epoch 15 | tr_loss 0.4376 acc 0.792 | val_loss 1.3172 acc 0.684 f1 0.663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bb9e6576980>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1628, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-434152698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mbest_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mva_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4205381524.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1311\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3461\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3462\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3463\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}