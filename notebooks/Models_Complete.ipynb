{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Models for Visual Emotion Recognition\n",
        "\n",
        "This notebook contains all model implementations for visual emotion recognition, consolidating functionality from the src/models directory.\n",
        "\n",
        "## Models Included:\n",
        "1. **CNN Baseline Model** - Simple CNN for emotion recognition\n",
        "2. **CNN Transfer Learning** - VGG16/VGG19/AlexNet based transfer learning\n",
        "3. **Improved CNN Transfer Learning** - ResNet50/101, EfficientNet with advanced features\n",
        "4. **ResNet Fine-tuning** - Specialized ResNet fine-tuning\n",
        "5. **Custom Classification Heads** - Various classifier architectures\n",
        "6. **Model Ensemble** - Combining multiple models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. CNN Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNBaseline(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple CNN baseline model for emotion recognition.\n",
        "    Designed for 48x48 grayscale images.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=7, input_size=48, dropout_rate=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the baseline CNN model.\n",
        "        \n",
        "        Args:\n",
        "            num_classes (int): Number of emotion classes\n",
        "            input_size (int): Input image size (assumed square)\n",
        "            dropout_rate (float): Dropout rate for regularization\n",
        "        \"\"\"\n",
        "        super(CNNBaseline, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.input_size = input_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.features = nn.Sequential(\n",
        "            # First block\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25),\n",
        "            \n",
        "            # Second block\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25),\n",
        "            \n",
        "            # Third block\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25),\n",
        "            \n",
        "            # Fourth block\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25)\n",
        "        )\n",
        "        \n",
        "        # Calculate flattened feature size\n",
        "        self.feature_size = self._calculate_feature_size()\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.feature_size, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "        \n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def _calculate_feature_size(self):\n",
        "        \"\"\"Calculate the flattened feature size after convolutional layers.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn(1, 1, self.input_size, self.input_size)\n",
        "            x = self.features(x)\n",
        "            return x.numel()\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize model weights.\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the model.\"\"\"\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    def get_num_params(self):\n",
        "        \"\"\"Get number of trainable parameters.\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Test the baseline model\n",
        "print(\"Testing CNN Baseline Model...\")\n",
        "baseline_model = CNNBaseline(num_classes=7, input_size=48)\n",
        "print(f\"Model parameters: {baseline_model.get_num_params():,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(4, 1, 48, 48)\n",
        "output = baseline_model(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. CNN Transfer Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNTransferLearning(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN Transfer Learning model using pre-trained backbones for emotion recognition.\n",
        "    \n",
        "    This model uses pre-trained CNN architectures (VGG, AlexNet, etc.) as feature \n",
        "    extractors and adds custom classifier layers for emotion classification.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=7, backbone='vgg16', pretrained=True, freeze_backbone=False):\n",
        "        \"\"\"\n",
        "        Initialize the transfer learning model.\n",
        "        \n",
        "        Args:\n",
        "            num_classes (int): Number of emotion classes\n",
        "            backbone (str): Pre-trained model to use ('vgg16', 'vgg19', 'alexnet')\n",
        "            pretrained (bool): Whether to use pre-trained weights\n",
        "            freeze_backbone (bool): Whether to freeze backbone weights during training\n",
        "        \"\"\"\n",
        "        super(CNNTransferLearning, self).__init__()\n",
        "        \n",
        "        self.backbone_name = backbone\n",
        "        self.num_classes = num_classes\n",
        "        self.frozen = freeze_backbone\n",
        "        \n",
        "        # Load pre-trained backbone\n",
        "        if backbone == 'vgg16':\n",
        "            self.backbone = models.vgg16(pretrained=pretrained)\n",
        "            backbone_out_features = 25088  # VGG16 feature output size\n",
        "        elif backbone == 'vgg19':\n",
        "            self.backbone = models.vgg19(pretrained=pretrained)\n",
        "            backbone_out_features = 25088  # VGG19 feature output size\n",
        "        elif backbone == 'alexnet':\n",
        "            self.backbone = models.alexnet(pretrained=pretrained)\n",
        "            backbone_out_features = 9216   # AlexNet feature output size\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}. \"\n",
        "                           f\"Supported: ['vgg16', 'vgg19', 'alexnet']\")\n",
        "        \n",
        "        # Extract features and adaptive pooling from backbone\n",
        "        if backbone in ['vgg16', 'vgg19']:\n",
        "            self.features = self.backbone.features\n",
        "            self.avgpool = self.backbone.avgpool\n",
        "        elif backbone == 'alexnet':\n",
        "            self.features = self.backbone.features\n",
        "            self.avgpool = self.backbone.avgpool\n",
        "        \n",
        "        # Freeze backbone if requested\n",
        "        if freeze_backbone:\n",
        "            self._freeze_backbone()\n",
        "        \n",
        "        # Create custom classifier for emotion recognition\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(backbone_out_features, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Initialize classifier weights\n",
        "        self._initialize_classifier()\n",
        "    \n",
        "    def _freeze_backbone(self):\n",
        "        \"\"\"Freeze all parameters in the backbone.\"\"\"\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(f\"Backbone ({self.backbone_name}) weights frozen\")\n",
        "    \n",
        "    def _unfreeze_backbone(self):\n",
        "        \"\"\"Unfreeze all parameters in the backbone.\"\"\"\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(f\"Backbone ({self.backbone_name}) weights unfrozen\")\n",
        "        self.frozen = False\n",
        "    \n",
        "    def _initialize_classifier(self):\n",
        "        \"\"\"Initialize classifier weights using Xavier initialization.\"\"\"\n",
        "        for module in self.classifier.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, 3, 224, 224)\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # Extract features using pre-trained backbone\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        # Flatten features\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        # Classify emotions\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def unfreeze_backbone(self):\n",
        "        \"\"\"Public method to unfreeze backbone for fine-tuning.\"\"\"\n",
        "        self._unfreeze_backbone()\n",
        "    \n",
        "    def freeze_backbone(self):\n",
        "        \"\"\"Public method to freeze backbone layers.\"\"\"\n",
        "        self._freeze_backbone()\n",
        "        self.frozen = True\n",
        "    \n",
        "    def get_num_params(self, trainable_only=True):\n",
        "        \"\"\"\n",
        "        Get number of parameters in the model.\n",
        "        \n",
        "        Args:\n",
        "            trainable_only (bool): If True, count only trainable parameters\n",
        "            \n",
        "        Returns:\n",
        "            int: Number of parameters\n",
        "        \"\"\"\n",
        "        if trainable_only:\n",
        "            return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        else:\n",
        "            return sum(p.numel() for p in self.parameters())\n",
        "    \n",
        "    def get_backbone_params(self):\n",
        "        \"\"\"Get number of parameters in the backbone.\"\"\"\n",
        "        return sum(p.numel() for p in self.features.parameters())\n",
        "    \n",
        "    def get_classifier_params(self):\n",
        "        \"\"\"Get number of parameters in the classifier.\"\"\"\n",
        "        return sum(p.numel() for p in self.classifier.parameters())\n",
        "    \n",
        "    def print_model_info(self):\n",
        "        \"\"\"Print detailed information about the model.\"\"\"\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"CNN Transfer Learning Model Information\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Backbone: {self.backbone_name}\")\n",
        "        print(f\"Pretrained: Yes\")\n",
        "        print(f\"Frozen: {self.frozen}\")\n",
        "        print(f\"Number of classes: {self.num_classes}\")\n",
        "        print(f\"Total parameters: {self.get_num_params(trainable_only=False):,}\")\n",
        "        print(f\"Trainable parameters: {self.get_num_params(trainable_only=True):,}\")\n",
        "        print(f\"Backbone parameters: {self.get_backbone_params():,}\")\n",
        "        print(f\"Classifier parameters: {self.get_classifier_params():,}\")\n",
        "        \n",
        "        if self.frozen:\n",
        "            print(f\"Training strategy: Feature extraction (backbone frozen)\")\n",
        "        else:\n",
        "            print(f\"Training strategy: Fine-tuning (all layers trainable)\")\n",
        "\n",
        "\n",
        "# Test the transfer learning model\n",
        "print(\"\\nTesting CNN Transfer Learning Model...\")\n",
        "transfer_model = CNNTransferLearning(num_classes=7, backbone='vgg16', pretrained=True, freeze_backbone=False)\n",
        "transfer_model.print_model_info()\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(4, 3, 224, 224)\n",
        "output = transfer_model(test_input)\n",
        "print(f\"\\nInput shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Improved CNN Transfer Learning with Advanced Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    \"\"\"Spatial Attention mechanism for improved feature focus.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        attention = self.conv(x)\n",
        "        attention = self.sigmoid(attention)\n",
        "        return x * attention\n",
        "\n",
        "\n",
        "class ImprovedCNNTransferLearning(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved CNN Transfer Learning model with enhanced architecture and features.\n",
        "    \n",
        "    This model implements several improvements over the basic transfer learning:\n",
        "    - ResNet50 backbone for better feature extraction\n",
        "    - Enhanced classifier with dropout and batch normalization\n",
        "    - Support for multiple backbone architectures\n",
        "    - Advanced regularization techniques\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=7, backbone='resnet50', pretrained=True, \n",
        "                 freeze_backbone=False, dropout_rate=0.5, use_attention=False):\n",
        "        \"\"\"\n",
        "        Initialize the improved transfer learning model.\n",
        "        \n",
        "        Args:\n",
        "            num_classes (int): Number of emotion classes\n",
        "            backbone (str): Pre-trained model to use ('resnet50', 'resnet101', 'efficientnet_b4', etc.)\n",
        "            pretrained (bool): Whether to use pre-trained weights\n",
        "            freeze_backbone (bool): Whether to freeze backbone weights during training\n",
        "            dropout_rate (float): Dropout rate for regularization\n",
        "            use_attention (bool): Whether to add attention mechanism\n",
        "        \"\"\"\n",
        "        super(ImprovedCNNTransferLearning, self).__init__()\n",
        "        \n",
        "        self.backbone_name = backbone\n",
        "        self.num_classes = num_classes\n",
        "        self.frozen = freeze_backbone\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_attention = use_attention\n",
        "        \n",
        "        # Load pre-trained backbone\n",
        "        if backbone == 'resnet50':\n",
        "            self.backbone = models.resnet50(pretrained=pretrained)\n",
        "            backbone_out_features = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()  # Remove final FC layer\n",
        "        elif backbone == 'resnet101':\n",
        "            self.backbone = models.resnet101(pretrained=pretrained)\n",
        "            backbone_out_features = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "        elif backbone == 'densenet121':\n",
        "            self.backbone = models.densenet121(pretrained=pretrained)\n",
        "            backbone_out_features = self.backbone.classifier.in_features\n",
        "            self.backbone.classifier = nn.Identity()\n",
        "        elif backbone == 'vgg16':\n",
        "            self.backbone = models.vgg16(pretrained=pretrained)\n",
        "            self.features = self.backbone.features\n",
        "            self.avgpool = self.backbone.avgpool\n",
        "            backbone_out_features = 25088\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "        \n",
        "        # Freeze backbone if requested\n",
        "        if freeze_backbone:\n",
        "            self._freeze_backbone()\n",
        "        \n",
        "        # Add attention mechanism if requested\n",
        "        if use_attention and backbone != 'vgg16':\n",
        "            self.attention = SpatialAttention(backbone_out_features)\n",
        "        \n",
        "        # Create enhanced classifier\n",
        "        if backbone == 'vgg16':\n",
        "            self.classifier = self._create_vgg_classifier(backbone_out_features)\n",
        "        else:\n",
        "            self.classifier = self._create_resnet_classifier(backbone_out_features)\n",
        "        \n",
        "        # Initialize classifier weights\n",
        "        self._initialize_classifier()\n",
        "    \n",
        "    def _create_resnet_classifier(self, in_features):\n",
        "        \"\"\"Create classifier for ResNet-like architectures.\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.BatchNorm1d(in_features),\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(in_features, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(self.dropout_rate * 0.8),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(self.dropout_rate * 0.6),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(self.dropout_rate * 0.4),\n",
        "            nn.Linear(256, self.num_classes)\n",
        "        )\n",
        "    \n",
        "    def _create_vgg_classifier(self, in_features):\n",
        "        \"\"\"Create classifier for VGG architectures.\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(in_features, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(self.dropout_rate * 0.6),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(self.dropout_rate * 0.4),\n",
        "            nn.Linear(256, self.num_classes)\n",
        "        )\n",
        "    \n",
        "    def _freeze_backbone(self):\n",
        "        \"\"\"Freeze backbone parameters.\"\"\"\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(f\"Backbone ({self.backbone_name}) weights frozen\")\n",
        "    \n",
        "    def _initialize_classifier(self):\n",
        "        \"\"\"Initialize classifier weights.\"\"\"\n",
        "        for module in self.classifier.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "                nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.BatchNorm1d):\n",
        "                nn.init.ones_(module.weight)\n",
        "                nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the model.\"\"\"\n",
        "        if self.backbone_name == 'vgg16':\n",
        "            x = self.features(x)\n",
        "            if self.use_attention:\n",
        "                x = self.attention(x)\n",
        "            x = self.avgpool(x)\n",
        "            x = torch.flatten(x, 1)\n",
        "        else:\n",
        "            x = self.backbone(x)\n",
        "        \n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    def get_num_params(self, trainable_only=True):\n",
        "        \"\"\"Get number of parameters in the model.\"\"\"\n",
        "        if trainable_only:\n",
        "            return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        else:\n",
        "            return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "# Test the improved model\n",
        "print(\"\\nTesting Improved CNN Transfer Learning Model...\")\n",
        "improved_model = ImprovedCNNTransferLearning(\n",
        "    num_classes=7, \n",
        "    backbone='resnet50', \n",
        "    pretrained=True, \n",
        "    freeze_backbone=False,\n",
        "    dropout_rate=0.5,\n",
        "    use_attention=True\n",
        ")\n",
        "print(f\"Model parameters: {improved_model.get_num_params():,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(4, 3, 224, 224)\n",
        "output = improved_model(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Custom Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    Label Smoothing Cross Entropy Loss.\n",
        "    Helps prevent overfitting by softening the labels.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        confidence = 1. - self.smoothing\n",
        "        logprobs = F.log_softmax(pred, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for addressing class imbalance.\n",
        "    Focuses learning on hard examples.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "\n",
        "# Test the loss functions\n",
        "print(\"Testing Custom Loss Functions...\")\n",
        "\n",
        "# Create dummy data\n",
        "pred = torch.randn(10, 7)  # 10 samples, 7 classes\n",
        "target = torch.randint(0, 7, (10,))  # Random targets\n",
        "\n",
        "# Test losses\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "ls_loss = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "focal_loss = FocalLoss(alpha=1, gamma=2)\n",
        "\n",
        "print(f\"Cross Entropy Loss: {ce_loss(pred, target).item():.4f}\")\n",
        "print(f\"Label Smoothing Loss: {ls_loss(pred, target).item():.4f}\")\n",
        "print(f\"Focal Loss: {focal_loss(pred, target).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Ensemble of multiple models for improved performance.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, models, weights=None):\n",
        "        \"\"\"\n",
        "        Initialize model ensemble.\n",
        "        \n",
        "        Args:\n",
        "            models (list): List of trained models\n",
        "            weights (list): Optional weights for each model\n",
        "        \"\"\"\n",
        "        super(ModelEnsemble, self).__init__()\n",
        "        self.models = nn.ModuleList(models)\n",
        "        \n",
        "        if weights is None:\n",
        "            self.weights = [1.0 / len(models)] * len(models)\n",
        "        else:\n",
        "            self.weights = weights\n",
        "        \n",
        "        print(f\"Ensemble created with {len(models)} models\")\n",
        "        print(f\"Weights: {self.weights}\")\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through ensemble.\"\"\"\n",
        "        outputs = []\n",
        "        \n",
        "        for model, weight in zip(self.models, self.weights):\n",
        "            with torch.no_grad():\n",
        "                output = model(x)\n",
        "                outputs.append(weight * F.softmax(output, dim=1))\n",
        "        \n",
        "        ensemble_output = torch.stack(outputs).sum(dim=0)\n",
        "        return torch.log(ensemble_output + 1e-8)  # Convert back to logits\n",
        "\n",
        "\n",
        "# Example of creating an ensemble\n",
        "print(\"\\nCreating Model Ensemble...\")\n",
        "\n",
        "# Create multiple models for ensemble\n",
        "model1 = CNNTransferLearning(num_classes=7, backbone='vgg16')\n",
        "model2 = ImprovedCNNTransferLearning(num_classes=7, backbone='resnet50')\n",
        "\n",
        "# Create ensemble\n",
        "ensemble = ModelEnsemble([model1, model2], weights=[0.4, 0.6])\n",
        "\n",
        "# Test ensemble\n",
        "test_input = torch.randn(4, 3, 224, 224)\n",
        "ensemble_output = ensemble(test_input)\n",
        "print(f\"Ensemble output shape: {ensemble_output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Factory Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_cnn_baseline(num_classes=7, input_size=48, dropout_rate=0.5, device='cpu'):\n",
        "    \"\"\"\n",
        "    Factory function to create CNN Baseline model.\n",
        "    \n",
        "    Args:\n",
        "        num_classes (int): Number of emotion classes\n",
        "        input_size (int): Input image size\n",
        "        dropout_rate (float): Dropout rate\n",
        "        device (str): Device to move model to\n",
        "        \n",
        "    Returns:\n",
        "        CNNBaseline: Initialized model\n",
        "    \"\"\"\n",
        "    model = CNNBaseline(num_classes=num_classes, input_size=input_size, dropout_rate=dropout_rate)\n",
        "    model = model.to(device)\n",
        "    print(f\"CNN Baseline created with {model.get_num_params():,} parameters\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_cnn_transfer(num_classes=7, backbone='vgg16', pretrained=True, \n",
        "                       freeze_backbone=False, device='cpu'):\n",
        "    \"\"\"\n",
        "    Factory function to create CNN Transfer Learning model.\n",
        "    \n",
        "    Args:\n",
        "        num_classes (int): Number of emotion classes\n",
        "        backbone (str): Pre-trained backbone to use\n",
        "        pretrained (bool): Whether to use pre-trained weights\n",
        "        freeze_backbone (bool): Whether to freeze backbone weights\n",
        "        device (str): Device to move model to\n",
        "        \n",
        "    Returns:\n",
        "        CNNTransferLearning: Initialized model\n",
        "    \"\"\"\n",
        "    model = CNNTransferLearning(\n",
        "        num_classes=num_classes,\n",
        "        backbone=backbone,\n",
        "        pretrained=pretrained,\n",
        "        freeze_backbone=freeze_backbone\n",
        "    )\n",
        "    \n",
        "    model = model.to(device)\n",
        "    model.print_model_info()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def create_improved_model(num_classes=7, backbone='resnet50', pretrained=True,\n",
        "                         freeze_backbone=False, dropout_rate=0.5, \n",
        "                         use_attention=False, device='cpu'):\n",
        "    \"\"\"\n",
        "    Factory function to create Improved CNN Transfer Learning model.\n",
        "    \n",
        "    Args:\n",
        "        num_classes (int): Number of emotion classes\n",
        "        backbone (str): Pre-trained backbone to use\n",
        "        pretrained (bool): Whether to use pre-trained weights\n",
        "        freeze_backbone (bool): Whether to freeze backbone weights\n",
        "        dropout_rate (float): Dropout rate for regularization\n",
        "        use_attention (bool): Whether to add attention mechanism\n",
        "        device (str): Device to move model to\n",
        "        \n",
        "    Returns:\n",
        "        ImprovedCNNTransferLearning: Initialized model\n",
        "    \"\"\"\n",
        "    model = ImprovedCNNTransferLearning(\n",
        "        num_classes=num_classes,\n",
        "        backbone=backbone,\n",
        "        pretrained=pretrained,\n",
        "        freeze_backbone=freeze_backbone,\n",
        "        dropout_rate=dropout_rate,\n",
        "        use_attention=use_attention\n",
        "    )\n",
        "    \n",
        "    model = model.to(device)\n",
        "    print(f\"Improved model created with {model.get_num_params():,} parameters\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Example usage\n",
        "print(\"\\nTesting Model Factory Functions...\")\n",
        "\n",
        "# Create different models using factory functions\n",
        "baseline = create_cnn_baseline(num_classes=7, input_size=48, device='cpu')\n",
        "transfer = create_cnn_transfer(num_classes=7, backbone='vgg16', device='cpu')\n",
        "improved = create_improved_model(num_classes=7, backbone='resnet50', use_attention=True, device='cpu')\n",
        "\n",
        "print(\"\\nAll models created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides a complete collection of model architectures for visual emotion recognition:\n",
        "\n",
        "1. **CNN Baseline**: Simple CNN for grayscale 48x48 images\n",
        "2. **CNN Transfer Learning**: VGG16/VGG19/AlexNet based transfer learning\n",
        "3. **Improved Transfer Learning**: ResNet50/101 with advanced features\n",
        "4. **Custom Loss Functions**: Label smoothing and focal loss\n",
        "5. **Model Ensemble**: Combining multiple models\n",
        "6. **Factory Functions**: Easy model creation utilities\n",
        "\n",
        "All models are self-contained within this notebook and can be used independently without requiring the src folder structure."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}