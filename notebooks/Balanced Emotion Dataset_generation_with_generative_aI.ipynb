{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d674cd14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running in Colab, uncomment installs:\n",
        "# !pip -q install diffusers==0.30.0 transformers==4.43.3 accelerate==0.33.0 \\\n",
        "#           sentencepiece==0.2.0 safetensors==0.4.3 \\\n",
        "#           facenet-pytorch==2.6.0 opencv-python==4.10.0.84 \\\n",
        "#           imagehash==4.3.1 ftfy==6.2 \\\n",
        "#           git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import os, json, random, time, uuid, math, hashlib, itertools\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Face detection\n",
        "from facenet_pytorch import MTCNN\n",
        "\n",
        "# CLIP for text-image similarity\n",
        "import clip\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Perceptual hamming hash for near-duplicate removal\n",
        "import imagehash\n",
        "\n",
        "# Diffusers (choose SDXL or SD 1.5 below)\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from diffusers import DPMSolverMultistepScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c220b25c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== YOUR PATHS (exactly as you asked) =====\n",
        "TRAIN_CSV      = \"/content/ann-visual-emotion/data/processed/EmoSet_splits/train.csv\"\n",
        "IMAGES_ROOT    = \"/content/ann-visual-emotion/data/raw/EmoSet\"\n",
        "SYNTH_ROOT     = \"/content/ann-visual-emotion/data/raw/EmoSet\"  # generate into the same root (separate subfolders/files)\n",
        "LABEL_MAP_JSON = \"/content/ann-visual-emotion/data/processed/EmoSet_splits/label_map.json\"\n",
        "\n",
        "Path(SYNTH_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Fallback label map (will be overwritten if file exists)\n",
        "LABEL_MAP = {\n",
        "    \"amusement\": 0,\n",
        "    \"anger\": 1,\n",
        "    \"awe\": 2,\n",
        "    \"contentment\": 3,\n",
        "    \"disgust\": 4,\n",
        "    \"excitement\": 5,\n",
        "    \"fear\": 6,\n",
        "    \"sadness\": 7,\n",
        "}\n",
        "\n",
        "# Overwrite with file contents if present\n",
        "if Path(LABEL_MAP_JSON).exists():\n",
        "    with open(LABEL_MAP_JSON, \"r\") as f:\n",
        "        mp = json.load(f)\n",
        "    # normalize to str->int\n",
        "    LABEL_MAP = {str(k): int(v) for k, v in mp.items()}\n",
        "\n",
        "IDX2LABEL = {v:k for k,v in LABEL_MAP.items()}\n",
        "CLASSES = list(LABEL_MAP.keys())\n",
        "\n",
        "# Target count per class\n",
        "TARGET_PER_CLASS = 2000\n",
        "\n",
        "# Read train.csv and count existing images per class\n",
        "df_tr = pd.read_csv(TRAIN_CSV)\n",
        "# Try to find reasonable column names\n",
        "img_col_candidates = [\"image_path\", \"img_path\", \"path\", \"filepath\", \"image\", \"img\"]\n",
        "lab_col_candidates = [\"label\", \"class\", \"target\", \"emotion\"]\n",
        "\n",
        "IMG_COL = next((c for c in img_col_candidates if c in df_tr.columns), None)\n",
        "LAB_COL = next((c for c in lab_col_candidates if c in df_tr.columns), None)\n",
        "if IMG_COL is None or LAB_COL is None:\n",
        "    raise ValueError(f\"Could not infer image/label columns. Found columns: {list(df_tr.columns)}\")\n",
        "\n",
        "existing_counts = Counter(df_tr[LAB_COL].astype(str).tolist())\n",
        "existing_counts = {k: int(existing_counts.get(k, 0)) for k in CLASSES}\n",
        "\n",
        "need_counts = {lab: max(TARGET_PER_CLASS - existing_counts.get(lab, 0), 0) for lab in CLASSES}\n",
        "need_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c27504",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core style for photoreal portraits\n",
        "CORE_STYLE = (\n",
        "    \"ultra-detailed RAW color photo, 85mm lens, shallow depth of field, \"\n",
        "    \"studio portrait, highly detailed skin, pores visible, sharp eyes, \"\n",
        "    \"soft key light, rim light, natural color grading\"\n",
        ")\n",
        "\n",
        "# Negative prompt to avoid artifacts\n",
        "NEGATIVE = (\n",
        "    \"low-res, blurry, deformed face, extra fingers, extra limbs, cropped head, \"\n",
        "    \"over-saturated, watermark, text, logo, jpeg artifacts, disfigured, doll-like, \"\n",
        "    \"unrealistic skin, severe shadow banding\"\n",
        ")\n",
        "\n",
        "# Diverse attributes to randomize per sample\n",
        "AGES = [\"teenager\", \"young adult\", \"adult\", \"middle-aged\", \"senior\"]\n",
        "GENDERS = [\"woman\", \"man\", \"non-binary person\"]\n",
        "SKIN_TONES = [\n",
        "    \"very fair skin\", \"fair skin\", \"medium skin\", \"olive skin\",\n",
        "    \"brown skin\", \"dark brown skin\", \"very dark skin\"\n",
        "]\n",
        "HAIR = [\"short hair\", \"long hair\", \"curly hair\", \"straight hair\", \"tied hair\", \"shaved head\"]\n",
        "LIGHTS = [\n",
        "    \"softbox lighting\", \"natural window light\", \"butterfly lighting\",\n",
        "    \"Rembrandt lighting\", \"cinematic lighting\", \"overcast daylight\"\n",
        "]\n",
        "\n",
        "# Emotion-specific wording that tends to produce crisp results\n",
        "EMOTION_VERBS = {\n",
        "    \"amusement\":   [\"laughing\", \"smiling with joy\", \"amused grin\", \"eyes crinkled in laughter\"],\n",
        "    \"anger\":       [\"angry expression\", \"furious glare\", \"tense jaw\", \"brows furrowed in anger\"],\n",
        "    \"awe\":         [\"awe-struck expression\", \"eyes wide in wonder\", \"astonished look\", \"breath taken in awe\"],\n",
        "    \"contentment\": [\"content smile\", \"relaxed and satisfied\", \"peaceful expression\", \"calm gentle smile\"],\n",
        "    \"disgust\":     [\"disgusted expression\", \"nose wrinkled\", \"upper lip raised\", \"eyes squinting in disgust\"],\n",
        "    \"excitement\":  [\"excited expression\", \"sparkling eyes\", \"cheerful wide smile\", \"thrilled look\"],\n",
        "    \"fear\":        [\"fearful expression\", \"eyes widened in fear\", \"tense mouth\", \"startled face\"],\n",
        "    \"sadness\":     [\"sad expression\", \"teary eyes\", \"downturned mouth\", \"gloomy look\"],\n",
        "}\n",
        "\n",
        "def make_prompt(emotion: str) -> str:\n",
        "    a = random.choice(AGES)\n",
        "    g = random.choice(GENDERS)\n",
        "    s = random.choice(SKIN_TONES)\n",
        "    h = random.choice(HAIR)\n",
        "    l = random.choice(LIGHTS)\n",
        "    e = random.choice(EMOTION_VERBS[emotion])\n",
        "\n",
        "    return (\n",
        "        f\"A photorealistic portrait of a {a} {g} with {s}, {h}, \"\n",
        "        f\"showing a clear {emotion} facial expression ({e}), \"\n",
        "        f\"{CORE_STYLE}, {l}, high dynamic range, 8k master\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6375812e",
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_SDXL = torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 14_000_000_000\n",
        "\n",
        "if USE_SDXL:\n",
        "    model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        model_id, torch_dtype=torch.float16, use_safetensors=True\n",
        "    )\n",
        "else:\n",
        "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        model_id, torch_dtype=torch.float16, use_safetensors=True\n",
        "    )\n",
        "\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Slight guidance helps clarity; you can tune 6.5–8.0 (SD 1.5) or 5.0–7.0 (SDXL)\n",
        "GUIDANCE_SCALE = 7.0 if not USE_SDXL else 6.0\n",
        "IMG_SIZE = (768, 768) if USE_SDXL else (512, 512)\n",
        "BATCH_SIZE = 4  # increase if your VRAM allows\n",
        "SEED = 42\n",
        "generator = torch.Generator(device=pipe.device).manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cf98b21",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Face detector\n",
        "mtcnn = MTCNN(keep_all=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# CLIP model\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model.eval()\n",
        "\n",
        "def clip_score(pil_img: Image.Image, text: str) -> float:\n",
        "    with torch.no_grad():\n",
        "        image_input = clip_preprocess(pil_img).unsqueeze(0).to(clip_model.visual.conv1.weight.device)\n",
        "        text_tokens = clip.tokenize([text]).to(clip_model.visual.conv1.weight.device)\n",
        "        image_features = clip_model.encode_image(image_input)\n",
        "        text_features  = clip_model.encode_text(text_tokens)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features  /= text_features.norm(dim=-1, keepdim=True)\n",
        "        sim = (image_features @ text_features.T).item()\n",
        "    return float(sim)\n",
        "\n",
        "# Simple near-duplicate check via perceptual hash\n",
        "def is_near_duplicate(pil_img: Image.Image, seen_hashes: set, max_dist: int = 5) -> bool:\n",
        "    h = imagehash.phash(pil_img)\n",
        "    for sh in seen_hashes:\n",
        "        if h - sh <= max_dist:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69fb79a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_with_meta(pil_img: Image.Image, out_dir: Path, meta: dict, fname_prefix: str = \"synth\"):\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    uid = str(uuid.uuid4())[:8]\n",
        "    img_fp = out_dir / f\"{fname_prefix}_{uid}.png\"\n",
        "    json_fp = out_dir / f\"{fname_prefix}_{uid}.json\"\n",
        "    pil_img.save(img_fp, \"PNG\")\n",
        "    with open(json_fp, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    return img_fp, json_fp\n",
        "\n",
        "def crop_face_if_present(pil_img: Image.Image, margin: float = 0.15) -> Image.Image:\n",
        "    # returns cropped image if a single face is detected; else original\n",
        "    img = pil_img.convert(\"RGB\")\n",
        "    boxes, _ = mtcnn.detect(img)\n",
        "    if boxes is not None and len(boxes) > 0:\n",
        "        x1, y1, x2, y2 = boxes[0]  # first face\n",
        "        w, h = img.size\n",
        "        dx, dy = (x2 - x1), (y2 - y1)\n",
        "        x1 = max(0, int(x1 - margin * dx))\n",
        "        x2 = min(w, int(x2 + margin * dx))\n",
        "        y1 = max(0, int(y1 - margin * dy))\n",
        "        y2 = min(h, int(y2 + margin * dy))\n",
        "        return img.crop((x1, y1, x2, y2)).resize(img.size, Image.LANCZOS)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b94c174",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLIP acceptance thresholds (tune per your taste)\n",
        "CLIP_MIN = 0.26 if USE_SDXL else 0.28  # SDXL often gives slightly higher semantic fit; adjust as needed\n",
        "\n",
        "# Keep rolling hash set for de-duplication\n",
        "seen_hashes_per_class = {lab: set() for lab in CLASSES}\n",
        "\n",
        "def generate_batch(prompts, negative_prompt):\n",
        "    if USE_SDXL:\n",
        "        images = pipe(\n",
        "            prompt=prompts,\n",
        "            negative_prompt=[negative_prompt]*len(prompts),\n",
        "            num_inference_steps=28,\n",
        "            guidance_scale=GUIDANCE_SCALE,\n",
        "            width=IMG_SIZE[0], height=IMG_SIZE[1],\n",
        "            generator=generator\n",
        "        ).images\n",
        "    else:\n",
        "        images = pipe(\n",
        "            prompt=prompts,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=30,\n",
        "            guidance_scale=GUIDANCE_SCALE,\n",
        "            width=IMG_SIZE[0], height=IMG_SIZE[1],\n",
        "            generator=generator\n",
        "        ).images\n",
        "    return images\n",
        "\n",
        "for emotion in CLASSES:\n",
        "    need = int(need_counts.get(emotion, 0))\n",
        "    if need <= 0:\n",
        "        print(f\"[{emotion}] Already >= {TARGET_PER_CLASS}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nGenerating for '{emotion}' -> need {need} images to reach {TARGET_PER_CLASS}\")\n",
        "    out_dir = Path(SYNTH_ROOT) / emotion\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pbar = tqdm(total=need, desc=f\"{emotion}\")\n",
        "    accepted = 0\n",
        "\n",
        "    while accepted < need:\n",
        "        # Build a small batch of diverse prompts\n",
        "        batch_prompts = [make_prompt(emotion) for _ in range(BATCH_SIZE)]\n",
        "        images = generate_batch(batch_prompts, NEGATIVE)\n",
        "\n",
        "        for img, prompt in zip(images, batch_prompts):\n",
        "            # Optional: crop to face framing to make expression clearer\n",
        "            img_c = crop_face_if_present(img)\n",
        "\n",
        "            # Filter 1: at least one face\n",
        "            boxes, _ = mtcnn.detect(img_c)\n",
        "            if boxes is None or len(boxes) == 0:\n",
        "                continue\n",
        "\n",
        "            # Filter 2: CLIP prompt-image similarity\n",
        "            sim = clip_score(img_c, prompt)\n",
        "            if sim < CLIP_MIN:\n",
        "                continue\n",
        "\n",
        "            # Filter 3: de-duplication\n",
        "            ph = imagehash.phash(img_c)\n",
        "            if is_near_duplicate(img_c, seen_hashes_per_class[emotion], max_dist=5):\n",
        "                continue\n",
        "\n",
        "            # Save + metadata\n",
        "            meta = {\n",
        "                \"emotion\": emotion,\n",
        "                \"prompt\": prompt,\n",
        "                \"negative_prompt\": NEGATIVE,\n",
        "                \"model_id\": model_id,\n",
        "                \"guidance_scale\": GUIDANCE_SCALE,\n",
        "                \"seed\": SEED,\n",
        "                \"clip_similarity\": sim,\n",
        "                \"use_sdxl\": bool(USE_SDXL)\n",
        "            }\n",
        "            save_with_meta(img_c, out_dir, meta, fname_prefix=\"synth\")\n",
        "            seen_hashes_per_class[emotion].add(ph)\n",
        "\n",
        "            accepted += 1\n",
        "            pbar.update(1)\n",
        "            if accepted >= need:\n",
        "                break\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "print(\"\\nDone! Synthetic balancing complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c189e34f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def append_synth_to_csv(train_csv, images_root, classes):\n",
        "    df = pd.read_csv(train_csv)\n",
        "    img_col = IMG_COL\n",
        "    lab_col = LAB_COL\n",
        "\n",
        "    rows = []\n",
        "    for c in classes:\n",
        "        folder = Path(SYNTH_ROOT) / c\n",
        "        if not folder.exists():\n",
        "            continue\n",
        "        for p in folder.glob(\"synth_*.png\"):\n",
        "            # store relative path (relative to IMAGES_ROOT) if your CSV uses relatives\n",
        "            try:\n",
        "                rel = str(Path(p).relative_to(IMAGES_ROOT))\n",
        "            except ValueError:\n",
        "                # if generation path is already inside IMAGES_ROOT, the above works; otherwise just store absolute\n",
        "                rel = str(p)\n",
        "            rows.append({img_col: rel, lab_col: c})\n",
        "\n",
        "    df_new = pd.concat([df, pd.DataFrame(rows)], ignore_index=True)\n",
        "    # Drop exact duplicates just in case\n",
        "    df_new = df_new.drop_duplicates(subset=[img_col, lab_col])\n",
        "    df_new.to_csv(train_csv, index=False)\n",
        "    return len(rows)\n",
        "\n",
        "added = append_synth_to_csv(TRAIN_CSV, IMAGES_ROOT, CLASSES)\n",
        "print(f\"Appended {added} synthetic rows to {TRAIN_CSV}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c664a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Path to your folder\n",
        "folder_path = \"/content/ann-visual-emotion/data/processed/NewEmoSet\"\n",
        "\n",
        "# Output zip file path (without .zip extension)\n",
        "output_path = \"/content/NewEmoSet\"\n",
        "\n",
        "# Create the zip file\n",
        "shutil.make_archive(output_path, 'zip', folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "853a02d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/NewEmoSet.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
