{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Training Pipeline for Visual Emotion Recognition\n",
        "\n",
        "This notebook contains all training functionality for visual emotion recognition, consolidating functionality from the src/training directory.\n",
        "\n",
        "## Components Included:\n",
        "1. **Training Loop** - Complete training pipeline with validation\n",
        "2. **Enhanced Training** - Advanced training with gradient accumulation, clipping, and scheduling\n",
        "3. **Loss Functions** - Various loss functions including focal loss and label smoothing\n",
        "4. **Optimizers** - Different optimizers and configurations\n",
        "5. **Learning Rate Schedulers** - Various LR scheduling strategies\n",
        "6. **Evaluation Functions** - Model evaluation and metrics\n",
        "7. **Checkpointing** - Model saving and loading\n",
        "8. **Training Monitoring** - Loss curves and training visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data and computation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau, OneCycleLR\n",
        "\n",
        "# Set device and random seeds\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "print(\"Random seeds set for reproducibility\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    Label Smoothing Cross Entropy Loss.\n",
        "    Helps prevent overfitting by softening the labels.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        confidence = 1. - self.smoothing\n",
        "        logprobs = F.log_softmax(pred, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for addressing class imbalance.\n",
        "    Focuses learning on hard examples.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha=1, gamma=2, weight=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        ce_loss = F.cross_entropy(pred, target, weight=self.weight, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "\n",
        "class WeightedFocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Weighted Focal Loss combining class weights with focal loss.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha=1, gamma=2, weight=None):\n",
        "        super(WeightedFocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        \n",
        "        if self.weight is not None:\n",
        "            # Apply class weights\n",
        "            weight_t = self.weight.gather(0, target)\n",
        "            focal_loss = weight_t * focal_loss\n",
        "        \n",
        "        return focal_loss.mean()\n",
        "\n",
        "\n",
        "def get_loss_function(loss_type='CrossEntropy', class_weights=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Factory function to create loss functions.\n",
        "    \n",
        "    Args:\n",
        "        loss_type (str): Type of loss function\n",
        "        class_weights (torch.Tensor): Class weights for handling imbalance\n",
        "        **kwargs: Additional arguments for specific loss functions\n",
        "        \n",
        "    Returns:\n",
        "        Loss function\n",
        "    \"\"\"\n",
        "    if loss_type.lower() == 'crossentropy':\n",
        "        return nn.CrossEntropyLoss(weight=class_weights)\n",
        "    elif loss_type.lower() == 'labelsmoothing':\n",
        "        smoothing = kwargs.get('smoothing', 0.1)\n",
        "        return LabelSmoothingCrossEntropy(smoothing=smoothing)\n",
        "    elif loss_type.lower() == 'focal':\n",
        "        alpha = kwargs.get('alpha', 1)\n",
        "        gamma = kwargs.get('gamma', 2)\n",
        "        return FocalLoss(alpha=alpha, gamma=gamma, weight=class_weights)\n",
        "    elif loss_type.lower() == 'weightedfocal':\n",
        "        alpha = kwargs.get('alpha', 1)\n",
        "        gamma = kwargs.get('gamma', 2)\n",
        "        return WeightedFocalLoss(alpha=alpha, gamma=gamma, weight=class_weights)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
        "\n",
        "\n",
        "# Test loss functions\n",
        "print(\"Testing loss functions...\")\n",
        "pred = torch.randn(10, 7)  # 10 samples, 7 classes\n",
        "target = torch.randint(0, 7, (10,))  # Random targets\n",
        "class_weights = torch.rand(7)  # Random weights\n",
        "\n",
        "losses = {\n",
        "    'CrossEntropy': get_loss_function('CrossEntropy'),\n",
        "    'Weighted CE': get_loss_function('CrossEntropy', class_weights),\n",
        "    'Label Smoothing': get_loss_function('LabelSmoothing', smoothing=0.1),\n",
        "    'Focal': get_loss_function('Focal', alpha=1, gamma=2),\n",
        "    'Weighted Focal': get_loss_function('WeightedFocal', class_weights, alpha=1, gamma=2)\n",
        "}\n",
        "\n",
        "for name, loss_fn in losses.items():\n",
        "    loss_value = loss_fn(pred, target)\n",
        "    print(f\"{name}: {loss_value.item():.4f}\")\n",
        "\n",
        "print(\"Loss functions created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Optimizers and Schedulers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_optimizer(model, optimizer_type='Adam', lr=1e-3, weight_decay=1e-4, **kwargs):\n",
        "    \"\"\"\n",
        "    Factory function to create optimizers.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        optimizer_type (str): Type of optimizer\n",
        "        lr (float): Learning rate\n",
        "        weight_decay (float): Weight decay\n",
        "        **kwargs: Additional optimizer arguments\n",
        "        \n",
        "    Returns:\n",
        "        PyTorch optimizer\n",
        "    \"\"\"\n",
        "    if optimizer_type.lower() == 'adam':\n",
        "        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay,\n",
        "                         betas=kwargs.get('betas', (0.9, 0.999)))\n",
        "    elif optimizer_type.lower() == 'adamw':\n",
        "        return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay,\n",
        "                          betas=kwargs.get('betas', (0.9, 0.999)))\n",
        "    elif optimizer_type.lower() == 'sgd':\n",
        "        return optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay,\n",
        "                        momentum=kwargs.get('momentum', 0.9))\n",
        "    elif optimizer_type.lower() == 'rmsprop':\n",
        "        return optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay,\n",
        "                           momentum=kwargs.get('momentum', 0.9))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer type: {optimizer_type}\")\n",
        "\n",
        "\n",
        "def get_differential_optimizer(model, backbone_lr=1e-4, classifier_lr=1e-3, \n",
        "                             optimizer_type='Adam', weight_decay=1e-4):\n",
        "    \"\"\"\n",
        "    Create optimizer with different learning rates for backbone and classifier.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model with 'backbone'/'features' and 'classifier' attributes\n",
        "        backbone_lr (float): Learning rate for backbone/features\n",
        "        classifier_lr (float): Learning rate for classifier\n",
        "        optimizer_type (str): Type of optimizer\n",
        "        weight_decay (float): Weight decay\n",
        "        \n",
        "    Returns:\n",
        "        PyTorch optimizer\n",
        "    \"\"\"\n",
        "    # Get backbone parameters\n",
        "    if hasattr(model, 'backbone'):\n",
        "        backbone_params = model.backbone.parameters()\n",
        "    elif hasattr(model, 'features'):\n",
        "        backbone_params = model.features.parameters()\n",
        "    else:\n",
        "        raise ValueError(\"Model must have 'backbone' or 'features' attribute\")\n",
        "    \n",
        "    # Get classifier parameters\n",
        "    classifier_params = model.classifier.parameters()\n",
        "    \n",
        "    # Create parameter groups\n",
        "    param_groups = [\n",
        "        {'params': backbone_params, 'lr': backbone_lr, 'name': 'backbone'},\n",
        "        {'params': classifier_params, 'lr': classifier_lr, 'name': 'classifier'}\n",
        "    ]\n",
        "    \n",
        "    if optimizer_type.lower() == 'adam':\n",
        "        return optim.Adam(param_groups, weight_decay=weight_decay)\n",
        "    elif optimizer_type.lower() == 'adamw':\n",
        "        return optim.AdamW(param_groups, weight_decay=weight_decay)\n",
        "    elif optimizer_type.lower() == 'sgd':\n",
        "        return optim.SGD(param_groups, weight_decay=weight_decay, momentum=0.9)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer type: {optimizer_type}\")\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, scheduler_type='StepLR', **kwargs):\n",
        "    \"\"\"\n",
        "    Factory function to create learning rate schedulers.\n",
        "    \n",
        "    Args:\n",
        "        optimizer: PyTorch optimizer\n",
        "        scheduler_type (str): Type of scheduler\n",
        "        **kwargs: Scheduler-specific arguments\n",
        "        \n",
        "    Returns:\n",
        "        PyTorch scheduler\n",
        "    \"\"\"\n",
        "    if scheduler_type.lower() == 'steplr':\n",
        "        step_size = kwargs.get('step_size', 7)\n",
        "        gamma = kwargs.get('gamma', 0.1)\n",
        "        return StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "    elif scheduler_type.lower() == 'cosineannealinglr':\n",
        "        T_max = kwargs.get('T_max', 10)\n",
        "        eta_min = kwargs.get('eta_min', 0)\n",
        "        return CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
        "    elif scheduler_type.lower() == 'reducelronplateau':\n",
        "        mode = kwargs.get('mode', 'min')\n",
        "        factor = kwargs.get('factor', 0.5)\n",
        "        patience = kwargs.get('patience', 5)\n",
        "        return ReduceLROnPlateau(optimizer, mode=mode, factor=factor, patience=patience)\n",
        "    elif scheduler_type.lower() == 'onecyclelr':\n",
        "        max_lr = kwargs.get('max_lr', 1e-3)\n",
        "        total_steps = kwargs.get('total_steps', 1000)\n",
        "        return OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported scheduler type: {scheduler_type}\")\n",
        "\n",
        "\n",
        "print(\"Optimizer and scheduler functions created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch_num=1, print_freq=100):\n",
        "    \"\"\"\n",
        "    Train model for one epoch.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        train_loader: Training data loader\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimizer\n",
        "        device: Device (cpu/cuda)\n",
        "        epoch_num (int): Current epoch number\n",
        "        print_freq (int): Print frequency\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct_predictions += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_predictions += target.size(0)\n",
        "        \n",
        "        # Print progress\n",
        "        if batch_idx % print_freq == 0:\n",
        "            current_acc = 100. * correct_predictions / total_predictions\n",
        "            current_loss = running_loss / (batch_idx + 1)\n",
        "            print(f'Epoch {epoch_num}, Batch [{batch_idx}/{len(train_loader)}]: '\n",
        "                  f'Loss: {current_loss:.4f}, Acc: {current_acc:.2f}%')\n",
        "    \n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100. * correct_predictions / total_predictions\n",
        "    \n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validate model for one epoch.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        val_loader: Validation data loader\n",
        "        criterion: Loss function\n",
        "        device: Device (cpu/cuda)\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy, predictions, targets)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            val_loss += criterion(output, target).item()\n",
        "            \n",
        "            # Predictions\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct_predictions += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total_predictions += target.size(0)\n",
        "            \n",
        "            # Store for detailed metrics\n",
        "            all_predictions.extend(pred.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    \n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    accuracy = 100. * correct_predictions / total_predictions\n",
        "    \n",
        "    return avg_loss, accuracy, all_predictions, all_targets\n",
        "\n",
        "\n",
        "def train_epoch_enhanced(model, train_loader, criterion, optimizer, device, \n",
        "                        gradient_accumulation_steps=1, max_grad_norm=1.0, \n",
        "                        epoch_num=1, print_freq=100):\n",
        "    \"\"\"\n",
        "    Enhanced training epoch with gradient accumulation and clipping.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        train_loader: Training data loader\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimizer\n",
        "        device: Device (cpu/cuda)\n",
        "        gradient_accumulation_steps (int): Steps to accumulate gradients\n",
        "        max_grad_norm (float): Maximum gradient norm for clipping\n",
        "        epoch_num (int): Current epoch number\n",
        "        print_freq (int): Print frequency\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        # Scale loss for gradient accumulation\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update weights every gradient_accumulation_steps\n",
        "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "            # Gradient clipping\n",
        "            if max_grad_norm > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item() * gradient_accumulation_steps\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct_predictions += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_predictions += target.size(0)\n",
        "        \n",
        "        # Print progress\n",
        "        if batch_idx % print_freq == 0:\n",
        "            current_acc = 100. * correct_predictions / total_predictions\n",
        "            current_loss = running_loss / (batch_idx + 1)\n",
        "            print(f'Epoch {epoch_num}, Batch [{batch_idx}/{len(train_loader)}]: '\n",
        "                  f'Loss: {current_loss:.4f}, Acc: {current_acc:.2f}%')\n",
        "    \n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100. * correct_predictions / total_predictions\n",
        "    \n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "print(\"Training functions created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Complete Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=25, \n",
        "               loss_type='CrossEntropy', optimizer_type='Adam', scheduler_type='StepLR',\n",
        "               lr=1e-3, weight_decay=1e-4, class_weights=None,\n",
        "               save_path='best_model.pth', patience=10, device='cpu',\n",
        "               enhanced_training=False, **kwargs):\n",
        "    \"\"\"\n",
        "    Complete training pipeline for emotion recognition models.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model to train\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        num_epochs (int): Number of epochs to train\n",
        "        loss_type (str): Type of loss function\n",
        "        optimizer_type (str): Type of optimizer\n",
        "        scheduler_type (str): Type of scheduler\n",
        "        lr (float): Learning rate\n",
        "        weight_decay (float): Weight decay\n",
        "        class_weights (torch.Tensor): Class weights for loss function\n",
        "        save_path (str): Path to save best model\n",
        "        patience (int): Early stopping patience\n",
        "        device (str): Device to use\n",
        "        enhanced_training (bool): Use enhanced training features\n",
        "        **kwargs: Additional arguments\n",
        "        \n",
        "    Returns:\n",
        "        dict: Training history\n",
        "    \"\"\"\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "    print(f\"Model: {model.__class__.__name__}\")\n",
        "    print(f\"Loss: {loss_type}\")\n",
        "    print(f\"Optimizer: {optimizer_type} (lr={lr})\")\n",
        "    print(f\"Scheduler: {scheduler_type}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Enhanced training: {enhanced_training}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Create loss function\n",
        "    criterion = get_loss_function(loss_type, class_weights, **kwargs)\n",
        "    \n",
        "    # Create optimizer\n",
        "    if 'backbone_lr' in kwargs and 'classifier_lr' in kwargs:\n",
        "        optimizer = get_differential_optimizer(\n",
        "            model, kwargs['backbone_lr'], kwargs['classifier_lr'], \n",
        "            optimizer_type, weight_decay\n",
        "        )\n",
        "        print(f\"Using differential learning rates: backbone={kwargs['backbone_lr']}, classifier={kwargs['classifier_lr']}\")\n",
        "    else:\n",
        "        optimizer = get_optimizer(model, optimizer_type, lr, weight_decay)\n",
        "    \n",
        "    # Create scheduler\n",
        "    scheduler = get_scheduler(optimizer, scheduler_type, **kwargs)\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_losses': [],\n",
        "        'train_accuracies': [],\n",
        "        'val_losses': [],\n",
        "        'val_accuracies': [],\n",
        "        'learning_rates': [],\n",
        "        'best_val_acc': 0.0,\n",
        "        'best_epoch': 0\n",
        "    }\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Training\n",
        "        if enhanced_training:\n",
        "            train_loss, train_acc = train_epoch_enhanced(\n",
        "                model, train_loader, criterion, optimizer, device, \n",
        "                gradient_accumulation_steps=kwargs.get('gradient_accumulation_steps', 1),\n",
        "                max_grad_norm=kwargs.get('max_grad_norm', 1.0),\n",
        "                epoch_num=epoch,\n",
        "                print_freq=kwargs.get('print_freq', 100)\n",
        "            )\n",
        "        else:\n",
        "            train_loss, train_acc = train_epoch(\n",
        "                model, train_loader, criterion, optimizer, device, \n",
        "                epoch_num=epoch,\n",
        "                print_freq=kwargs.get('print_freq', 100)\n",
        "            )\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, val_pred, val_target = validate_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "        \n",
        "        # Update learning rate\n",
        "        if isinstance(scheduler, ReduceLROnPlateau):\n",
        "            scheduler.step(val_loss)\n",
        "        else:\n",
        "            scheduler.step()\n",
        "        \n",
        "        # Get current learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Update history\n",
        "        history['train_losses'].append(train_loss)\n",
        "        history['train_accuracies'].append(train_acc)\n",
        "        history['val_losses'].append(val_loss)\n",
        "        history['val_accuracies'].append(val_acc)\n",
        "        history['learning_rates'].append(current_lr)\n",
        "        \n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            history['best_val_acc'] = best_val_acc\n",
        "            history['best_epoch'] = epoch\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        # Print epoch summary\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs} Summary:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"  LR: {current_lr:.2e}, Time: {epoch_time:.2f}s\")\n",
        "        print(f\"  Best Val Acc: {best_val_acc:.2f}% (Epoch {history['best_epoch']})\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {patience} epochs without improvement\")\n",
        "            break\n",
        "    \n",
        "    # Save best model\n",
        "    if best_model_state is not None:\n",
        "        torch.save({\n",
        "            'epoch': history['best_epoch'],\n",
        "            'model_state_dict': best_model_state,\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_val_acc': best_val_acc,\n",
        "            'history': history\n",
        "        }, save_path)\n",
        "        print(f\"Best model saved to {save_path}\")\n",
        "        \n",
        "        # Load best model weights\n",
        "        model.load_state_dict(best_model_state)\n",
        "    \n",
        "    print(f\"\\nTraining completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {history['best_epoch']}\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "\n",
        "print(\"Complete training pipeline ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device, label_map=None, print_results=True):\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        test_loader: Test data loader\n",
        "        device: Device to use\n",
        "        label_map (dict): Mapping from class names to indices\n",
        "        print_results (bool): Whether to print results\n",
        "        \n",
        "    Returns:\n",
        "        dict: Evaluation results\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_probabilities = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            output = model(data)\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "            predictions = output.argmax(dim=1)\n",
        "            \n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_targets, all_predictions)\n",
        "    f1_macro = f1_score(all_targets, all_predictions, average='macro')\n",
        "    f1_weighted = f1_score(all_targets, all_predictions, average='weighted')\n",
        "    \n",
        "    # Create class names\n",
        "    if label_map is not None:\n",
        "        class_names = [k for k, v in sorted(label_map.items(), key=lambda x: x[1])]\n",
        "    else:\n",
        "        unique_labels = sorted(list(set(all_targets)))\n",
        "        class_names = [f'Class_{i}' for i in unique_labels]\n",
        "    \n",
        "    # Classification report\n",
        "    report = classification_report(all_targets, all_predictions, \n",
        "                                 target_names=class_names, \n",
        "                                 output_dict=True, zero_division=0)\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_predictions)\n",
        "    \n",
        "    results = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'classification_report': report,\n",
        "        'confusion_matrix': cm,\n",
        "        'predictions': all_predictions,\n",
        "        'targets': all_targets,\n",
        "        'probabilities': np.array(all_probabilities),\n",
        "        'class_names': class_names\n",
        "    }\n",
        "    \n",
        "    if print_results:\n",
        "        print(f\"\\nModel Evaluation Results:\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "        print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
        "        print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "        print(f\"\\nPer-Class Results:\")\n",
        "        for class_name in class_names:\n",
        "            if class_name in report:\n",
        "                precision = report[class_name]['precision']\n",
        "                recall = report[class_name]['recall']\n",
        "                f1 = report[class_name]['f1-score']\n",
        "                support = report[class_name]['support']\n",
        "                print(f\"  {class_name:10s}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}, N={support}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(confusion_matrix, class_names, normalize=True, figsize=(10, 8)):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix.\n",
        "    \n",
        "    Args:\n",
        "        confusion_matrix (np.array): Confusion matrix\n",
        "        class_names (list): List of class names\n",
        "        normalize (bool): Whether to normalize the matrix\n",
        "        figsize (tuple): Figure size\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
        "        title = 'Normalized Confusion Matrix'\n",
        "        fmt = '.2f'\n",
        "    else:\n",
        "        cm = confusion_matrix\n",
        "        title = 'Confusion Matrix'\n",
        "        fmt = 'd'\n",
        "    \n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_history(history, figsize=(15, 5)):\n",
        "    \"\"\"\n",
        "    Plot training history.\n",
        "    \n",
        "    Args:\n",
        "        history (dict): Training history\n",
        "        figsize (tuple): Figure size\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "    \n",
        "    epochs = range(1, len(history['train_losses']) + 1)\n",
        "    \n",
        "    # Loss plot\n",
        "    axes[0].plot(epochs, history['train_losses'], 'b-', label='Training Loss')\n",
        "    axes[0].plot(epochs, history['val_losses'], 'r-', label='Validation Loss')\n",
        "    axes[0].set_title('Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "    \n",
        "    # Accuracy plot\n",
        "    axes[1].plot(epochs, history['train_accuracies'], 'b-', label='Training Accuracy')\n",
        "    axes[1].plot(epochs, history['val_accuracies'], 'r-', label='Validation Accuracy')\n",
        "    axes[1].set_title('Accuracy')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy (%)')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "    \n",
        "    # Learning rate plot\n",
        "    axes[2].plot(epochs, history['learning_rates'], 'g-')\n",
        "    axes[2].set_title('Learning Rate')\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('Learning Rate')\n",
        "    axes[2].set_yscale('log')\n",
        "    axes[2].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print best performance\n",
        "    print(f\"Best validation accuracy: {history['best_val_acc']:.2f}% at epoch {history['best_epoch']}\")\n",
        "\n",
        "\n",
        "print(\"Evaluation functions created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, epoch, val_acc, history, filepath):\n",
        "    \"\"\"\n",
        "    Save model checkpoint.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        optimizer: PyTorch optimizer\n",
        "        epoch (int): Current epoch\n",
        "        val_acc (float): Validation accuracy\n",
        "        history (dict): Training history\n",
        "        filepath (str): Path to save checkpoint\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_acc': val_acc,\n",
        "        'history': history\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved to {filepath}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filepath, device='cpu'):\n",
        "    \"\"\"\n",
        "    Load model checkpoint.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        optimizer: PyTorch optimizer\n",
        "        filepath (str): Path to checkpoint\n",
        "        device (str): Device to load to\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (epoch, val_acc, history)\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    epoch = checkpoint['epoch']\n",
        "    val_acc = checkpoint['val_acc']\n",
        "    history = checkpoint.get('history', {})\n",
        "    \n",
        "    print(f\"Checkpoint loaded from {filepath}\")\n",
        "    print(f\"Epoch: {epoch}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "    \n",
        "    return epoch, val_acc, history\n",
        "\n",
        "\n",
        "def save_model_for_inference(model, filepath, label_map=None, transforms_config=None, \n",
        "                           model_config=None):\n",
        "    \"\"\"\n",
        "    Save model for inference with metadata.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        filepath (str): Path to save model\n",
        "        label_map (dict): Label mapping\n",
        "        transforms_config (dict): Transform configuration\n",
        "        model_config (dict): Model configuration\n",
        "    \"\"\"\n",
        "    model_info = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'model_config': model_config or {},\n",
        "        'label_map': label_map or {},\n",
        "        'transforms_config': transforms_config or {},\n",
        "        'model_class': model.__class__.__name__\n",
        "    }\n",
        "    \n",
        "    torch.save(model_info, filepath)\n",
        "    print(f\"Model saved for inference: {filepath}\")\n",
        "    print(f\"Model class: {model.__class__.__name__}\")\n",
        "    if label_map:\n",
        "        print(f\"Classes: {list(label_map.keys())}\")\n",
        "\n",
        "\n",
        "def load_model_for_inference(model_class, filepath, device='cpu'):\n",
        "    \"\"\"\n",
        "    Load model for inference.\n",
        "    \n",
        "    Args:\n",
        "        model_class: Model class constructor\n",
        "        filepath (str): Path to model file\n",
        "        device (str): Device to load to\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (model, label_map, transforms_config)\n",
        "    \"\"\"\n",
        "    model_info = torch.load(filepath, map_location=device)\n",
        "    \n",
        "    # Get model configuration\n",
        "    model_config = model_info.get('model_config', {})\n",
        "    label_map = model_info.get('label_map', {})\n",
        "    transforms_config = model_info.get('transforms_config', {})\n",
        "    \n",
        "    # Create model with configuration\n",
        "    if model_config:\n",
        "        model = model_class(**model_config)\n",
        "    else:\n",
        "        # Try to create with default parameters\n",
        "        num_classes = len(label_map) if label_map else 7\n",
        "        model = model_class(num_classes=num_classes)\n",
        "    \n",
        "    # Load state dict\n",
        "    model.load_state_dict(model_info['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Model loaded for inference from {filepath}\")\n",
        "    print(f\"Classes: {list(label_map.keys()) if label_map else 'Unknown'}\")\n",
        "    \n",
        "    return model, label_map, transforms_config\n",
        "\n",
        "\n",
        "print(\"Checkpointing functions created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Quick Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quick_train_baseline(model, train_loader, val_loader, num_epochs=20, device='cpu'):\n",
        "    \"\"\"\n",
        "    Quick training function for baseline CNN models.\n",
        "    \n",
        "    Args:\n",
        "        model: CNN baseline model\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        num_epochs (int): Number of epochs\n",
        "        device (str): Device to use\n",
        "        \n",
        "    Returns:\n",
        "        dict: Training history\n",
        "    \"\"\"\n",
        "    return train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        loss_type='CrossEntropy',\n",
        "        optimizer_type='Adam',\n",
        "        scheduler_type='StepLR',\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        save_path='baseline_model.pth',\n",
        "        device=device,\n",
        "        step_size=7,\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "\n",
        "def quick_train_transfer(model, train_loader, val_loader, num_epochs=20, \n",
        "                        class_weights=None, device='cpu'):\n",
        "    \"\"\"\n",
        "    Quick training function for transfer learning models.\n",
        "    \n",
        "    Args:\n",
        "        model: Transfer learning model\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        num_epochs (int): Number of epochs\n",
        "        class_weights (torch.Tensor): Class weights\n",
        "        device (str): Device to use\n",
        "        \n",
        "    Returns:\n",
        "        dict: Training history\n",
        "    \"\"\"\n",
        "    return train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        loss_type='CrossEntropy',\n",
        "        optimizer_type='Adam',\n",
        "        scheduler_type='ReduceLROnPlateau',\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        class_weights=class_weights,\n",
        "        save_path='transfer_model.pth',\n",
        "        device=device,\n",
        "        backbone_lr=1e-4,  # Lower LR for pre-trained features\n",
        "        classifier_lr=1e-3,  # Higher LR for new classifier\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=5\n",
        "    )\n",
        "\n",
        "\n",
        "def quick_train_enhanced(model, train_loader, val_loader, num_epochs=30, \n",
        "                        class_weights=None, device='cpu'):\n",
        "    \"\"\"\n",
        "    Quick training function for enhanced models with advanced features.\n",
        "    \n",
        "    Args:\n",
        "        model: Enhanced model\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        num_epochs (int): Number of epochs\n",
        "        class_weights (torch.Tensor): Class weights\n",
        "        device (str): Device to use\n",
        "        \n",
        "    Returns:\n",
        "        dict: Training history\n",
        "    \"\"\"\n",
        "    return train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        loss_type='WeightedFocal',\n",
        "        optimizer_type='AdamW',\n",
        "        scheduler_type='CosineAnnealingLR',\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        class_weights=class_weights,\n",
        "        save_path='enhanced_model.pth',\n",
        "        device=device,\n",
        "        enhanced_training=True,\n",
        "        alpha=1,\n",
        "        gamma=2,\n",
        "        T_max=num_epochs,\n",
        "        gradient_accumulation_steps=2,\n",
        "        max_grad_norm=1.0\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Quick training functions created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides a complete training pipeline for visual emotion recognition:\n",
        "\n",
        "### Core Components:\n",
        "1. **Loss Functions**: CrossEntropy, Label Smoothing, Focal Loss, Weighted Focal Loss\n",
        "2. **Optimizers**: Adam, AdamW, SGD, RMSprop with differential learning rates\n",
        "3. **Schedulers**: StepLR, CosineAnnealingLR, ReduceLROnPlateau, OneCycleLR\n",
        "4. **Training Functions**: Basic and enhanced training with gradient accumulation/clipping\n",
        "5. **Evaluation**: Comprehensive metrics, confusion matrices, classification reports\n",
        "6. **Visualization**: Training curves, confusion matrices\n",
        "7. **Checkpointing**: Save/load models with metadata\n",
        "\n",
        "### Key Features:\n",
        "- **Flexible Training**: Support for different models, losses, optimizers\n",
        "- **Advanced Training**: Gradient accumulation, clipping, differential learning rates\n",
        "- **Early Stopping**: Prevent overfitting with patience-based stopping\n",
        "- **Class Balancing**: Handle imbalanced datasets with weighted losses\n",
        "- **Comprehensive Evaluation**: Multiple metrics and visualizations\n",
        "- **Easy Checkpointing**: Save/load models with full metadata\n",
        "- **Quick Functions**: Pre-configured training for common scenarios\n",
        "\n",
        "All functionality is self-contained within this notebook and doesn't require the src folder structure.\n",
        "\n",
        "### Usage Examples:\n",
        "```python\n",
        "# Basic training\n",
        "history = train_model(model, train_loader, val_loader, num_epochs=25)\n",
        "\n",
        "# Enhanced training with focal loss\n",
        "history = train_model(\n",
        "    model, train_loader, val_loader,\n",
        "    loss_type='WeightedFocal', \n",
        "    enhanced_training=True,\n",
        "    alpha=1, gamma=2\n",
        ")\n",
        "\n",
        "# Evaluation\n",
        "results = evaluate_model(model, test_loader, device, label_map)\n",
        "plot_confusion_matrix(results['confusion_matrix'], results['class_names'])\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}