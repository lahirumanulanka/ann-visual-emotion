{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahirumanulanka/ann-visual-emotion/blob/reduce_features/notebooks/CNN_Transfer_Learning_not_over_fittingv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpOTlhgalZoU",
        "outputId": "3e258346-e211-4b2d-c690-928a4ff13579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ann-visual-emotion' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b added_new_dataset https://github.com/lahirumanulanka/ann-visual-emotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNYScK2vlU3V"
      },
      "source": [
        "# CNN Transfer Learning for Visual Emotion Recognition\n",
        "\n",
        "This notebook demonstrates how to implement transfer learning with CNN networks for visual emotion recognition. We'll use a pre-trained VGG16 model and fine-tune it for our emotion classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPhiOh1PlU3W"
      },
      "source": [
        "## Step 1: Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# Sklearn for metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "Ym1iQ_r_5x2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dce04e7-4069-402f-b839-2dd8aed25eb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Preparation\n",
        "\n",
        "First, let's set up our data paths and load the dataset splits."
      ],
      "metadata": {
        "id": "a2eA_MiJXi5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data paths (modify these if running locally)\n",
        "\n",
        "PROJECT_ROOT = Path('/content/ann-visual-emotion')\n",
        "CSV_TRAIN = PROJECT_ROOT / 'data/processed/EmoSet_splits/train.csv'\n",
        "CSV_VAL = PROJECT_ROOT / 'data/processed/EmoSet_splits/val.csv'\n",
        "CSV_TEST = PROJECT_ROOT / 'data/processed/EmoSet_splits/test.csv'\n",
        "LABEL_MAP_PATH = PROJECT_ROOT / 'data/processed/EmoSet_splits/label_map.json'\n",
        "DATA_DIR = PROJECT_ROOT / 'data/raw/EmoSet'\n",
        "\n",
        "# Check if files exist\n",
        "print(\"Checking data files...\")\n",
        "for path in [CSV_TRAIN, CSV_VAL, CSV_TEST, LABEL_MAP_PATH]:\n",
        "    if path.exists():\n",
        "        print(f\"✓ Found: {path}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {path}\")\n",
        "\n",
        "# Load label map if it exists\n",
        "if LABEL_MAP_PATH.exists():\n",
        "    with open(LABEL_MAP_PATH, 'r') as f:\n",
        "        label_map = json.load(f)\n",
        "    num_classes = len(label_map)\n",
        "    print(f'Number of emotion classes: {num_classes}')\n",
        "    print(f'Emotion classes: {list(label_map.keys())}')\n",
        "else:\n",
        "    # Create a dummy label map for testing\n",
        "    label_map = {'anger': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, 'sadness': 4, 'surprise': 5, 'neutral': 6}\n",
        "    num_classes = len(label_map)\n",
        "    print(f'Using dummy label map with {num_classes} classes: {list(label_map.keys())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo7o0b4sXg70",
        "outputId": "c23ebf4c-ac80-4825-dc58-253a040db280"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking data files...\n",
            "✓ Found: /content/ann-visual-emotion/data/processed/EmoSet_splits/train.csv\n",
            "✓ Found: /content/ann-visual-emotion/data/processed/EmoSet_splits/val.csv\n",
            "✓ Found: /content/ann-visual-emotion/data/processed/EmoSet_splits/test.csv\n",
            "✓ Found: /content/ann-visual-emotion/data/processed/EmoSet_splits/label_map.json\n",
            "Number of emotion classes: 5\n",
            "Emotion classes: ['angry', 'happy', 'neutral', 'sad', 'surprise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Step 2b: Generate 224×224 Resized Dataset and Updated CSV Splits\n",
        "\n",
        "In case your original FER-style images are 48×48 (grayscale) and you want to:\n",
        "\n",
        "1. Upscale them to 224×224 (required for many ImageNet pre-trained backbones).\n",
        "2. Store the resized copies in a separate folder structure (`class` subfolders).\n",
        "3. Regenerate new CSV split files (`train_224.csv`, `val_224.csv`, `test_224.csv`) that point directly to the resized images.\n",
        "4. Produce an updated stats file (`stats_resized_224.json`) summarizing counts per class.\n",
        "\n",
        "You can run the following cells once. They are resume-friendly (will skip already processed files by default). Set `FORCE_OVERWRITE = True` to re-create images.\n",
        "\n",
        "If you already have resized images / CSVs, you can skip this section and set `USE_RESIZED_SPLITS = True` later to automatically pick them up for training."
      ],
      "metadata": {
        "id": "faaDFRLlXojJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Configuration for resizing\n",
        "RESIZE_ROOT = PROJECT_ROOT  # base project root (already defined earlier)\n",
        "RAW_IMG_ROOT = PROJECT_ROOT / 'data/raw/EmoSet'\n",
        "PROCESSED_SPLITS_DIR = PROJECT_ROOT / 'data/processed/EmoSet_splits'\n",
        "RESIZED_OUT_DIR = PROJECT_ROOT / 'data/processed/EmoSet_resized_224'\n",
        "RESIZED_IMG_DIR = RESIZED_OUT_DIR / 'images_224'\n",
        "RESIZED_SPLIT_DIR = RESIZED_OUT_DIR / 'splits'\n",
        "RESIZED_STATS_PATH = RESIZED_OUT_DIR / 'stats_resized_224.json'\n",
        "\n",
        "# Control flags\n",
        "SKIP_IF_EXISTS = True       # Don't re-generate images if target file exists\n",
        "FORCE_OVERWRITE = False     # If True, ignore SKIP_IF_EXISTS\n",
        "USE_RESIZED_SPLITS = True   # Later, if True and splits exist, training section will use them\n",
        "GRAYSCALE_SOURCE = True     # If original images are grayscale 48x48\n",
        "TARGET_SIZE = (224, 224)\n",
        "\n",
        "RESIZED_IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESIZED_SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SUPPORTED_PATH_COLS  = [\"path\", \"filepath\", \"file_path\", \"image_path\", \"img_path\"]\n",
        "SUPPORTED_LABEL_COLS = [\"label\", \"class\", \"emotion\", \"target\", \"y\", \"label_id\"]\n",
        "\n",
        "\n",
        "def robust_read_csv(csv_path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.rename(columns={c: c.strip().lower() for c in df.columns}, inplace=True)\n",
        "    path_col = next((c for c in SUPPORTED_PATH_COLS if c in df.columns), None)\n",
        "    label_col = next((c for c in SUPPORTED_LABEL_COLS if c in df.columns), None)\n",
        "    if path_col is None or label_col is None:\n",
        "        raise AssertionError(f\"CSV must contain a path+label column. Found columns: {list(df.columns)}\")\n",
        "    out = df[[path_col, label_col]].copy()\n",
        "    out.columns = [\"path\", \"label\"]\n",
        "    out[\"label\"] = out[\"label\"].astype(str)\n",
        "    return out\n",
        "\n",
        "\n",
        "def resolve_path(p: str) -> Path:\n",
        "    pth = Path(p)\n",
        "    if pth.exists():\n",
        "        return pth\n",
        "    alt = RAW_IMG_ROOT / pth\n",
        "    return alt if alt.exists() else alt  # return alt even if missing for error reporting\n",
        "\n",
        "\n",
        "def out_path_for(in_path: Path, label: str) -> Path:\n",
        "    out_dir = RESIZED_IMG_DIR / label\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    return out_dir / (in_path.stem + '.jpg')\n",
        "\n",
        "\n",
        "def resize_to_target(img: Image.Image) -> Image.Image:\n",
        "    if GRAYSCALE_SOURCE:\n",
        "        img = img.convert('L')  # ensure grayscale\n",
        "        # convert to 3-channel RGB for pre-trained models after resizing\n",
        "        img = img.resize(TARGET_SIZE, resample=Image.BICUBIC).convert('RGB')\n",
        "    else:\n",
        "        img = img.convert('RGB').resize(TARGET_SIZE, resample=Image.BICUBIC)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_split(csv_path: Path, out_csv_path: Path):\n",
        "    if not csv_path.exists():\n",
        "        print(f\"[skip] Missing split file: {csv_path}\")\n",
        "        return None\n",
        "\n",
        "    df = robust_read_csv(csv_path)\n",
        "    rows = []\n",
        "    print(f\"Processing {csv_path.name} -> {out_csv_path.name}  (n={len(df)})\")\n",
        "\n",
        "    for r in tqdm(df.itertuples(index=False), total=len(df)):\n",
        "        in_path = resolve_path(r.path)\n",
        "        label = str(r.label)\n",
        "        out_img_path = out_path_for(in_path, label)\n",
        "\n",
        "        if FORCE_OVERWRITE is False and SKIP_IF_EXISTS and out_img_path.exists():\n",
        "            rows.append({\"path\": str(out_img_path.relative_to(PROJECT_ROOT)), \"label\": label})\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            img = Image.open(in_path)\n",
        "            img224 = resize_to_target(img)\n",
        "            out_img_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            img224.save(out_img_path, format='JPEG', quality=95)\n",
        "            rows.append({\"path\": str(out_img_path.relative_to(PROJECT_ROOT)), \"label\": label})\n",
        "        except Exception as e:\n",
        "            print(f\"[error] {in_path}: {e}\")\n",
        "\n",
        "    if rows:\n",
        "        out_df = pd.DataFrame(rows)\n",
        "        out_df.to_csv(out_csv_path, index=False)\n",
        "        print(f\"Wrote {out_csv_path} (rows={len(out_df)})\")\n",
        "    else:\n",
        "        print(f\"No rows written for {csv_path}\")\n",
        "    return rows\n",
        "\n",
        "# Run resizing for splits if original CSVs exist\n",
        "resized_train_csv = RESIZED_SPLIT_DIR / 'train_224.csv'\n",
        "resized_val_csv = RESIZED_SPLIT_DIR / 'val_224.csv'\n",
        "resized_test_csv = RESIZED_SPLIT_DIR / 'test_224.csv'\n",
        "\n",
        "if CSV_TRAIN.exists():\n",
        "    process_split(CSV_TRAIN, resized_train_csv)\n",
        "if CSV_VAL.exists():\n",
        "    process_split(CSV_VAL, resized_val_csv)\n",
        "if CSV_TEST.exists():\n",
        "    process_split(CSV_TEST, resized_test_csv)\n",
        "\n",
        "# Aggregate stats\n",
        "if resized_train_csv.exists():\n",
        "    train_df_r = pd.read_csv(resized_train_csv)\n",
        "    cls_counts = train_df_r['label'].value_counts().to_dict()\n",
        "    stats = {\n",
        "        'total_train_images': int(len(train_df_r)),\n",
        "        'classes': cls_counts,\n",
        "        'num_classes': int(len(cls_counts)),\n",
        "        'target_size': list(TARGET_SIZE),\n",
        "        'grayscale_source': GRAYSCALE_SOURCE,\n",
        "    }\n",
        "    with open(RESIZED_STATS_PATH, 'w') as f:\n",
        "        json.dump(stats, f, indent=2)\n",
        "    print(f\"Wrote stats: {RESIZED_STATS_PATH}\")\n",
        "else:\n",
        "    print(\"Resized training CSV not found; skipping stats generation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAp1FRAhXmHc",
        "outputId": "ab04b1d5-b198-4d18-96fd-b3b1fb9f533e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train.csv -> train_224.csv  (n=20150)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20150/20150 [00:02<00:00, 7401.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/ann-visual-emotion/data/processed/EmoSet_resized_224/splits/train_224.csv (rows=20150)\n",
            "Processing val.csv -> val_224.csv  (n=4030)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4030/4030 [00:00<00:00, 7477.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/ann-visual-emotion/data/processed/EmoSet_resized_224/splits/val_224.csv (rows=4030)\n",
            "Processing test.csv -> test_224.csv  (n=1005)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1005/1005 [00:00<00:00, 7982.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/ann-visual-emotion/data/processed/EmoSet_resized_224/splits/test_224.csv (rows=1005)\n",
            "Wrote stats: /content/ann-visual-emotion/data/processed/EmoSet_resized_224/stats_resized_224.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Custom Dataset Class\n",
        "\n",
        "We'll create a custom dataset class that handles both grayscale and RGB images for transfer learning."
      ],
      "metadata": {
        "id": "p12QLbHUYN-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for emotion recognition.\n",
        "    Supports both grayscale and RGB images for transfer learning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, root_dir, transform=None, label_map=None, rgb=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe: pandas DataFrame with image paths and labels\n",
        "            root_dir: Root directory containing images\n",
        "            transform: Optional transform to be applied on images\n",
        "            label_map: Dictionary mapping emotion names to indices\n",
        "            rgb: If True, convert grayscale images to RGB for pre-trained models\n",
        "        \"\"\"\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.label_map = label_map\n",
        "        self.rgb = rgb\n",
        "\n",
        "        # Auto-detect column names\n",
        "        possible_path_cols = [c for c in self.df.columns if 'path' in c.lower() or 'file' in c.lower() or 'image' in c.lower()]\n",
        "        self.path_col = possible_path_cols[0] if possible_path_cols else self.df.columns[0]\n",
        "\n",
        "        possible_label_cols = [c for c in self.df.columns if 'label' in c.lower() or 'class' in c.lower() or 'emotion' in c.lower()]\n",
        "        self.label_col = possible_label_cols[0] if possible_label_cols else self.df.columns[1]\n",
        "\n",
        "        print(f\"Using columns - Path: '{self.path_col}', Label: '{self.label_col}'\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        rel_path = row[self.path_col]\n",
        "        label = row[self.label_col]\n",
        "\n",
        "        # Convert label to index if needed\n",
        "        if self.label_map and isinstance(label, str):\n",
        "            label_idx = self.label_map[label]\n",
        "        else:\n",
        "            label_idx = int(label)\n",
        "\n",
        "        # Load image\n",
        "        img_path = self.root_dir / rel_path\n",
        "\n",
        "        try:\n",
        "            if self.rgb:\n",
        "                # Load as RGB for pre-trained models\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "            else:\n",
        "                # Load as grayscale\n",
        "                image = Image.open(img_path).convert('L')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a dummy image in case of error\n",
        "            if self.rgb:\n",
        "                image = Image.new('RGB', (48, 48), color='black')\n",
        "            else:\n",
        "                image = Image.new('L', (48, 48), color='black')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label_idx"
      ],
      "metadata": {
        "id": "FEK1_AcuYNwx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Data Transforms for Transfer Learning\n",
        "\n",
        "For transfer learning with pre-trained models, we need to:\n",
        "1. Resize images to the expected input size (224x224 for VGG)\n",
        "2. Normalize with ImageNet statistics\n",
        "3. Apply data augmentation for training"
      ],
      "metadata": {
        "id": "agSqBaQAYV2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grayscale normalization values derived from global_pixel_stats.json\n",
        "# global_mean_mean ~127.03, global_mean_std ~34.50 (values in [0,255]) so scale to [0,1]\n",
        "GRAYSCALE_MEAN = [128.15308723372334 / 255.0]\n",
        "GRAYSCALE_STD = [33.58377534716462 / 255.0]\n",
        "\n",
        "# ImageNet normalization (always define for potential use)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "INPUT_SIZE = 224  # Already resized dataset\n",
        "USE_GRAYSCALE = False\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.75, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.05),\n",
        "    transforms.RandomGrayscale(p=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "# if USE_GRAYSCALE:\n",
        "#     train_transform = transforms.Compose([\n",
        "#         transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "#         transforms.RandomHorizontalFlip(p=0.5),\n",
        "#         transforms.RandomRotation(degrees=15),\n",
        "#         transforms.ToTensor(),  # Produces 1xHxW for L mode\n",
        "#         transforms.Normalize(mean=GRAYSCALE_MEAN, std=GRAYSCALE_STD)\n",
        "#     ])\n",
        "#     val_transform = transforms.Compose([\n",
        "#         transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=GRAYSCALE_MEAN, std=GRAYSCALE_STD)\n",
        "#     ])\n",
        "# else:\n",
        "#     train_transform = transforms.Compose([\n",
        "#         transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.8, 1.0)),\n",
        "#         transforms.RandomHorizontalFlip(p=0.5),\n",
        "#         transforms.RandomRotation(degrees=30),\n",
        "#         transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
        "#         transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "#     ])\n",
        "#     val_transform = transforms.Compose([\n",
        "#         transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "#     ])\n",
        "\n",
        "print(\"Data transforms created (grayscale mode):\" if USE_GRAYSCALE else \"Data transforms created (RGB mode):\")\n",
        "print(f\"- Input size: {INPUT_SIZE}x{INPUT_SIZE}\")\n",
        "if USE_GRAYSCALE:\n",
        "    print(f\"- Grayscale normalization: mean={GRAYSCALE_MEAN}, std={GRAYSCALE_STD}\")\n",
        "else:\n",
        "    print(f\"- ImageNet normalization: mean={IMAGENET_MEAN}, std={IMAGENET_STD}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeysugEaYMQX",
        "outputId": "525e0cdb-0b6f-434c-b5e1-aa7a001025a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data transforms created (RGB mode):\n",
            "- Input size: 224x224\n",
            "- ImageNet normalization: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "9hWQpGasYcgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy datasets if actual data files don't exist\n",
        "if not all(path.exists() for path in [CSV_TRAIN, CSV_VAL, CSV_TEST]):\n",
        "    print(\"Creating dummy datasets for demonstration...\")\n",
        "\n",
        "    # Create dummy data\n",
        "    dummy_data = {\n",
        "        'image_path': [f'dummy_{i}.jpg' for i in range(100)],\n",
        "        'emotion': np.random.choice(list(label_map.keys()), 100)\n",
        "    }\n",
        "\n",
        "    train_df = pd.DataFrame(dummy_data)\n",
        "    val_df = pd.DataFrame(dummy_data)\n",
        "    test_df = pd.DataFrame(dummy_data)\n",
        "\n",
        "    print(\"Using dummy datasets (replace with actual data loading when available)\")\n",
        "else:\n",
        "    # Load actual datasets, choose resized if requested\n",
        "    print(\"Loading datasets...\")\n",
        "    use_resized = False\n",
        "    if 'USE_RESIZED_SPLITS' in globals() and USE_RESIZED_SPLITS:\n",
        "        candidate_train = PROJECT_ROOT / 'data/processed/EmoSet_resized_224/splits/train_224.csv'\n",
        "        candidate_val = PROJECT_ROOT / 'data/processed/EmoSet_resized_224/splits/val_224.csv'\n",
        "        candidate_test = PROJECT_ROOT / 'data/processed/EmoSet_resized_224/splits/test_224.csv'\n",
        "        if candidate_train.exists():\n",
        "            print(\"✓ Using resized 224×224 split CSVs\")\n",
        "            CSV_TRAIN_ACTIVE = candidate_train\n",
        "            CSV_VAL_ACTIVE = candidate_val if candidate_val.exists() else CSV_VAL\n",
        "            CSV_TEST_ACTIVE = candidate_test if candidate_test.exists() else CSV_TEST\n",
        "            use_resized = True\n",
        "        else:\n",
        "            print(\"Resized splits not found; falling back to original CSVs\")\n",
        "            CSV_TRAIN_ACTIVE, CSV_VAL_ACTIVE, CSV_TEST_ACTIVE = CSV_TRAIN, CSV_VAL, CSV_TEST\n",
        "    else:\n",
        "        CSV_TRAIN_ACTIVE, CSV_VAL_ACTIVE, CSV_TEST_ACTIVE = CSV_TRAIN, CSV_VAL, CSV_TEST\n",
        "\n",
        "    train_df = pd.read_csv(CSV_TRAIN_ACTIVE)\n",
        "    val_df = pd.read_csv(CSV_VAL_ACTIVE) if CSV_VAL_ACTIVE.exists() else pd.read_csv(CSV_TRAIN_ACTIVE).sample(frac=0.2, random_state=42)\n",
        "    test_df = pd.read_csv(CSV_TEST_ACTIVE) if CSV_TEST_ACTIVE.exists() else pd.read_csv(CSV_TRAIN_ACTIVE).sample(frac=0.2, random_state=123)\n",
        "\n",
        "    if use_resized:\n",
        "        # Paths in resized CSVs are relative to project root already (we made them relative earlier)\n",
        "        DATA_DIR = PROJECT_ROOT  # so concatenation below works\n",
        "        print(\"DATA_DIR adjusted to project root for resized relative paths.\")\n",
        "\n",
        "print(f\"Dataset sizes:\")\n",
        "print(f\"- Training: {len(train_df)}\")\n",
        "print(f\"- Validation: {len(val_df)}\")\n",
        "print(f\"- Testing: {len(test_df)}\")\n",
        "\n",
        "# Create datasets (using RGB for transfer learning)\n",
        "train_dataset = EmotionDataset(train_df, DATA_DIR, transform=train_transform,\n",
        "                              label_map=label_map, rgb=True)\n",
        "val_dataset = EmotionDataset(val_df, DATA_DIR, transform=val_transform,\n",
        "                            label_map=label_map, rgb=True)\n",
        "test_dataset = EmotionDataset(test_df, DATA_DIR, transform=val_transform,\n",
        "                             label_map=label_map, rgb=True)\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"DataLoaders created with batch size: {BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9romfpnYUmn",
        "outputId": "dbf65988-288a-410f-a356-a6293123a441"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "✓ Using resized 224×224 split CSVs\n",
            "DATA_DIR adjusted to project root for resized relative paths.\n",
            "Dataset sizes:\n",
            "- Training: 20150\n",
            "- Validation: 4030\n",
            "- Testing: 1005\n",
            "Using columns - Path: 'path', Label: 'label'\n",
            "Using columns - Path: 'path', Label: 'label'\n",
            "Using columns - Path: 'path', Label: 'label'\n",
            "DataLoaders created with batch size: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What is Transfer Learning?\n",
        "Transfer learning is a machine learning technique where we use a model that has been trained on one task and adapt it for a related task. In our case, we'll use a CNN pre-trained on ImageNet and adapt it for emotion recognition.\n",
        "\n",
        "## Benefits of Transfer Learning:\n",
        "1. **Faster training**: We start with pre-trained weights instead of random initialization\n",
        "2. **Better performance**: Especially when we have limited training data\n",
        "3. **Lower computational requirements**: Less training time needed\n",
        "4. **Proven feature extractors**: Pre-trained networks have learned robust low-level features\n",
        "\n",
        "## CNN Transfer Learning Model\n",
        "\n",
        "Now we'll create our transfer learning model using a pre-trained VGG16 network.\n",
        "\n",
        "### Transfer Learning Strategies:\n",
        "1. **Feature Extraction**: Freeze pre-trained layers, only train classifier\n",
        "2. **Fine-tuning**: Train all layers with very small learning rate\n",
        "3. **Gradual unfreezing**: Start with frozen layers, gradually unfreeze\n",
        "\n",
        "We'll implement strategy #2 (fine-tuning) as it typically gives the best results."
      ],
      "metadata": {
        "id": "fOjMf63_Y4ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from torchvision.models import (\n",
        "    VGG16_Weights, VGG19_Weights, AlexNet_Weights,\n",
        "    ResNet18_Weights\n",
        ")\n",
        "\n",
        "class CNNTransferLearning(nn.Module):\n",
        "    \"\"\"\n",
        "    Transfer Learning head over a pretrained backbone.\n",
        "    - Supports: vgg16, vgg19, alexnet, resnet18\n",
        "    - Uses AdaptiveAvgPool2d((1,1)) so output dim = last conv channels (robust to input size)\n",
        "    - Freeze strategy: 'all' (features frozen), 'none' (full finetune), 'partial' (unfreeze last block only for VGG/ResNet)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes: int,\n",
        "                 backbone: str = 'vgg16',\n",
        "                 pretrained: bool = True,\n",
        "                 freeze_mode: str = 'all',   # 'all' | 'none' | 'partial'\n",
        "                 classifier_hidden: int = 512 # compact head since 1x1 pooling\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.backbone_name = backbone.lower()\n",
        "        self.num_classes = num_classes\n",
        "        self.freeze_mode = freeze_mode\n",
        "\n",
        "        # ---- Load backbone with new Weights API\n",
        "        if self.backbone_name == 'vgg16':\n",
        "            weights = VGG16_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            net = models.vgg16(weights=weights)\n",
        "            self.features = net.features                  # conv stack\n",
        "            last_channels = 512                           # VGG16 last conv planes\n",
        "        elif self.backbone_name == 'vgg19':\n",
        "            weights = VGG19_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            net = models.vgg19(weights=weights)\n",
        "            self.features = net.features\n",
        "            last_channels = 512\n",
        "        elif self.backbone_name == 'alexnet':\n",
        "            weights = AlexNet_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            net = models.alexnet(weights=weights)\n",
        "            self.features = net.features\n",
        "            last_channels = 256                           # AlexNet last conv planes\n",
        "        elif self.backbone_name == 'resnet18':\n",
        "            weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            net = models.resnet18(weights=weights)\n",
        "            # build a conv feature extractor (everything except avgpool & fc)\n",
        "            self.features = nn.Sequential(\n",
        "                net.conv1, net.bn1, net.relu, net.maxpool,\n",
        "                net.layer1, net.layer2, net.layer3, net.layer4\n",
        "            )\n",
        "            last_channels = 512\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        # ---- Robust pooling to handle arbitrary input sizes\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))   # -> [B, C, 1, 1]\n",
        "        feature_dim = last_channels                    # flatten -> [B, C]\n",
        "\n",
        "        # ---- Freeze strategy\n",
        "        if freeze_mode not in ('all', 'none', 'partial'):\n",
        "            raise ValueError(\"freeze_mode must be 'all' | 'none' | 'partial'\")\n",
        "\n",
        "        if freeze_mode == 'all':\n",
        "            for p in self.features.parameters():\n",
        "                p.requires_grad = False\n",
        "            print(f\"[Freeze] backbone ALL layers frozen.\")\n",
        "        elif freeze_mode == 'partial':\n",
        "            # Freeze all first\n",
        "            for p in self.features.parameters():\n",
        "                p.requires_grad = False\n",
        "            # Unfreeze the last block only (heuristics per backbone)\n",
        "            self._unfreeze_last_block()\n",
        "            print(f\"[Freeze] backbone PARTIAL: last block unfrozen.\")\n",
        "        else:\n",
        "            print(f\"[Freeze] backbone NONE: full finetuning.\")\n",
        "\n",
        "        # ---- Compact classifier (since feature_dim is small)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(feature_dim, classifier_hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(classifier_hidden, num_classes)\n",
        "        )\n",
        "        self._initialize_classifier()\n",
        "\n",
        "        print(f\"Model: backbone={self.backbone_name} pretrained={pretrained} \"\n",
        "              f\"freeze_mode={freeze_mode} feature_dim={feature_dim} classes={num_classes}\")\n",
        "\n",
        "    # === helpers ===\n",
        "    def _initialize_classifier(self):\n",
        "        for m in self.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _unfreeze_last_block(self):\n",
        "        \"\"\"\n",
        "        Unfreezes only the last stage of the backbone (good for small datasets).\n",
        "        Heuristics per-architecture to keep code simple.\n",
        "        \"\"\"\n",
        "        if self.backbone_name in ('vgg16', 'vgg19', 'alexnet'):\n",
        "            # features is a Sequential of conv/pool layers; unfreeze last ~5 layers\n",
        "            # (VGG block5 roughly ~ last 5 layers with conv+relu)\n",
        "            k = 5\n",
        "            for m in list(self.features.children())[-k:]:\n",
        "                for p in m.parameters():\n",
        "                    p.requires_grad = True\n",
        "        elif self.backbone_name == 'resnet18':\n",
        "            # Unfreeze only layer4 (last residual stage)\n",
        "            for m in self.features[-1].modules():  # self.features[-1] == layer4\n",
        "                for p in m.parameters():\n",
        "                    p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)        # [B,C,H,W]\n",
        "        x = self.avgpool(x)         # [B,C,1,1]\n",
        "        x = torch.flatten(x, 1)     # [B,C]\n",
        "        return self.classifier(x)\n",
        "\n",
        "    # Optional: parameter groups for different LRs\n",
        "    def param_groups(self):\n",
        "        backbone_params = [p for p in self.features.parameters() if p.requires_grad]\n",
        "        head_params     = list(self.classifier.parameters())\n",
        "        return {'backbone': backbone_params, 'head': head_params}\n",
        "\n",
        "    def get_num_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = 5\n",
        "\n",
        "model = CNNTransferLearning(\n",
        "    num_classes=num_classes,\n",
        "    backbone='resnet18',          # 'vgg16' | 'vgg19' | 'alexnet' | 'resnet18'\n",
        "    pretrained=True,\n",
        "    freeze_mode='partial',      # 'all' (smallest data) | 'partial' (small data) | 'none' (big data)\n",
        "    classifier_hidden=512\n",
        ").to(device)\n",
        "\n",
        "print(f\"Trainable params: {model.get_num_params():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ikJv2WmYeVV",
        "outputId": "feacefff-9bdd-4bc3-92d0-7c7afaf575c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Freeze] backbone PARTIAL: last block unfrozen.\n",
            "Model: backbone=resnet18 pretrained=True freeze_mode=partial feature_dim=512 classes=5\n",
            "Trainable params: 8,658,949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Training Setup\n",
        "\n",
        "For transfer learning, we need to use different learning rates:\n",
        "- Smaller learning rate for pre-trained layers (if unfrozen)\n",
        "- Regular learning rate for new classifier layers"
      ],
      "metadata": {
        "id": "UmlcGIAuZSSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompute (or drop) class weights\n",
        "use_class_weights = False\n",
        "if use_class_weights:\n",
        "    class_counts = train_df['label'].value_counts()\n",
        "    weights = []\n",
        "    for lbl in ['angry','happy','neutral','sad','surprise']:\n",
        "        c = class_counts.get(lbl, 1)\n",
        "        weights.append(len(train_df)/ (len(class_counts) * c))\n",
        "    weights_tensor = torch.tensor(weights, dtype=torch.float, device=device)\n",
        "else:\n",
        "    weights_tensor = None\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=0.05)\n",
        "\n",
        "# Differential LRs\n",
        "backbone_lr = 3e-5   # warmup\n",
        "head_lr     = 1e-3\n",
        "weight_decay_backbone = 5e-4\n",
        "weight_decay_head     = 1e-3\n",
        "\n",
        "pg = model.param_groups()\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': pg['backbone'], 'lr': backbone_lr, 'weight_decay': weight_decay_backbone},\n",
        "    {'params': pg['head'],     'lr': head_lr,     'weight_decay': weight_decay_head},\n",
        "])\n",
        "\n",
        "# Cosine schedule across full training horizon\n",
        "EPOCHS = 80\n",
        "warmup_epochs = 3\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)"
      ],
      "metadata": {
        "id": "y0MDXtRfYj78"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_warmup(optimizer, epoch, warmup_epochs):\n",
        "    if epoch < warmup_epochs:\n",
        "        warmup_factor = (epoch + 1) / warmup_epochs\n",
        "        # scale group-wise base LR\n",
        "        optimizer.param_groups[0]['lr'] = backbone_lr * warmup_factor\n",
        "        optimizer.param_groups[1]['lr'] = head_lr * warmup_factor\n",
        "\n",
        "# In your training epoch logic, call:\n",
        "# adjust_warmup(optimizer, epoch, warmup_epochs)\n",
        "# then after epoch: scheduler.step()"
      ],
      "metadata": {
        "id": "106cugJxRXFI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Training Functions\n",
        "\n",
        "Let's create training and validation functions with proper progress tracking."
      ],
      "metadata": {
        "id": "ptGNTrHXZdIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    train_loader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    scaler: Optional[torch.cuda.amp.GradScaler] = None,\n",
        "    progress_every: int = 100,\n",
        "    max_grad_norm: Optional[float] = None,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Train the model for one epoch with mixed precision, optional grad clipping, and progress logging.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    use_amp = (scaler is not None)\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if use_amp:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "            scaler.scale(loss).backward()\n",
        "            if max_grad_norm is not None:\n",
        "                # Unscale first, then clip\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            if max_grad_norm is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += float(loss.detach())\n",
        "        pred = output.argmax(dim=1)\n",
        "        total += target.size(0)\n",
        "        correct += (pred == target).sum().item()\n",
        "\n",
        "        if progress_every and (batch_idx % progress_every == 0) and (batch_idx > 0):\n",
        "            print(f'    [Epoch {epoch}] Batch {batch_idx}/{len(train_loader)} '\n",
        "                  f'Loss={running_loss/(batch_idx+1):.4f} '\n",
        "                  f'Acc={100.0*correct/total:.2f}%')\n",
        "\n",
        "    epoch_loss = running_loss / max(1, len(train_loader))\n",
        "    epoch_acc  = 100.0 * correct / max(1, total)\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate_epoch(\n",
        "    model: nn.Module,\n",
        "    val_loader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        "    use_amp: bool = True\n",
        ") -> Tuple[float, float, List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Validate the model with no grad. Uses autocast by default for speed.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions: List[int] = []\n",
        "    all_targets: List[int] = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        amp_ctx = torch.cuda.amp.autocast() if (use_amp and device.type == 'cuda') else torch.cuda.amp.autocast(enabled=False)\n",
        "        with amp_ctx:\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                running_loss += float(loss)\n",
        "                pred = output.argmax(dim=1)\n",
        "                total += target.size(0)\n",
        "                correct += (pred == target).sum().item()\n",
        "\n",
        "                all_predictions.extend(pred.detach().cpu().tolist())\n",
        "                all_targets.extend(target.detach().cpu().tolist())\n",
        "\n",
        "    epoch_loss = running_loss / max(1, len(val_loader))\n",
        "    epoch_acc  = 100.0 * correct / max(1, total)\n",
        "    return epoch_loss, epoch_acc, all_predictions, all_targets"
      ],
      "metadata": {
        "id": "yvOtKsi8Zdgs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Training Loop with Early Stopping"
      ],
      "metadata": {
        "id": "MzYmXn3jAssf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                device, num_epochs=30, patience=5, save_path='best_cnn_transfer_model.pth'):\n",
        "    \"\"\"\n",
        "    Train the model with early stopping.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"STARTING TRAINING - CNN TRANSFER LEARNING\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Epochs: {num_epochs}, Patience: {patience}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print()\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Training phase\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
        "\n",
        "        # Validation phase\n",
        "        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e} (backbone), {optimizer.param_groups[1]['lr']:.2e} (classifier)\")\n",
        "\n",
        "        # Check for improvement\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'train_accuracies': train_accuracies,\n",
        "                'val_accuracies': val_accuracies\n",
        "            }, save_path)\n",
        "            print(f\"✓ New best model saved! Val Acc: {best_val_acc:.2f}%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_accuracies': val_accuracies,\n",
        "        'best_val_acc': best_val_acc\n",
        "    }\n",
        "\n",
        "print(\"Training function defined. Ready to start training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zspDrivLZgko",
        "outputId": "eb866167-2387-4d4a-95b3-4c4b7f89b913"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training function defined. Ready to start training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Start Training\n",
        "\n",
        "Note: If running with dummy data, this will not produce meaningful results. Replace with actual data for real training."
      ],
      "metadata": {
        "id": "lB-Ihc7FZjbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if we have actual data or dummy data\n",
        "if DATA_DIR.exists() and any(DATA_DIR.iterdir()):\n",
        "    print(\"Starting training with actual data...\")\n",
        "    EPOCHS = 50\n",
        "else:\n",
        "    print(\"Using dummy data - training for demonstration only...\")\n",
        "    EPOCHS = 3  # Shorter training for demo\n",
        "\n",
        "# Start training\n",
        "training_history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    num_epochs=EPOCHS,\n",
        "    patience=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TyKCAi57Zhu-",
        "outputId": "a94b2a05-03b3-4423-b112-86f53abb23df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with actual data...\n",
            "\n",
            "============================================================\n",
            "STARTING TRAINING - CNN TRANSFER LEARNING\n",
            "============================================================\n",
            "Epochs: 5, Patience: 3\n",
            "Device: cuda\n",
            "\n",
            "Epoch 1/5\n",
            "----------------------------------------\n",
            "    [Epoch 0] Batch 100/315 Loss=1.7077 Acc=30.93%\n",
            "    [Epoch 0] Batch 200/315 Loss=1.5844 Acc=34.03%\n",
            "    [Epoch 0] Batch 300/315 Loss=1.5137 Acc=37.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1831838612.py:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  amp_ctx = torch.cuda.amp.autocast() if (use_amp and device.type == 'cuda') else torch.cuda.amp.autocast(enabled=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.5035, Train Acc: 37.58%\n",
            "Val Loss: 1.2614, Val Acc: 50.94%\n",
            "Current LR: 3.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "✓ New best model saved! Val Acc: 50.94%\n",
            "\n",
            "Epoch 2/5\n",
            "----------------------------------------\n",
            "    [Epoch 1] Batch 100/315 Loss=1.3051 Acc=47.29%\n",
            "    [Epoch 1] Batch 200/315 Loss=1.2788 Acc=48.96%\n",
            "    [Epoch 1] Batch 300/315 Loss=1.2555 Acc=50.30%\n",
            "Train Loss: 1.2514, Train Acc: 50.50%\n",
            "Val Loss: 1.1363, Val Acc: 58.31%\n",
            "Current LR: 3.00e-05 (backbone), 9.98e-04 (classifier)\n",
            "✓ New best model saved! Val Acc: 58.31%\n",
            "\n",
            "Epoch 3/5\n",
            "----------------------------------------\n",
            "    [Epoch 2] Batch 100/315 Loss=1.1758 Acc=55.26%\n",
            "    [Epoch 2] Batch 200/315 Loss=1.1587 Acc=55.73%\n",
            "    [Epoch 2] Batch 300/315 Loss=1.1508 Acc=56.27%\n",
            "Train Loss: 1.1475, Train Acc: 56.53%\n",
            "Val Loss: 1.0702, Val Acc: 60.42%\n",
            "Current LR: 2.99e-05 (backbone), 9.97e-04 (classifier)\n",
            "✓ New best model saved! Val Acc: 60.42%\n",
            "\n",
            "Epoch 4/5\n",
            "----------------------------------------\n",
            "    [Epoch 3] Batch 100/315 Loss=1.0958 Acc=59.00%\n",
            "    [Epoch 3] Batch 200/315 Loss=1.0971 Acc=58.96%\n",
            "    [Epoch 3] Batch 300/315 Loss=1.0908 Acc=59.25%\n",
            "Train Loss: 1.0888, Train Acc: 59.38%\n",
            "Val Loss: 1.0374, Val Acc: 61.81%\n",
            "Current LR: 2.98e-05 (backbone), 9.94e-04 (classifier)\n",
            "✓ New best model saved! Val Acc: 61.81%\n",
            "\n",
            "Epoch 5/5\n",
            "----------------------------------------\n",
            "    [Epoch 4] Batch 100/315 Loss=1.0733 Acc=60.69%\n",
            "    [Epoch 4] Batch 200/315 Loss=1.0665 Acc=61.12%\n",
            "    [Epoch 4] Batch 300/315 Loss=1.0609 Acc=61.38%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2717324433.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m training_history = train_model(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2546706858.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, patience, save_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Validation phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Update scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1831838612.py\u001b[0m in \u001b[0;36mvalidate_epoch\u001b[0;34m(model, val_loader, criterion, device, use_amp)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mamp_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_amp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Plot Training History"
      ],
      "metadata": {
        "id": "My1jawZUZpzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    ax1.plot(history['train_losses'], label='Training Loss', marker='o')\n",
        "    ax1.plot(history['val_losses'], label='Validation Loss', marker='s')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracies\n",
        "    ax2.plot(history['train_accuracies'], label='Training Accuracy', marker='o')\n",
        "    ax2.plot(history['val_accuracies'], label='Validation Accuracy', marker='s')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Training Summary:\")\n",
        "    print(f\"- Final Training Loss: {history['train_losses'][-1]:.4f}\")\n",
        "    print(f\"- Final Validation Loss: {history['val_losses'][-1]:.4f}\")\n",
        "    print(f\"- Final Training Accuracy: {history['train_accuracies'][-1]:.2f}%\")\n",
        "    print(f\"- Final Validation Accuracy: {history['val_accuracies'][-1]:.2f}%\")\n",
        "    print(f\"- Best Validation Accuracy: {history['best_val_acc']:.2f}%\")\n",
        "\n",
        "# Plot the training history\n",
        "plot_training_history(training_history)"
      ],
      "metadata": {
        "id": "_6dc7vFUZlCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Load Best Model and Evaluate on Test Set"
      ],
      "metadata": {
        "id": "LTf9u_NFZvhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "if Path('best_cnn_transfer_model.pth').exists():\n",
        "    checkpoint = torch.load('best_cnn_transfer_model.pth', map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model from epoch {checkpoint['epoch']} with val acc: {checkpoint['best_val_acc']:.2f}%\")\n",
        "else:\n",
        "    print(\"No saved model found, using current model state\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_loss, test_acc, test_predictions, test_targets = validate_epoch(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Results:\")\n",
        "print(f\"- Test Loss: {test_loss:.4f}\")\n",
        "print(f\"- Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "# Detailed classification report\n",
        "if len(set(test_targets)) > 1:  # Only if we have multiple classes\n",
        "    print(\"\\nClassification Report:\")\n",
        "    emotion_names = list(label_map.keys())\n",
        "    report = classification_report(test_targets, test_predictions,\n",
        "                                 target_names=emotion_names,\n",
        "                                 zero_division=0)\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "NeWyrZiUZn4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Visualize Results - Confusion Matrix"
      ],
      "metadata": {
        "id": "uATYqMe_Zz3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, class_names, title='Confusion Matrix'):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Emotion')\n",
        "    plt.ylabel('True Emotion')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print accuracy per class\n",
        "    print(\"\\nPer-class Accuracy:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_correct = cm[i, i]\n",
        "        class_total = cm[i, :].sum()\n",
        "        if class_total > 0:\n",
        "            acc = 100 * class_correct / class_total\n",
        "            print(f\"- {class_name}: {acc:.2f}% ({class_correct}/{class_total})\")\n",
        "\n",
        "# Plot confusion matrix if we have predictions\n",
        "if len(set(test_targets)) > 1:\n",
        "    emotion_names = list(label_map.keys())\n",
        "    plot_confusion_matrix(test_targets, test_predictions, emotion_names,\n",
        "                         'CNN Transfer Learning - Test Set Confusion Matrix')\n",
        "else:\n",
        "    print(\"Skipping confusion matrix (insufficient data/classes)\")"
      ],
      "metadata": {
        "id": "jKA7lp_eZuRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Model Comparison and Analysis\n",
        "\n",
        "Let's compare our transfer learning CNN with the original baseline CNN."
      ],
      "metadata": {
        "id": "yy7McfQzZ90E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL ANALYSIS AND COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Model parameters\n",
        "print(f\"\\n1. MODEL ARCHITECTURE:\")\n",
        "print(f\"   - Transfer Learning CNN (VGG16 backbone)\")\n",
        "print(f\"   - Total parameters: {model.get_num_params():,}\")\n",
        "print(f\"   - Input size: {INPUT_SIZE}x{INPUT_SIZE}x3 (RGB)\")\n",
        "print(f\"   - Output classes: {num_classes}\")\n",
        "\n",
        "# Performance summary\n",
        "print(f\"\\n2. PERFORMANCE SUMMARY:\")\n",
        "if 'training_history' in locals():\n",
        "    print(f\"   - Best Validation Accuracy: {training_history['best_val_acc']:.2f}%\")\n",
        "    print(f\"   - Final Test Accuracy: {test_acc:.2f}%\")\n",
        "    print(f\"   - Training Epochs: {len(training_history['train_losses'])}\")\n",
        "\n",
        "# Transfer Learning Benefits\n",
        "print(f\"\\n3. TRANSFER LEARNING BENEFITS:\")\n",
        "print(f\"   ✓ Pre-trained features: Learned from ImageNet (1.2M images)\")\n",
        "print(f\"   ✓ Faster convergence: Starts with meaningful weights\")\n",
        "print(f\"   ✓ Better generalization: Robust low-level feature extraction\")\n",
        "print(f\"   ✓ Less overfitting: Pre-trained features are well-regularized\")\n",
        "\n",
        "print(f\"\\n4. KEY DIFFERENCES FROM BASELINE CNN:\")\n",
        "print(f\"   - Uses pre-trained VGG16 backbone vs. random initialization\")\n",
        "print(f\"   - RGB input (224x224) vs. Grayscale (48x48)\")\n",
        "print(f\"   - ImageNet normalization vs. simple normalization\")\n",
        "print(f\"   - Transfer learning strategy vs. training from scratch\")\n",
        "print(f\"   - Different learning rates for backbone vs. classifier\")\n",
        "\n",
        "print(f\"\\n5. TRAINING STRATEGY USED:\")\n",
        "print(f\"   - Fine-tuning: All layers trainable\")\n",
        "# Retrieve learning rates from optimizer\n",
        "backbone_lr = optimizer.param_groups[0]['lr']\n",
        "classifier_lr = optimizer.param_groups[1]['lr']\n",
        "print(f\"   - Backbone LR: {backbone_lr:.2e} (very small)\")\n",
        "print(f\"   - Classifier LR: {classifier_lr:.2e} (regular)\")\n",
        "print(f\"   - Data augmentation: Rotation, flip, color jitter, affine\")\n",
        "print(f\"   - Early stopping with patience={5}\")"
      ],
      "metadata": {
        "id": "_HytFIwgZywX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 15: Save Final Model for Production\n",
        "\n",
        "Let's save our model in a format that can be easily loaded for inference."
      ],
      "metadata": {
        "id": "685JJbxRaDaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save complete model information\n",
        "final_model_info = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_config': {\n",
        "        'num_classes': num_classes,\n",
        "        'backbone': 'vgg16',\n",
        "        'input_size': INPUT_SIZE,\n",
        "        'pretrained': True,\n",
        "        'freeze_backbone': False\n",
        "    },\n",
        "    'label_map': label_map,\n",
        "    'transforms': {\n",
        "        'mean': IMAGENET_MEAN,\n",
        "        'std': IMAGENET_STD,\n",
        "        'input_size': INPUT_SIZE\n",
        "    },\n",
        "    'training_info': {\n",
        "        'final_test_acc': test_acc,\n",
        "        'backbone_lr': backbone_lr,\n",
        "        'classifier_lr': classifier_lr\n",
        "    }\n",
        "}\n",
        "\n",
        "torch.save(final_model_info, 'cnn_transfer_learning_final.pth')\n",
        "print(\"✓ Final model saved as 'cnn_transfer_learning_final.pth'\")\n",
        "\n",
        "# Create a simple inference function\n",
        "def create_inference_function():\n",
        "    \"\"\"\n",
        "    Create a simple inference function that can be used in production.\n",
        "    \"\"\"\n",
        "    inference_code = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "def load_emotion_model(model_path):\n",
        "    \"\"\"Load the trained emotion recognition model.\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "    # Recreate model architecture\n",
        "    model = CNNTransferLearning(\n",
        "        num_classes=checkpoint['model_config']['num_classes'],\n",
        "        backbone=checkpoint['model_config']['backbone'],\n",
        "        pretrained=False  # We're loading trained weights\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Create transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((checkpoint['transforms']['input_size'],\n",
        "                          checkpoint['transforms']['input_size'])),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=checkpoint['transforms']['mean'],\n",
        "                           std=checkpoint['transforms']['std'])\n",
        "    ])\n",
        "\n",
        "    return model, transform, checkpoint['label_map']\n",
        "\n",
        "def predict_emotion(model, transform, label_map, image_path):\n",
        "    \"\"\"Predict emotion from image.\"\"\"\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    # Convert to emotion name\n",
        "    emotion_names = {v: k for k, v in label_map.items()}\n",
        "    predicted_emotion = emotion_names[predicted_class]\n",
        "    confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "    return predicted_emotion, confidence, probabilities[0].numpy()\n",
        "'''\n",
        "\n",
        "    with open('emotion_inference.py', 'w') as f:\n",
        "        f.write(inference_code)\n",
        "\n",
        "    print(\"✓ Inference code saved as 'emotion_inference.py'\")\n",
        "\n",
        "create_inference_function()\n",
        "\n",
        "print(\"\\n✓ Model deployment ready!\")\n",
        "print(\"Files created:\")\n",
        "print(\"- cnn_transfer_learning_final.pth (complete model)\")\n",
        "print(\"- emotion_inference.py (inference functions)\")"
      ],
      "metadata": {
        "id": "7hU4VLbkZ_np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary: CNN Transfer Learning Implementation\n",
        "\n",
        "### What We've Accomplished:\n",
        "\n",
        "1. **Transfer Learning Setup**: Implemented CNN transfer learning using VGG16 backbone pre-trained on ImageNet\n",
        "2. **Optional Resizing Pipeline**: Added automated 48×48 → 224×224 upscaling with per-class folder structure and regenerated split CSVs (`train_224.csv`, `val_224.csv`, `test_224.csv`) plus stats file.\n",
        "3. **Architecture**:\n",
        "   - Pre-trained VGG16 feature extractor\n",
        "   - Custom classifier head for emotion recognition\n",
        "   - Support for RGB images (224×224)\n",
        "4. **Training Strategy**:\n",
        "   - Fine-tuning approach with different learning rates\n",
        "   - ImageNet normalization for compatibility\n",
        "   - Data augmentation for better generalization\n",
        "   - Early stopping to prevent overfitting\n",
        "5. **Key Benefits Over Baseline CNN**:\n",
        "   - ✅ **Faster Convergence**: Pre-trained weights provide good starting point\n",
        "   - ✅ **Better Feature Learning**: Robust low-level features from ImageNet\n",
        "   - ✅ **Improved Generalization**: Less prone to overfitting\n",
        "   - ✅ **State-of-the-art Architecture**: Proven CNN design\n",
        "6. **Resized Dataset Artifacts** (if generated):\n",
        "   - Images: `data/processed/EmoSet_resized_224/images_224/<class>/*.jpg`\n",
        "   - Splits: `data/processed/EmoSet_resized_224/splits/train_224.csv` (and val/test)\n",
        "   - Stats:  `data/processed/EmoSet_resized_224/stats_resized_224.json`\n",
        "   - Toggle usage via `USE_RESIZED_SPLITS = True` in the notebook.\n",
        "\n",
        "### Transfer Learning Strategies Implemented:\n",
        "1. **Feature Extraction** (freezing backbone)\n",
        "2. **Fine-tuning** (training all layers with different LRs)\n",
        "3. **Gradual unfreezing** (implemented as methods)\n",
        "\n",
        "### Production-Ready Features:\n",
        "- Complete model serialization\n",
        "- Inference functions\n",
        "- Proper preprocessing pipeline\n",
        "- Model configuration storage\n",
        "- Optional resized dataset workflow"
      ],
      "metadata": {
        "id": "yA-j6AbQaJSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 16: Model Interpretability (Grad-CAM heatmaps)\n",
        "\n",
        "We’ll generate Grad-CAM heatmaps to understand which image regions drive the model’s predictions. This helps validate that the model focuses on relevant facial areas.\n",
        "\n",
        "What we’ll do:\n",
        "- Hook the last convolutional layer in the backbone\n",
        "- Compute class-specific gradients and weights\n",
        "- Produce heatmaps and overlay them on the original images"
      ],
      "metadata": {
        "id": "YRT0rY5UabRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 16 (run): Grad-CAM utilities and visualization helpers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.target_layer = target_layer\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "        self.hook_handles = []\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def fwd_hook(module, inp, out):\n",
        "            self.activations = out.detach()\n",
        "        def bwd_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0].detach()\n",
        "        self.hook_handles.append(self.target_layer.register_forward_hook(fwd_hook))\n",
        "        self.hook_handles.append(self.target_layer.register_backward_hook(bwd_hook))\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for h in self.hook_handles:\n",
        "            h.remove()\n",
        "\n",
        "    def __call__(self, input_tensor, class_idx=None):\n",
        "        input_tensor = input_tensor.requires_grad_(True)\n",
        "        logits = self.model(input_tensor)\n",
        "        if class_idx is None:\n",
        "            class_idx = logits.argmax(dim=1).item()\n",
        "        score = logits[:, class_idx]\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "\n",
        "        # activations: [B, C, H, W]; gradients: [B, C, H, W]\n",
        "        weights = self.gradients.mean(dim=(2,3), keepdim=True)  # [B, C, 1, 1]\n",
        "        cam = (weights * self.activations).sum(dim=1, keepdim=True)  # [B,1,H,W]\n",
        "        cam = F.relu(cam)\n",
        "        # Normalize to [0,1]\n",
        "        cam_min, cam_max = cam.min(), cam.max()\n",
        "        cam = (cam - cam_min) / (cam_max - cam_min + 1e-6)\n",
        "        return cam.squeeze(0).squeeze(0).cpu().numpy(), class_idx\n",
        "\n",
        "# Helper to overlay heatmap\n",
        "def overlay_cam_on_image(img_pil, cam, alpha=0.4):\n",
        "    img = np.array(img_pil.convert('RGB'))\n",
        "    h, w = img.shape[:2]\n",
        "    cam_resized = cv2.resize(cam, (w, h))\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255*cam_resized), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    overlay = np.uint8(alpha*heatmap + (1-alpha)*img)\n",
        "    return img, heatmap, overlay\n",
        "\n",
        "# Pick the last conv block for the chosen backbone (VGG16/AlexNet/ResNet18 supported)\n",
        "last_conv = None\n",
        "if hasattr(model, 'features'):\n",
        "    for m in reversed(model.features):\n",
        "        if isinstance(m, torch.nn.Conv2d):\n",
        "            last_conv = m\n",
        "            break\n",
        "else:\n",
        "    # ResNet18 path: find last Conv2d under features (layer4)\n",
        "    for m in reversed(list(model.features.modules())):\n",
        "        if isinstance(m, torch.nn.Conv2d):\n",
        "            last_conv = m\n",
        "            break\n",
        "assert last_conv is not None, \"Could not find a Conv2d layer for Grad-CAM\"\n",
        "\n",
        "cam_explainer = GradCAM(model, last_conv)\n",
        "\n",
        "# Visualize Grad-CAM on a few validation samples\n",
        "def show_gradcam_for_samples(df, root, transform, k=4):\n",
        "    samples = df.sample(n=min(k, len(df)), random_state=42)\n",
        "    plt.figure(figsize=(12, 3*k))\n",
        "    for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
        "        img_path = Path(root) / row[train_dataset.path_col]\n",
        "        img_pil = Image.open(img_path).convert('L')\n",
        "        inp = transform(img_pil)\n",
        "        if inp.shape[0]==1:\n",
        "            inp = inp.repeat(3,1,1)  # expand to 3 channels for CNN backbones\n",
        "        cam, pred_idx = cam_explainer(inp.unsqueeze(0).to(device))\n",
        "        img, heatmap, overlay = overlay_cam_on_image(img_pil, cam)\n",
        "        pred_name = list(label_map.keys())[pred_idx]\n",
        "\n",
        "        plt.subplot(k, 3, 3*(i-1)+1); plt.imshow(img); plt.axis('off'); plt.title('Input')\n",
        "        plt.subplot(k, 3, 3*(i-1)+2); plt.imshow(heatmap); plt.axis('off'); plt.title('Grad-CAM')\n",
        "        plt.subplot(k, 3, 3*(i-1)+3); plt.imshow(overlay); plt.axis('off'); plt.title(f'Overlay (pred={pred_name})')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "print(\"Grad-CAM ready. Run the next cell to visualize on validation samples.\")"
      ],
      "metadata": {
        "id": "WQ5hj5UuaFIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Visualize Grad-CAM on a few validation images\n",
        "try:\n",
        "    show_gradcam_for_samples(val_df, DATA_DIR, val_transform, k=4)\n",
        "except Exception as e:\n",
        "    print('Grad-CAM visualization error:', e)"
      ],
      "metadata": {
        "id": "7LIraKOMaHgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 17: Evaluation Plan — Overview\n",
        "\n",
        "Primary metrics:\n",
        "- Macro-F1 (treats all classes equally)\n",
        "- Balanced Accuracy (mean recall across classes)\n",
        "\n",
        "Secondary metrics:\n",
        "- Per-class precision/recall/F1\n",
        "- ROC-AUC (macro and micro, OvR)\n",
        "\n",
        "Tools:\n",
        "- Confusion matrix and per-class accuracy\n",
        "- Error analysis (high-confidence mistakes)\n",
        "- Cross-validation (K-fold skeleton)"
      ],
      "metadata": {
        "id": "idFkaAhiafWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 18: Compute Metrics — Macro-F1, Balanced Accuracy, ROC-AUC, and Report\n",
        "from sklearn.metrics import f1_score, balanced_accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_probs(model, loader, device):\n",
        "    model.eval()\n",
        "    all_probs, all_preds, all_targets = [], [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model(xb)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        preds = probs.argmax(dim=1)\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(yb.cpu().numpy())\n",
        "    y_true = np.concatenate(all_targets)\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    y_proba = np.concatenate(all_probs)\n",
        "    return y_true, y_pred, y_proba\n",
        "\n",
        "# Evaluate on test set\n",
        "y_true, y_pred, y_proba = evaluate_with_probs(model, test_loader, device)\n",
        "\n",
        "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "print(f\"Primary metrics:\\n- Macro-F1: {macro_f1:.4f}\\n- Balanced Accuracy: {bal_acc:.4f}\")\n",
        "\n",
        "print(\"\\nPer-class metrics (precision/recall/F1):\")\n",
        "print(classification_report(y_true, y_pred, target_names=list(label_map.keys()), zero_division=0))\n",
        "\n",
        "# ROC-AUC (multi-class OvR)\n",
        "try:\n",
        "    y_true_1h = label_binarize(y_true, classes=list(range(len(label_map))))\n",
        "    roc_auc_macro = roc_auc_score(y_true_1h, y_proba, average='macro', multi_class='ovr')\n",
        "    roc_auc_micro = roc_auc_score(y_true_1h, y_proba, average='micro', multi_class='ovr')\n",
        "    print(f\"ROC-AUC (macro): {roc_auc_macro:.4f}\\nROC-AUC (micro): {roc_auc_micro:.4f}\")\n",
        "except Exception as e:\n",
        "    print(\"ROC-AUC could not be computed:\", e)"
      ],
      "metadata": {
        "id": "LxanC4Jtaglb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 19: Error Analysis — High-confidence Misclassifications\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure y_true/y_pred/y_proba are available; else recompute\n",
        "y_true, y_pred, y_proba = (y_true if 'y_true' in globals() else None,\n",
        "                           y_pred if 'y_pred' in globals() else None,\n",
        "                           y_proba if 'y_proba' in globals() else None)\n",
        "if y_true is None or y_pred is None or y_proba is None:\n",
        "    y_true, y_pred, y_proba = evaluate_with_probs(model, test_loader, device)\n",
        "\n",
        "# Paths in test order\n",
        "paths_in_order = test_dataset.df[test_dataset.path_col].tolist()\n",
        "abs_paths = [str(Path(DATA_DIR) / p) for p in paths_in_order]\n",
        "\n",
        "mis_idx = np.where(y_pred != y_true)[0]\n",
        "if mis_idx.size == 0:\n",
        "    print(\"No misclassifications found on the test set.\")\n",
        "else:\n",
        "    top2_idx = np.argsort(-y_proba, axis=1)[:, :2]\n",
        "    top1_prob = y_proba[np.arange(len(y_proba)), top2_idx[:, 0]]\n",
        "    top2_prob = y_proba[np.arange(len(y_proba)), top2_idx[:, 1]]\n",
        "\n",
        "    inv_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    rows = []\n",
        "    for i in mis_idx:\n",
        "        rows.append({\n",
        "            'index': int(i),\n",
        "            'image_path': abs_paths[i] if i < len(abs_paths) else None,\n",
        "            'true_idx': int(y_true[i]),\n",
        "            'true_label': inv_label_map.get(int(y_true[i]), str(y_true[i])),\n",
        "            'pred_idx': int(top2_idx[i, 0]),\n",
        "            'pred_label': inv_label_map.get(int(top2_idx[i, 0]), str(top2_idx[i, 0])),\n",
        "            'pred_prob': float(top1_prob[i]),\n",
        "            'second_idx': int(top2_idx[i, 1]),\n",
        "            'second_label': inv_label_map.get(int(top2_idx[i, 1]), str(top2_idx[i, 1])),\n",
        "            'second_prob': float(top2_prob[i])\n",
        "        })\n",
        "\n",
        "    mis_df = pd.DataFrame(rows).sort_values(by='pred_prob', ascending=False).reset_index(drop=True)\n",
        "    display(mis_df.head(20))\n",
        "\n",
        "    out_dir = Path(PROJECT_ROOT) / 'results' / 'error_analysis'\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_csv = out_dir / 'misclassifications.csv'\n",
        "    mis_df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved misclassifications to {out_csv}\")"
      ],
      "metadata": {
        "id": "GURPGVCpaihM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 20: Explainable AI (XAI) — Why and How\n",
        "\n",
        "Why XAI?\n",
        "- Trust & Adoption: Understand predictions.\n",
        "- Debugging & Improvement: Find weaknesses or biases.\n",
        "- Compliance & Ethics: Ensure fairness and explainability.\n",
        "\n",
        "Techniques in this notebook:\n",
        "- Grad-CAM (already implemented) and Grad-CAM++ (optional)\n",
        "- LIME (local explanations)\n",
        "- SHAP (Shapley values; optional and compute-heavy)\n",
        "- Attention visualization (for Transformers; not applicable here)"
      ],
      "metadata": {
        "id": "HvW-0R-falhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "4fJaqbxVakAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 21: LIME — Local Explanations for Individual Predictions\n",
        "try:\n",
        "    from lime import lime_image\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    def lime_predict(images: np.ndarray):\n",
        "        tensors = []\n",
        "        for img in images:\n",
        "            pil = Image.fromarray((img * 255).astype(np.uint8)) if img.max() <= 1.0 else Image.fromarray(img.astype(np.uint8))\n",
        "            pil = pil.convert('L')\n",
        "            t = val_transform(pil)\n",
        "            if t.shape[0] == 1:\n",
        "                t = t.repeat(3, 1, 1)\n",
        "            tensors.append(t)\n",
        "        batch = torch.stack(tensors, dim=0)\n",
        "        with torch.no_grad():\n",
        "            logits = model(batch.to(device))\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "        return probs\n",
        "\n",
        "    sample_idx = 0\n",
        "    img_path = Path(DATA_DIR) / test_dataset.df.iloc[sample_idx][test_dataset.path_col]\n",
        "    img_rgb = Image.open(img_path).convert('RGB')\n",
        "    img_np = np.asarray(img_rgb) / 255.0\n",
        "\n",
        "    explainer = lime_image.LimeImageExplainer()\n",
        "    explanation = explainer.explain_instance(\n",
        "        img_np,\n",
        "        classifier_fn=lime_predict,\n",
        "        top_labels=1,\n",
        "        hide_color=0,\n",
        "        num_samples=500\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pil_gray = Image.open(img_path).convert('L')\n",
        "        t = val_transform(pil_gray)\n",
        "        if t.shape[0] == 1:\n",
        "            t = t.repeat(3, 1, 1)\n",
        "        pred_idx = int(torch.softmax(model(t.unsqueeze(0).to(device)), dim=1).argmax().item())\n",
        "\n",
        "    lime_img, mask = explanation.get_image_and_mask(\n",
        "        label=pred_idx,\n",
        "        positive_only=True,\n",
        "        num_features=5,\n",
        "        hide_rest=False\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1); plt.imshow(img_rgb); plt.axis('off'); plt.title('Original')\n",
        "    plt.subplot(1, 3, 2); plt.imshow(mask, cmap='gray'); plt.axis('off'); plt.title('LIME Mask')\n",
        "    plt.subplot(1, 3, 3); plt.imshow(lime_img); plt.axis('off'); plt.title('LIME Overlay')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    print(\"LIME explanation generated.\")\n",
        "except Exception as e:\n",
        "    print(\"LIME explanation skipped. Install with: pip install lime. Error:\", e)"
      ],
      "metadata": {
        "id": "eVIvAaooanyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 20 (run): Install optional XAI packages (safe with fallbacks)\n",
        "import importlib, sys, subprocess\n",
        "\n",
        "pkgs = [\n",
        "    { 'pip': 'pytorch-grad-cam', 'import': 'pytorch_grad_cam', 'alt': 'git+https://github.com/jacobgil/pytorch-grad-cam@master' },\n",
        "    { 'pip': 'lime',              'import': 'lime',             'alt': None },\n",
        "    { 'pip': 'shap',              'import': 'shap',             'alt': None },\n",
        "]\n",
        "\n",
        "def ensure_package(pip_name: str, import_name: str, alt: str | None = None):\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        print(f\"✓ {pip_name} already available as '{import_name}'\")\n",
        "        return\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pip_name}...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools', 'wheel'])\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_name])\n",
        "            importlib.import_module(import_name)\n",
        "            print(f\"✓ Installed {pip_name}\")\n",
        "            return\n",
        "        except Exception as e1:\n",
        "            if alt:\n",
        "                print(f\"Primary install failed for {pip_name}. Trying fallback: {alt}\")\n",
        "                try:\n",
        "                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', alt])\n",
        "                    importlib.import_module(import_name)\n",
        "                    print(f\"✓ Installed via fallback: {alt}\")\n",
        "                    return\n",
        "                except Exception as e2:\n",
        "                    print(f\"✗ Failed to install {pip_name} via pip and fallback. Error: {e2}\")\n",
        "            else:\n",
        "                print(f\"✗ Failed to install {pip_name}. Error: {e1}\")\n",
        "\n",
        "for p in pkgs:\n",
        "    ensure_package(p['pip'], p['import'], p['alt'])\n",
        "\n",
        "print(\"Optional XAI packages installation attempt complete. Proceed to Steps 21–23; cells will skip gracefully if a package is still unavailable.\")"
      ],
      "metadata": {
        "id": "oH-ufSjfaqXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 22: Grad-CAM++ (optional via pytorch-grad-cam)\n",
        "try:\n",
        "    from pytorch_grad_cam import GradCAMPlusPlus\n",
        "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "\n",
        "    target_conv = None\n",
        "    if hasattr(model, 'features'):\n",
        "        for m in reversed(model.features):\n",
        "            if isinstance(m, torch.nn.Conv2d):\n",
        "                target_conv = m\n",
        "                break\n",
        "    else:\n",
        "        for m in reversed(list(model.features.modules())):\n",
        "            if isinstance(m, torch.nn.Conv2d):\n",
        "                target_conv = m\n",
        "                break\n",
        "    assert target_conv is not None\n",
        "\n",
        "    campp = GradCAMPlusPlus(model=model, target_layers=[target_conv]) # Removed use_cuda\n",
        "\n",
        "    img_path = Path(DATA_DIR) / val_dataset.df.iloc[0][val_dataset.path_col]\n",
        "    img_pil = Image.open(img_path).convert('RGB')\n",
        "    img_np = np.array(img_pil).astype(np.float32) / 255.0\n",
        "\n",
        "    img_gray = Image.open(img_path).convert('L')\n",
        "    inp = val_transform(img_gray)\n",
        "    if inp.shape[0] == 1:\n",
        "        inp = inp.repeat(3, 1, 1)\n",
        "    input_tensor = inp.unsqueeze(0)\n",
        "\n",
        "    grayscale_cam = campp(input_tensor=input_tensor.to(device))  # [1,H,W]\n",
        "    grayscale_cam = grayscale_cam[0]\n",
        "\n",
        "    visualization = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1); plt.imshow(img_np); plt.axis('off'); plt.title('Input')\n",
        "    plt.subplot(1,2,2); plt.imshow(visualization); plt.axis('off'); plt.title('Grad-CAM++')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    print(\"Grad-CAM++ visualization ready.\")\n",
        "except Exception as e:\n",
        "    print(\"Grad-CAM++ skipped. Install with: pip install pytorch-grad-cam. Error:\", e)"
      ],
      "metadata": {
        "id": "aR3GtSk_ar3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 23: SHAP (optional) — DeepExplainer for CNNs\n",
        "try:\n",
        "    import shap\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Background for SHAP (small)\n",
        "    N_BG = 8\n",
        "    bg_tensors = []\n",
        "    for i in range(min(N_BG, len(val_dataset))):\n",
        "        p = Path(DATA_DIR) / val_dataset.df.iloc[i][val_dataset.path_col]\n",
        "        img_gray = Image.open(p).convert('L')\n",
        "        t = val_transform(img_gray)\n",
        "        if t.shape[0] == 1:\n",
        "            t = t.repeat(3, 1, 1)\n",
        "        bg_tensors.append(t)\n",
        "\n",
        "    if len(bg_tensors) < 1:\n",
        "         raise ValueError(\"Not enough background samples for SHAP.\")\n",
        "\n",
        "    background = torch.stack(bg_tensors, dim=0)\n",
        "\n",
        "    def f_predict(x: np.ndarray):\n",
        "        # Input to f_predict is expected to be 2D (n_samples, n_features)\n",
        "        # Reshape back to (n_samples, C, H, W)\n",
        "        x_reshaped = x.reshape(-1, 3, INPUT_SIZE, INPUT_SIZE) # Assuming 3 channels, 224x224\n",
        "        with torch.no_grad():\n",
        "            xb = torch.from_numpy(x_reshaped).float().to(device)\n",
        "            logits = model(xb)\n",
        "            probs = torch.softmax(logits, dim=1).detach().cpu().numpy()\n",
        "        return probs\n",
        "\n",
        "    idx = 0\n",
        "    p = Path(DATA_DIR) / val_dataset.df.iloc[idx][val_dataset.path_col]\n",
        "    img_gray = Image.open(p).convert('L')\n",
        "    x = val_transform(img_gray)\n",
        "    if x.shape[0] == 1:\n",
        "        x = x.repeat(3, 1, 1)\n",
        "    x = x.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # Flatten the input tensor for KernelExplainer\n",
        "    x_flat = x.cpu().numpy().reshape(1, -1)\n",
        "    background_flat = background.cpu().numpy().reshape(background.shape[0], -1)\n",
        "\n",
        "\n",
        "    try:\n",
        "        explainer = shap.DeepExplainer(model.to(device), background.to(device))\n",
        "        shap_values = explainer.shap_values(x.to(device))\n",
        "    except Exception:\n",
        "        print(\"Falling back to KernelExplainer (slower)...\")\n",
        "        explainer = shap.KernelExplainer(f_predict, background_flat[:min(50, background_flat.shape[0])]) # Use a subset of background if large\n",
        "        shap_values = explainer.shap_values(x_flat, nsamples=100)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_idx = int(torch.softmax(model(x.to(device)), dim=1).argmax().item())\n",
        "\n",
        "    try:\n",
        "        # Reshape SHAP values back to image dimensions for plotting\n",
        "        if isinstance(shap_values, list): # Output of multi-output models\n",
        "             shap_values_reshaped = [np.reshape(v, (INPUT_SIZE, INPUT_SIZE, 3)) for v in shap_values] # Assuming 3 channels\n",
        "        else: # Single output\n",
        "             shap_values_reshaped = np.reshape(shap_values, (INPUT_SIZE, INPUT_SIZE, 3)) # Assuming 3 channels\n",
        "\n",
        "        shap.image_plot(shap_values_reshaped[pred_idx] if isinstance(shap_values_reshaped, list) else shap_values_reshaped,\n",
        "                         np.transpose(x.cpu().numpy()[0], (1, 2, 0))) # Transpose (C, H, W) to (H, W, C)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"SHAP image plot skipped:\", e)\n",
        "\n",
        "    print(\"SHAP explanation generated for one validation sample.\")\n",
        "except Exception as e:\n",
        "    print(\"SHAP skipped. Install with: pip install shap. Error:\", e)"
      ],
      "metadata": {
        "id": "vItwTK7fatmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 24: Cross-validation (skeleton)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "k = 3\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "labels_series = train_df[train_dataset.label_col]\n",
        "\n",
        "print(f\"Preparing {k}-fold CV on training set (size={len(train_df)})...\")\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, labels_series), start=1):\n",
        "    print(f\"Fold {fold}: train={len(train_idx)}, val={len(val_idx)}\")\n",
        "\n",
        "# To fully run CV per fold:\n",
        "# 1) subset train_df/val_df to train_idx/val_idx\n",
        "# 2) create new Datasets/DataLoaders\n",
        "# 3) re-initialize model/optimizer\n",
        "# 4) train and record metrics, then aggregate"
      ],
      "metadata": {
        "id": "BUj6DF2GavTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 25: Optimize for Deployment — Quantization + ONNX\n",
        "\n",
        "We’ll export two artifacts:\n",
        "- Dynamically-quantized PyTorch checkpoint (smaller and faster on CPU).\n",
        "- ONNX model (dynamic batch axis) for ONNX Runtime and cross-platform use.\n",
        "\n",
        "The next cell performs both and optionally verifies ONNX forward with onnxruntime if available."
      ],
      "metadata": {
        "id": "6NAO01RKaw__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 25 (run): Export quantized PyTorch and ONNX models\n",
        "import torch\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "\n",
        "# 25a. Dynamic quantization (CPU inference)\n",
        "cpu_model = deepcopy(model).to('cpu').eval()\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    cpu_model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "quant_ckpt = {\n",
        "    'model_state_dict': quantized_model.state_dict(),\n",
        "    'model_config': {\n",
        "        'num_classes': num_classes,\n",
        "        'backbone': 'vgg16',\n",
        "        'input_size': INPUT_SIZE,\n",
        "        'pretrained': False,\n",
        "        'quantized': True,\n",
        "        'grayscale': True,\n",
        "        'channel_repeat': True\n",
        "    },\n",
        "    'label_map': label_map,\n",
        "}\n",
        "torch.save(quant_ckpt, 'cnn_transfer_learning_quantized.pth')\n",
        "print('✓ Saved dynamic-quantized model: cnn_transfer_learning_quantized.pth')\n",
        "\n",
        "# 25b. ONNX export\n",
        "dummy = torch.randn(1, 3, INPUT_SIZE, INPUT_SIZE, dtype=torch.float32)\n",
        "model_cpu = model.to('cpu').eval()\n",
        "onnx_path = 'cnn_transfer_learning.onnx'\n",
        "torch.onnx.export(\n",
        "    model_cpu, dummy, onnx_path,\n",
        "    input_names=['input'], output_names=['logits'],\n",
        "    dynamic_axes={'input': {0: 'batch'}, 'logits': {0: 'batch'}},\n",
        "    opset_version=13\n",
        ")\n",
        "print(f'✓ Exported ONNX: {onnx_path}')\n",
        "\n",
        "try:\n",
        "    import onnxruntime as ort\n",
        "    sess = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
        "    ort_out = sess.run(['logits'], { 'input': dummy.numpy() })[0]\n",
        "    print('ONNX forward ok. Output shape:', ort_out.shape)\n",
        "except Exception as e:\n",
        "    print('ONNXRuntime check skipped or failed:', e)\n",
        "\n",
        "print('Deployment artifacts ready.')"
      ],
      "metadata": {
        "id": "rZ8F-aI_axYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MF5gdGD8aysr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}