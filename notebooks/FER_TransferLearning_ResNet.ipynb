{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# FER Transfer Learning (PyTorch) \u2014 ResNet / EfficientNet\n", "*Auto-generated on 2025-09-08 17:27*\n", "\n", "This notebook upgrades your baseline CNN to **transfer learning** using pretrained backbones from `torchvision`\n", "(e.g., **ResNet-18/50**, **EfficientNet-B0**). It includes:\n", "- robust **CSV loader** (paths + labels)\n", "- **data augmentations** suitable for face images\n", "- **label smoothing**, **class weights**, optional **Focal Loss**\n", "- **CosineAnnealingLR**, **Warmup**, **Early Stopping**\n", "- **mixed precision** (AMP) and **gradient clipping**\n", "- full **evaluation** (accuracy, classification report, confusion matrix)\n", "- **Checkpointing** and **Resume**\n", "\n", "> \u2699\ufe0f Fill the paths in **Config** below to point to your `train.csv`, `val.csv`, `test.csv`, and `label_map.json`."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# If you're in Colab, uncomment the following (torch/torchvision are usually preinstalled).\n", "# !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n", "# !pip install -q scikit-learn pandas matplotlib tqdm"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import os, json, math, time, random, shutil\n", "from pathlib import Path\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n", "from torchvision import transforms as T\n", "from torchvision import models\n", "\n", "from sklearn.metrics import classification_report, confusion_matrix\n", "from sklearn.preprocessing import LabelEncoder\n", "\n", "from tqdm.auto import tqdm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Config \u2014 set your paths and hyperparameters"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# ==== PATHS (EDIT THESE) ====\n", "CSV_TRAIN = \"/content/drive/MyDrive/ann-visual-emotion/data/processed/EmoSet_splits/train.csv\"\n", "CSV_VAL   = \"/content/drive/MyDrive/ann-visual-emotion/data/processed/EmoSet_splits/val.csv\"\n", "CSV_TEST  = \"/content/drive/MyDrive/ann-visual-emotion/data/processed/EmoSet_splits/test.csv\"  # can be missing\n", "LABEL_MAP = \"/content/drive/MyDrive/ann-visual-emotion/data/processed/EmoSet_splits/label_map.json\"\n", "\n", "# If your CSVs are already uploaded to this environment (e.g., /mnt/data/train.csv), set like:\n", "# CSV_TRAIN = \"/mnt/data/train.csv\"\n", "# CSV_VAL   = \"/mnt/data/val.csv\"\n", "# CSV_TEST  = \"/mnt/data/test.csv\"\n", "# LABEL_MAP = \"/mnt/data/label_map.json\"\n", "\n", "# ==== DATA FORMAT ====\n", "# The CSV is expected to have at least two columns: 'image' and 'label'\n", "# - image: absolute path or path relative to some root.\n", "# - label: class name string (e.g., 'anger', 'happiness', etc.)\n", "# If your column names differ, edit the names below:\n", "COL_IMAGE = \"image\"\n", "COL_LABEL = \"label\"\n", "\n", "# If your CSV stores relative paths and you need to join a root directory, set IMG_ROOT:\n", "IMG_ROOT = \"\"  # e.g., \"/content/drive/MyDrive/ann-visual-emotion/data/raw/EmoSet\"\n", "\n", "# ==== HYPERPARAMETERS ====\n", "IMG_SIZE    = 224            # 224 works for most backbones\n", "BATCH_SIZE  = 64\n", "EPOCHS      = 30\n", "LR          = 3e-4\n", "WEIGHT_DECAY= 1e-4\n", "LABEL_SMOOTH= 0.05           # 0..0.2 is typical\n", "EARLY_STOP  = 8              # epochs of patience\n", "\n", "# Choose a backbone: 'resnet18', 'resnet50', 'efficientnet_b0'\n", "BACKBONE    = \"resnet18\"\n", "\n", "# Imbalance handling: class weights in CE loss (True) and/or WeightedRandomSampler (False)\n", "USE_CLASS_WEIGHTS      = True\n", "USE_WEIGHTED_SAMPLER   = False   # Set True if you prefer sampling instead of weights\n", "\n", "# Tricks\n", "USE_MIXED_PRECISION = True\n", "GRAD_CLIP_NORM      = 1.0\n", "USE_FOCAL_LOSS      = False   # If True, focal loss overrides CE w/ label-smoothing\n", "FOCAL_GAMMA         = 2.0\n", "WARMUP_EPOCHS       = 2       # simple linear warmup for LR at start\n", "SEED                = 42\n", "\n", "# === OUTPUTS ===\n", "OUT_DIR = \"./runs_fer_transfer\"\n", "os.makedirs(OUT_DIR, exist_ok=True)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def set_seed(seed=42):\n", "    random.seed(seed)\n", "    np.random.seed(seed)\n", "    torch.manual_seed(seed)\n", "    torch.cuda.manual_seed_all(seed)\n", "\n", "set_seed(SEED)\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(\"Device:\", device)\n", "\n", "def read_label_map(path):\n", "    if os.path.exists(path):\n", "        with open(path, \"r\") as f:\n", "            data = json.load(f)\n", "        # Accept either {\"amusement\":0, ...} or {\"idx2str\":[\"...\"], \"str2idx\":{...}}\n", "        if isinstance(data, dict) and \"str2idx\" in data and \"idx2str\" in data:\n", "            str2idx = {k:int(v) for k,v in data[\"str2idx\"].items()}\n", "            idx2str = {int(k):v for k,v in data[\"idx2str\"].items()}\n", "        else:\n", "            # assume flat mapping str->idx\n", "            str2idx = {k:int(v) for k,v in data.items()}\n", "            idx2str = {v:k for k,v in str2idx.items()}\n", "        return str2idx, idx2str\n", "    return None, None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Dataset & Dataloaders"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from PIL import Image\n", "\n", "class CSVDataset(Dataset):\n", "    def __init__(self, csv_path, transform=None, img_root=\"\", col_img=\"image\", col_lab=\"label\", encoder=None):\n", "        self.df = pd.read_csv(csv_path)\n", "        self.transform = transform\n", "        self.img_root = img_root\n", "        self.col_img = col_img\n", "        self.col_lab = col_lab\n", "        \n", "        if encoder is None:\n", "            self.encoder = LabelEncoder().fit(self.df[self.col_lab].astype(str).values)\n", "        else:\n", "            self.encoder = encoder\n", "        self.labels = self.encoder.transform(self.df[self.col_lab].astype(str).values)\n", "        self.n_classes = len(self.encoder.classes_)\n", "        \n", "        self.paths = self.df[self.col_img].astype(str).tolist()\n", "        \n", "    def __len__(self):\n", "        return len(self.df)\n", "    \n", "    def __getitem__(self, idx):\n", "        path = self.paths[idx]\n", "        if self.img_root and not os.path.isabs(path):\n", "            path = os.path.join(self.img_root, path)\n", "        img = Image.open(path).convert(\"RGB\")\n", "        if self.transform:\n", "            img = self.transform(img)\n", "        label = self.labels[idx]\n", "        return img, label\n", "\n", "def build_transforms(img_size=224):\n", "    # Augmentations tuned for faces (avoid crazy rotations/crops)\n", "    train_tf = T.Compose([\n", "        T.Resize(int(img_size*1.1)),\n", "        T.CenterCrop(img_size),\n", "        T.RandomHorizontalFlip(p=0.5),\n", "        T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.02),\n", "        T.ToTensor(),\n", "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n", "    ])\n", "    eval_tf = T.Compose([\n", "        T.Resize(int(img_size*1.1)),\n", "        T.CenterCrop(img_size),\n", "        T.ToTensor(),\n", "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n", "    ])\n", "    return train_tf, eval_tf\n", "\n", "train_tf, eval_tf = build_transforms(IMG_SIZE)\n", "\n", "# Label map (optional). If absent, will infer from CSV via LabelEncoder.\n", "str2idx, idx2str = read_label_map(LABEL_MAP)\n", "\n", "# Build train set (to establish encoder)\n", "enc = None\n", "if str2idx is not None:\n", "    # enforce mapping order\n", "    classes_sorted = [k for k,_ in sorted(str2idx.items(), key=lambda kv: kv[1])]\n", "    enc = LabelEncoder()\n", "    enc.fit(classes_sorted)\n", "\n", "ds_tr = CSVDataset(CSV_TRAIN, transform=train_tf, img_root=IMG_ROOT, col_img=COL_IMAGE, col_lab=COL_LABEL, encoder=enc)\n", "ds_va = CSVDataset(CSV_VAL,   transform=eval_tf,  img_root=IMG_ROOT, col_img=COL_IMAGE, col_lab=COL_LABEL, encoder=ds_tr.encoder)\n", "ds_te = None\n", "if os.path.exists(CSV_TEST):\n", "    ds_te = CSVDataset(CSV_TEST, transform=eval_tf, img_root=IMG_ROOT, col_img=COL_IMAGE, col_lab=COL_LABEL, encoder=ds_tr.encoder)\n", "\n", "classes = list(ds_tr.encoder.classes_)\n", "num_classes = len(classes)\n", "print(\"Classes:\", classes)\n", "print(\"Train/Val/Test sizes:\", len(ds_tr), len(ds_va), 0 if ds_te is None else len(ds_te))"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Compute class distribution for weights/sampler\n", "y_tr = ds_tr.labels\n", "class_counts = np.bincount(y_tr, minlength=num_classes).astype(float)\n", "class_weights = class_counts.sum() / (num_classes * np.maximum(class_counts, 1.0))\n", "class_weights_t = torch.tensor(class_weights, dtype=torch.float32)\n", "\n", "print(\"Class counts:\", class_counts)\n", "print(\"Class weights:\", class_weights)\n", "\n", "if USE_WEIGHTED_SAMPLER:\n", "    # Sampling probability inversely proportional to class frequency\n", "    sample_weights = class_weights[y_tr]\n", "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n", "    loader_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n", "else:\n", "    loader_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n", "\n", "loader_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n", "loader_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True) if ds_te else None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Model \u2014 choose a pretrained backbone"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def build_model(backbone:str, n_classes:int):\n", "    backbone = backbone.lower()\n", "    if backbone == \"resnet18\":\n", "        weights = models.ResNet18_Weights.DEFAULT\n", "        net = models.resnet18(weights=weights)\n", "        in_feats = net.fc.in_features\n", "        net.fc = nn.Sequential(\n", "            nn.Dropout(p=0.2),\n", "            nn.Linear(in_feats, n_classes)\n", "        )\n", "    elif backbone == \"resnet50\":\n", "        weights = models.ResNet50_Weights.DEFAULT\n", "        net = models.resnet50(weights=weights)\n", "        in_feats = net.fc.in_features\n", "        net.fc = nn.Sequential(\n", "            nn.Dropout(p=0.3),\n", "            nn.Linear(in_feats, n_classes)\n", "        )\n", "    elif backbone == \"efficientnet_b0\":\n", "        weights = models.EfficientNet_B0_Weights.DEFAULT\n", "        net = models.efficientnet_b0(weights=weights)\n", "        in_feats = net.classifier[1].in_features\n", "        net.classifier = nn.Sequential(\n", "            nn.Dropout(p=0.2),\n", "            nn.Linear(in_feats, n_classes)\n", "        )\n", "    else:\n", "        raise ValueError(f\"Unknown backbone: {backbone}\")\n", "    return net\n", "\n", "model = build_model(BACKBONE, num_classes).to(device)\n", "print(f\"Built model: {BACKBONE} with {num_classes} classes\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Losses and Optimizer"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["class LabelSmoothingCE(nn.Module):\n", "    def __init__(self, smoothing=0.0, weight=None, reduction=\"mean\"):\n", "        super().__init__()\n", "        self.smoothing = smoothing\n", "        self.weight = weight\n", "        self.reduction = reduction\n", "    def forward(self, logits, target):\n", "        n = logits.size(-1)\n", "        logp = F.log_softmax(logits, dim=-1)\n", "        with torch.no_grad():\n", "            true_dist = torch.zeros_like(logp)\n", "            true_dist.fill_(self.smoothing / (n - 1))\n", "            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n", "        if self.weight is not None:\n", "            w = self.weight.unsqueeze(0)\n", "            loss = -(true_dist * logp * w).sum(dim=1)\n", "        else:\n", "            loss = -(true_dist * logp).sum(dim=1)\n", "        if self.reduction == \"mean\":\n", "            return loss.mean()\n", "        elif self.reduction == \"sum\":\n", "            return loss.sum()\n", "        return loss\n", "\n", "class FocalLoss(nn.Module):\n", "    def __init__(self, gamma=2.0, weight=None, reduction=\"mean\"):\n", "        super().__init__()\n", "        self.gamma = gamma\n", "        self.weight = weight\n", "        self.reduction = reduction\n", "    def forward(self, logits, target):\n", "        logp = F.log_softmax(logits, dim=-1)\n", "        p = torch.exp(logp)\n", "        logpt = logp.gather(1, target.unsqueeze(1)).squeeze(1)\n", "        pt = p.gather(1, target.unsqueeze(1)).squeeze(1)\n", "        loss = -((1-pt)**self.gamma) * logpt\n", "        if self.weight is not None:\n", "            w = self.weight[target]\n", "            loss = loss * w\n", "        if self.reduction == \"mean\":\n", "            return loss.mean()\n", "        elif self.reduction == \"sum\":\n", "            return loss.sum()\n", "        return loss\n", "\n", "# Choose criterion\n", "weight_vec = class_weights_t.to(device) if USE_CLASS_WEIGHTS else None\n", "if USE_FOCAL_LOSS:\n", "    criterion = FocalLoss(gamma=FOCAL_GAMMA, weight=weight_vec)\n", "else:\n", "    criterion = LabelSmoothingCE(smoothing=LABEL_SMOOTH, weight=weight_vec)\n", "\n", "# Optimizer & Scheduler\n", "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n", "# Cosine schedule with T_max = EPOCHS; we'll do a simple warmup in the loop\n", "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Training & Evaluation"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def accuracy_from_logits(logits, y):\n", "    pred = logits.argmax(dim=1)\n", "    return (pred == y).float().mean().item()\n", "\n", "def train_one_epoch(model, loader, optimizer, scaler, epoch, total_epochs, warmup_epochs, grad_clip):\n", "    model.train()\n", "    losses = []\n", "    accs = []\n", "    pbar = tqdm(loader, desc=f\"Train {epoch+1}/{total_epochs}\")\n", "    for x, y in pbar:\n", "        x = x.to(device, non_blocking=True)\n", "        y = y.to(device, non_blocking=True)\n", "\n", "        # LR warmup (linear)\n", "        if warmup_epochs > 0 and epoch < warmup_epochs:\n", "            warmup_factor = (epoch + 1) / max(1, warmup_epochs)\n", "            for g in optimizer.param_groups:\n", "                g['lr'] = LR * warmup_factor\n", "\n", "        optimizer.zero_grad(set_to_none=True)\n", "        if scaler is not None:\n", "            with torch.cuda.amp.autocast():\n", "                logits = model(x)\n", "                loss = criterion(logits, y)\n", "            scaler.scale(loss).backward()\n", "            if grad_clip is not None and grad_clip > 0:\n", "                scaler.unscale_(optimizer)\n", "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n", "            scaler.step(optimizer)\n", "            scaler.update()\n", "        else:\n", "            logits = model(x)\n", "            loss = criterion(logits, y)\n", "            loss.backward()\n", "            if grad_clip is not None and grad_clip > 0:\n", "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n", "            optimizer.step()\n", "\n", "        acc = accuracy_from_logits(logits, y)\n", "        losses.append(loss.item())\n", "        accs.append(acc)\n", "        pbar.set_postfix(loss=np.mean(losses), acc=np.mean(accs))\n", "    return float(np.mean(losses)), float(np.mean(accs))\n", "\n", "@torch.no_grad()\n", "def evaluate(model, loader):\n", "    model.eval()\n", "    losses = []\n", "    accs = []\n", "    y_true, y_pred = [], []\n", "    for x, y in tqdm(loader, desc=\"Eval\", leave=False):\n", "        x = x.to(device, non_blocking=True)\n", "        y = y.to(device, non_blocking=True)\n", "        logits = model(x)\n", "        loss = criterion(logits, y)\n", "        acc = accuracy_from_logits(logits, y)\n", "        losses.append(loss.item())\n", "        accs.append(acc)\n", "        y_true.extend(y.cpu().numpy().tolist())\n", "        y_pred.extend(logits.argmax(dim=1).cpu().numpy().tolist())\n", "    return (float(np.mean(losses)), float(np.mean(accs)), \n", "            np.array(y_true, dtype=int), np.array(y_pred, dtype=int))\n", "\n", "# AMP scaler\n", "scaler = torch.cuda.amp.GradScaler() if (USE_MIXED_PRECISION and device.type==\"cuda\") else None\n", "\n", "best_val_acc = -1.0\n", "epochs_no_improve = 0\n", "ckpt_path = os.path.join(OUT_DIR, f\"{BACKBONE}_best.pt\")\n", "\n", "history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n", "\n", "for epoch in range(EPOCHS):\n", "    tr_loss, tr_acc = train_one_epoch(model, loader_tr, optimizer, scaler, epoch, EPOCHS, WARMUP_EPOCHS, GRAD_CLIP_NORM)\n", "    # Step cosine after each epoch (note: LR may have been warm-up adjusted; cosine still applies)\n", "    scheduler.step()\n", "    va_loss, va_acc, y_true, y_pred = evaluate(model, loader_va)\n", "\n", "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n", "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n", "\n", "    print(f\"Epoch {epoch+1:02d}: train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_loss={va_loss:.4f} acc={va_acc:.4f}\")\n", "    \n", "    # Early stopping + checkpoint\n", "    if va_acc > best_val_acc:\n", "        best_val_acc = va_acc\n", "        epochs_no_improve = 0\n", "        torch.save({\"model\": model.state_dict(), \"classes\": classes, \"backbone\": BACKBONE}, ckpt_path)\n", "        print(f\"  \u2705 New best val acc {best_val_acc:.4f}. Saved: {ckpt_path}\")\n", "    else:\n", "        epochs_no_improve += 1\n", "        if epochs_no_improve >= EARLY_STOP:\n", "            print(f\"  \u23f9\ufe0f Early stopping after {epoch+1} epochs (no improvement for {EARLY_STOP})\")\n", "            break\n", "\n", "# Plot history\n", "plt.figure(figsize=(6,4))\n", "plt.plot(history[\"train_acc\"], label=\"train_acc\")\n", "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n", "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.legend(); plt.title(\"Accuracy\")\n", "plt.show()\n", "\n", "plt.figure(figsize=(6,4))\n", "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n", "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n", "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.title(\"Loss\")\n", "plt.show()\n", "\n", "# Load best model for final eval\n", "state = torch.load(ckpt_path, map_location=device)\n", "model = build_model(state[\"backbone\"], len(state[\"classes\"])).to(device)\n", "model.load_state_dict(state[\"model\"])\n", "\n", "# Validation report\n", "_, va_acc, y_true, y_pred = evaluate(model, loader_va)\n", "print(\"\\nValidation accuracy (best):\", va_acc)\n", "print(\"\\nClassification Report (Val):\")\n", "print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n", "\n", "# Confusion matrix\n", "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n", "fig = plt.figure(figsize=(6,6))\n", "plt.imshow(cm, interpolation='nearest')\n", "plt.title(\"Confusion Matrix (Val)\")\n", "plt.colorbar()\n", "tick_marks = np.arange(num_classes)\n", "plt.xticks(tick_marks, classes, rotation=45, ha=\"right\")\n", "plt.yticks(tick_marks, classes)\n", "plt.xlabel(\"Predicted\")\n", "plt.ylabel(\"True\")\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Optional Test set\n", "if loader_te is not None:\n", "    _, te_acc, y_true_te, y_pred_te = evaluate(model, loader_te)\n", "    print(\"\\nTest accuracy:\", te_acc)\n", "    print(\"\\nClassification Report (Test):\")\n", "    print(classification_report(y_true_te, y_pred_te, target_names=classes, digits=4))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Inference on a single image"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from PIL import Image\n", "\n", "def load_model(ckpt_path, device=device):\n", "    state = torch.load(ckpt_path, map_location=device)\n", "    net = build_model(state[\"backbone\"], len(state[\"classes\"])).to(device)\n", "    net.load_state_dict(state[\"model\"])\n", "    net.eval()\n", "    return net, state[\"classes\"]\n", "\n", "infer_tf = T.Compose([\n", "    T.Resize(int(IMG_SIZE*1.1)),\n", "    T.CenterCrop(IMG_SIZE),\n", "    T.ToTensor(),\n", "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n", "])\n", "\n", "def predict_image(path, model, classes):\n", "    img = Image.open(path).convert(\"RGB\")\n", "    x = infer_tf(img).unsqueeze(0).to(device)\n", "    with torch.no_grad():\n", "        logits = model(x)\n", "        probs = logits.softmax(dim=1).cpu().numpy().squeeze()\n", "    topk = probs.argsort()[::-1][:5]\n", "    return [(classes[i], float(probs[i])) for i in topk]\n", "\n", "# Example:\n", "# model_loaded, classes_loaded = load_model(ckpt_path)\n", "# predict_image(\"/path/to/image.jpg\", model_loaded, classes_loaded)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}