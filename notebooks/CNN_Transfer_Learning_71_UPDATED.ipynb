{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d4bf90",
   "metadata": {},
   "source": [
    "\n",
    "# üß† CNN Transfer Learning ‚Äî 48√ó48 ‚ûú 224 (Updated)\n",
    "\n",
    "**Notebook:** `CNN_Transfer_Learning_71_UPDATED.ipynb`  \n",
    "**Focus:** 48√ó48 grayscale dataset ‚ûú upscale to 224, Transfer Learning (ResNet-18 by default), class-imbalance handling, warmup+cosine LR, early stopping, checkpoints, evaluation (macro-F1 + confusion matrix), simple TTA, and export (TorchScript/ONNX).\n",
    "\n",
    "> **Sinhala summary:**  \n",
    "> ‡∂î‡∂∂‡∂ú‡∑ö 48√ó48 ‡∂ö‡∑Ö‡∑î-‡∑É‡∑î‡∂Ø‡∑î (grayscale) emotion dataset ‡∂ë‡∂ö **224√ó224** ‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è upsample ‡∂ö‡∂ª‡∂Ω‡∑è **pretrained CNN** (ResNet-18) ‡∂ë‡∂ö‡∂ö‡∑ä **Transfer Learning** ‡∂∏‡∂ú‡∑í‡∂±‡∑ä train ‡∂ö‡∂ª‡∂±, imbalance ‡∂Ø‡∑î‡∂ª‡∂Ω‡∂±, LR schedule (warmup + cosine) ‡∂ë‡∂ö‡∑ä‡∂ö **‡∂∏‡∂±‡∑è‡∂¥ pipeline** ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂∏‡∑ô‡∑Ñ‡∑í ‡∑É‡∂∏‡∑ä‡∂¥‡∑ñ‡∂ª‡∑ä‡∂´‡∂∫‡∑í.  \n",
    "> **Train‚ÜíValidate‚ÜíFine-tune‚ÜíTest‚ÜíExport** ‡∂ë‡∂ö‡∂∏ file ‡∂ë‡∂ö‡∂ö!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed457fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# CONFIG (edit these first)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Your CSVs must have columns: path,label\n",
    "CSV_TRAIN = Path('/content/ann-visual-emotion/data/processed/EmoSet_splits/train.csv')\n",
    "CSV_VAL   = Path('/content/ann-visual-emotion/data/processed/EmoSet_splits/val.csv')\n",
    "CSV_TEST  = Path('/content/ann-visual-emotion/data/processed/EmoSet_splits/test.csv')\n",
    "\n",
    "# If image paths in CSV are relative, set the root so they resolve correctly.\n",
    "RAW_IMG_ROOT = Path('/content/ann-visual-emotion/data/raw/EmoSet')\n",
    "\n",
    "# Where to save model/checkpoints/metrics\n",
    "OUT_DIR = Path('./outputs_transfer_71')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training params\n",
    "NUM_CLASSES = 7               # change as needed\n",
    "BATCH_SIZE  = 32\n",
    "EPOCHS_HEAD = 10              # train classifier head first\n",
    "EPOCHS_FT   = 15              # fine-tune (unfreeze top layers)\n",
    "LR_HEAD     = 1e-3\n",
    "LR_FT_BB    = 1e-4            # backbone LR during fine-tune\n",
    "LR_FT_HEAD  = 5e-4            # head LR during fine-tune\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTH = 0.1            # 0.0 to disable\n",
    "USE_MIXUP    = True\n",
    "MIXUP_ALPHA  = 0.2\n",
    "USE_FP16     = True           # mixed precision\n",
    "\n",
    "# Sampler / imbalance\n",
    "USE_WEIGHTED_SAMPLER = True   # set False to disable\n",
    "CLASS_WEIGHTS_IN_LOSS = False # alternative to sampler\n",
    "\n",
    "# Inference / TTA\n",
    "USE_TTA = True\n",
    "\n",
    "# Export\n",
    "EXPORT_ONNX = True\n",
    "EXPORT_TORCHSCRIPT = True\n",
    "\n",
    "SEED = 42\n",
    "DEVICE_PREF = 'cuda'  # 'cuda' or 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Imports & Reproducibility\n",
    "# ========================\n",
    "import os, json, math, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from torch.cuda import amp\n",
    "    AMP_AVAILABLE = True\n",
    "except Exception:\n",
    "    AMP_AVAILABLE = False\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(DEVICE_PREF if torch.cuda.is_available() and DEVICE_PREF=='cuda' else 'cpu')\n",
    "print('Using device:', device)\n",
    "print('Torch:', torch.__version__, '| Torchvision:', torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# CSV Reading & Dataset\n",
    "# ========================\n",
    "READ_COLUMNS = ['path', 'label']\n",
    "\n",
    "def robust_read_csv(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # flexible column normalization\n",
    "    colmap = {}\n",
    "    for c in df.columns:\n",
    "        lc = c.strip().lower()\n",
    "        if lc in ['image','img','filepath','file','path']:\n",
    "            colmap[c] = 'path'\n",
    "        if lc in ['label','class','target','emotion']:\n",
    "            colmap[c] = 'label'\n",
    "    df.rename(columns=colmap, inplace=True)\n",
    "    assert READ_COLUMNS[0] in df.columns, f\"Missing column '{READ_COLUMNS[0]}'\"\n",
    "    assert READ_COLUMNS[1] in df.columns, f\"Missing column '{READ_COLUMNS[1]}'\"\n",
    "    return df\n",
    "\n",
    "def resolve_path(p: str, root: Path):\n",
    "    pp = Path(p)\n",
    "    if pp.is_absolute():\n",
    "        return pp\n",
    "    return (root / pp).resolve()\n",
    "\n",
    "train_df = robust_read_csv(CSV_TRAIN)\n",
    "val_df   = robust_read_csv(CSV_VAL)\n",
    "test_df  = robust_read_csv(CSV_TEST) if CSV_TEST.exists() else None\n",
    "\n",
    "# Build label maps (string‚Üîindex) from train set\n",
    "classes = sorted(train_df['label'].astype(str).unique().tolist())\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "assert len(classes) == NUM_CLASSES, f\"NUM_CLASSES={NUM_CLASSES} but found {len(classes)} in CSV\"\n",
    "\n",
    "def to_rgb_from_gray(img: Image.Image):\n",
    "    # if grayscale, convert to 3-ch by repeat\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('L')\n",
    "        img = Image.merge('RGB', (img, img, img))\n",
    "    return img\n",
    "\n",
    "INPUT_SIZE = 224\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Lambda(to_rgb_from_gray),\n",
    "    transforms.Resize((INPUT_SIZE, INPUT_SIZE), antialias=True),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=12),\n",
    "    transforms.RandomResizedCrop((INPUT_SIZE, INPUT_SIZE), scale=(0.9, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Lambda(to_rgb_from_gray),\n",
    "    transforms.Resize((INPUT_SIZE, INPUT_SIZE), antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, root: Path, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        p   = resolve_path(str(row['path']), self.root)\n",
    "        y   = class_to_idx[str(row['label'])]\n",
    "        img = Image.open(p).convert('RGB') if p.suffix.lower() in ['.jpg','.jpeg','.png','.bmp'] else Image.open(p)\n",
    "        # ensure RGB (even if input is gray)\n",
    "        img = to_rgb_from_gray(img)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, y, str(p)\n",
    "\n",
    "train_ds = CSVDataset(train_df, RAW_IMG_ROOT, transform=train_tfms)\n",
    "val_ds   = CSVDataset(val_df,   RAW_IMG_ROOT, transform=val_tfms)\n",
    "test_ds  = CSVDataset(test_df,  RAW_IMG_ROOT, transform=val_tfms) if test_df is not None else None\n",
    "\n",
    "print('Samples:', len(train_ds), len(val_ds), (len(test_ds) if test_ds else 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670150bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Sampler / Class Weights\n",
    "# ========================\n",
    "train_labels = [class_to_idx[str(l)] for l in train_df['label'].tolist()]\n",
    "counts = np.bincount(train_labels, minlength=NUM_CLASSES)\n",
    "class_weights = 1.0 / np.maximum(counts, 1)\n",
    "class_weights = class_weights / class_weights.sum() * NUM_CLASSES  # normalize\n",
    "\n",
    "print('Class counts:', dict(zip(classes, counts)))\n",
    "print('Class weights (normalized):', class_weights.round(3))\n",
    "\n",
    "sampler = None\n",
    "if USE_WEIGHTED_SAMPLER:\n",
    "    sample_weights = [class_weights[y] for y in train_labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    print('Using WeightedRandomSampler')\n",
    "else:\n",
    "    print('Using regular random sampling')\n",
    "\n",
    "if CLASS_WEIGHTS_IN_LOSS:\n",
    "    ce_weights = torch.tensor(class_weights, dtype=torch.float32, device='cpu')\n",
    "else:\n",
    "    ce_weights = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# DataLoaders\n",
    "# ========================\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=(sampler is None),\n",
    "                          sampler=sampler, num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True) if test_ds else None\n",
    "\n",
    "len(train_loader), len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Model: ResNet-18 (pretrained)\n",
    "# ========================\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "def build_model(num_classes: int):\n",
    "    weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "    m = resnet18(weights=weights)\n",
    "    # replace classifier\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.4),\n",
    "        nn.Linear(in_feats, num_classes)\n",
    "    )\n",
    "    return m\n",
    "\n",
    "model = build_model(NUM_CLASSES).to(device)\n",
    "\n",
    "# Freeze all (for head training)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "def count_trainable(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "print('Trainable params (head stage):', count_trainable(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14973508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Loss / MixUp / Schedulers\n",
    "# ========================\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, eps=0.1, reduction='mean', weight=None):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "        self.register_buffer('weight', weight if weight is not None else None)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        n = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logp)\n",
    "            true_dist.fill_(self.eps / (n - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.eps)\n",
    "        if self.weight is not None:\n",
    "            # apply per-class weights\n",
    "            w = self.weight[target].unsqueeze(1)\n",
    "            loss = (-true_dist * logp) * w\n",
    "        else:\n",
    "            loss = -true_dist * logp\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.sum(dim=1).mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha <= 0.0:\n",
    "        return x, y, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, (y_a, y_b), lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def make_criterion():\n",
    "    if LABEL_SMOOTH > 0.0:\n",
    "        return LabelSmoothingCE(eps=LABEL_SMOOTH, weight=ce_weights)\n",
    "    else:\n",
    "        return nn.CrossEntropyLoss(weight=ce_weights)\n",
    "\n",
    "def make_optimizer_head(model):\n",
    "    return torch.optim.AdamW(model.fc.parameters(), lr=LR_HEAD, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def make_optimizer_finetune(model):\n",
    "    # Different LR for backbone vs head\n",
    "    params = [\n",
    "        {'params': [p for n,p in model.named_parameters() if p.requires_grad and not n.startswith('fc.')], 'lr': LR_FT_BB},\n",
    "        {'params': model.fc.parameters(), 'lr': LR_FT_HEAD},\n",
    "    ]\n",
    "    return torch.optim.AdamW(params, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def make_warmup_cosine(optimizer, total_steps, warmup_steps=0.1):\n",
    "    if warmup_steps < 1.0:  # interpret as fraction\n",
    "        warmup_steps = int(total_steps * warmup_steps)\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Train / Eval loops\n",
    "# ========================\n",
    "def train_one_epoch(model, loader, optimizer, criterion, scaler=None):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if USE_MIXUP:\n",
    "            xm, (ya, yb), lam = mixup_data(x, y, alpha=MIXUP_ALPHA)\n",
    "            if scaler and AMP_AVAILABLE and USE_FP16:\n",
    "                with amp.autocast():\n",
    "                    logits = model(xm)\n",
    "                    loss = mixup_criterion(criterion, logits, ya, yb, lam)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                logits = model(xm)\n",
    "                loss = mixup_criterion(criterion, logits, ya, yb, lam)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # accuracy (approx) ‚Äì not exact with mixup; we skip accuracy here\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            total += x.size(0)\n",
    "        else:\n",
    "            if scaler and AMP_AVAILABLE and USE_FP16:\n",
    "                with amp.autocast():\n",
    "                    logits = model(x)\n",
    "                    loss = criterion(logits, y)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            total += x.size(0)\n",
    "\n",
    "    avg_loss = running_loss / max(1, total)\n",
    "    if USE_MIXUP:\n",
    "        return avg_loss, None  # accuracy not tracked under mixup\n",
    "    else:\n",
    "        return avg_loss, correct / max(1, total)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_y, all_p = [], []\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "        all_y.append(y.cpu().numpy())\n",
    "        all_p.append(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = running_loss / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    y_true = np.concatenate(all_y) if all_y else np.array([])\n",
    "    y_pred = np.concatenate(all_p) if all_p else np.array([])\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro') if len(y_true)>0 else 0.0\n",
    "    return avg_loss, acc, macro_f1, y_true, y_pred\n",
    "\n",
    "def save_ckpt(model, path: Path, extra: dict=None):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    payload = {'state_dict': model.state_dict(), 'classes': classes}\n",
    "    if extra: payload.update(extra)\n",
    "    torch.save(payload, path)\n",
    "    print('Saved:', path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Stage 1: Train classifier head\n",
    "# ========================\n",
    "criterion = make_criterion()\n",
    "optimizer = make_optimizer_head(model)\n",
    "total_steps = EPOCHS_HEAD * len(train_loader)\n",
    "scheduler = make_warmup_cosine(optimizer, total_steps, warmup_steps=0.1)\n",
    "scaler = amp.GradScaler(enabled=(AMP_AVAILABLE and USE_FP16))\n",
    "\n",
    "best_val_f1 = -1.0\n",
    "best_path = OUT_DIR / 'best_head.pt'\n",
    "\n",
    "for epoch in range(EPOCHS_HEAD):\n",
    "    tl, tacc = train_one_epoch(model, train_loader, optimizer, criterion, scaler=scaler)\n",
    "    vl, vacc, vf1, y_true, y_pred = evaluate(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"[Head] Epoch {epoch+1}/{EPOCHS_HEAD} | train_loss={tl:.4f} \"\n",
    "          f\"| val_loss={vl:.4f} | val_acc={vacc:.4f} | val_macroF1={vf1:.4f}\")\n",
    "\n",
    "    if vf1 > best_val_f1:\n",
    "        best_val_f1 = vf1\n",
    "        save_ckpt(model, best_path, extra={'stage': 'head', 'epoch': epoch+1, 'val_f1': vf1})\n",
    "\n",
    "print('Best val F1 (head):', round(best_val_f1, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Stage 2: Fine-tune (unfreeze top layers)\n",
    "# ========================\n",
    "# Unfreeze layer4 (top block) + fc\n",
    "for name, p in model.named_parameters():\n",
    "    if name.startswith('layer4.') or name.startswith('fc.'):\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "print('Trainable params (fine-tune stage):', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "criterion_ft = make_criterion()\n",
    "optimizer_ft = make_optimizer_finetune(model)\n",
    "total_steps_ft = EPOCHS_FT * len(train_loader)\n",
    "scheduler_ft = make_warmup_cosine(optimizer_ft, total_steps_ft, warmup_steps=0.1)\n",
    "scaler_ft = amp.GradScaler(enabled=(AMP_AVAILABLE and USE_FP16))\n",
    "\n",
    "best_val_f1_ft = -1.0\n",
    "best_path_ft = OUT_DIR / 'best_finetune.pt'\n",
    "patience = 7\n",
    "since_best = 0\n",
    "\n",
    "for epoch in range(EPOCHS_FT):\n",
    "    tl, tacc = train_one_epoch(model, train_loader, optimizer_ft, criterion_ft, scaler=scaler_ft)\n",
    "    vl, vacc, vf1, y_true, y_pred = evaluate(model, val_loader, criterion_ft)\n",
    "    scheduler_ft.step()\n",
    "\n",
    "    print(f\"[FT] Epoch {epoch+1}/{EPOCHS_FT} | train_loss={tl:.4f} \"\n",
    "          f\"| val_loss={vl:.4f} | val_acc={vacc:.4f} | val_macroF1={vf1:.4f}\")\n",
    "\n",
    "    if vf1 > best_val_f1_ft:\n",
    "        best_val_f1_ft = vf1\n",
    "        save_ckpt(model, best_path_ft, extra={'stage': 'finetune', 'epoch': epoch+1, 'val_f1': vf1})\n",
    "        since_best = 0\n",
    "    else:\n",
    "        since_best += 1\n",
    "        if since_best >= patience:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "print('Best val F1 (finetune):', round(best_val_f1_ft, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Final Eval (Test) + Confusion Matrix\n",
    "# ========================\n",
    "# Load best from FT if exists, else from head\n",
    "best_final = OUT_DIR / 'best_finetune.pt'\n",
    "if not best_final.exists():\n",
    "    best_final = OUT_DIR / 'best_head.pt'\n",
    "\n",
    "payload = torch.load(best_final, map_location='cpu')\n",
    "model.load_state_dict(payload['state_dict'])\n",
    "model.to(device).eval()\n",
    "print('Loaded best:', best_final)\n",
    "\n",
    "crit_eval = nn.CrossEntropyLoss(weight=ce_weights)\n",
    "\n",
    "if test_loader is not None:\n",
    "    tl, acc, mf1, y_true, y_pred = evaluate(model, test_loader, crit_eval)\n",
    "    print(f\"[TEST] loss={tl:.4f} | acc={acc:.4f} | macroF1={mf1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES)))\n",
    "    fig = plt.figure(figsize=(7,6))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(NUM_CLASSES)\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Pred')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No TEST set provided; skipping test evaluation.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54829220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# TTA + Single Image Inference\n",
    "# ========================\n",
    "base_tfms = transforms.Compose([\n",
    "    transforms.Lambda(to_rgb_from_gray),\n",
    "    transforms.Resize((INPUT_SIZE, INPUT_SIZE), antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "tta_tfms = [\n",
    "    base_tfms,\n",
    "    transforms.Compose([\n",
    "        transforms.Lambda(to_rgb_from_gray),\n",
    "        transforms.Resize((INPUT_SIZE, INPUT_SIZE), antialias=True),\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "]\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path: Path, use_tta: bool=True):\n",
    "    img = Image.open(img_path)\n",
    "    img = to_rgb_from_gray(img)\n",
    "    model.eval()\n",
    "    if use_tta and USE_TTA:\n",
    "        probs_accum = None\n",
    "        for t in tta_tfms:\n",
    "            x = t(img).unsqueeze(0).to(device)\n",
    "            logits = model(x)\n",
    "            probs = logits.softmax(dim=1)\n",
    "            probs_accum = probs if probs_accum is None else probs_accum + probs\n",
    "        probs = probs_accum / len(tta_tfms)\n",
    "    else:\n",
    "        x = base_tfms(img).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        probs = logits.softmax(dim=1)\n",
    "    conf, pred_idx = probs.max(dim=1)\n",
    "    return idx_to_class[pred_idx.item()], conf.item(), probs.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Example:\n",
    "# sample_img = resolve_path(train_df.iloc[0]['path'], RAW_IMG_ROOT)\n",
    "# label, conf, prob_vec = predict_image(sample_img, use_tta=True)\n",
    "# print('Pred:', label, 'conf:', round(conf,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Export: TorchScript / ONNX\n",
    "# ========================\n",
    "DUMMY = torch.randn(1,3,INPUT_SIZE,INPUT_SIZE, device=device)\n",
    "\n",
    "if EXPORT_TORCHSCRIPT:\n",
    "    traced = torch.jit.trace(model, DUMMY)\n",
    "    ts_path = OUT_DIR / 'best_model.ts'\n",
    "    traced.save(str(ts_path))\n",
    "    print('Saved TorchScript ->', ts_path)\n",
    "\n",
    "if EXPORT_ONNX:\n",
    "    onnx_path = OUT_DIR / 'best_model.onnx'\n",
    "    torch.onnx.export(\n",
    "        model, DUMMY, str(onnx_path),\n",
    "        input_names=['input'], output_names=['logits'],\n",
    "        dynamic_axes={'input': {0: 'batch'}, 'logits': {0: 'batch'}},\n",
    "        opset_version=12\n",
    "    )\n",
    "    print('Saved ONNX ->', onnx_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b020d7ed",
   "metadata": {},
   "source": [
    "\n",
    "## Tips & Troubleshooting (‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω‡∑ô‡∂±‡∑ä ‡∑É‡∑ô‡∂ß‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏)\n",
    "\n",
    "- **CSV columns** `path,label` ‡∂∫‡∂±‡∑ä‡∂± ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑ä‡∂±‡∂∏ ‡∂ï‡∂±. ‡∑Ä‡∑ô‡∂± ‡∂±‡∂∏‡∑ä top cell ‡∂ë‡∂ö‡∑ö mapping ‡∑Ä‡∑ô‡∂±‡∑É‡∑ä ‡∂ö‡∂ª‡∂Ω‡∑è ‡∂≠‡∑í‡∂∂‡∑ö.\n",
    "- **Paths relative** ‡∂±‡∂∏‡∑ä `RAW_IMG_ROOT` ‡∑Ñ‡∂ª‡∑í‡∂∫‡∑è‡∂ö‡∑è‡∂ª‡∑í‡∑Ä ‡∑É‡∂ö‡∑É‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
    "- **Class imbalance** ‡∂ú‡∑ê‡∂ß‡∂Ω‡∑î‡∑Ä‡∂ö‡∑ä ‡∂±‡∂∏‡∑ä: `USE_WEIGHTED_SAMPLER=True` ‡∂≠‡∂∂‡∂±‡∑ä‡∂± (default).\n",
    "- **Val macro-F1** ‡∑Ä‡∑ê‡∂©‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂±: LR ‡∂Ö‡∂©‡∑î/‡∑Ä‡∑ê‡∂©‡∑í ‡∂ö‡∂ª‡∂Ω‡∑è ‡∂∂‡∂Ω‡∂±‡∑ä‡∂±, `LABEL_SMOOTH=0.1 ‚Üí 0.05` ‡∑Ä‡∂ú‡∑ö tune ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
    "- **Slow training?** `BATCH_SIZE` ‡∂Ö‡∂©‡∑î/‡∑Ä‡∑ê‡∂©‡∑í, `USE_FP16=True` (if GPU), `num_workers=2‚Üí4` try ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
    "- **Visual sanity-check:** Confusion matrix ‡∂∂‡∂Ω‡∂Ω‡∑è ‡∑Ä‡∑ä‚Äç‡∂∫‡∑è‡∂ö‡∑ñ‡∂Ω pair (e.g., fear vs surprise) ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑ä‡∂©‡∑ô augmentations ‡∂ß‡∑ä‡∑Ä‡∑ì‡∂ö‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
    "- **Export**: TorchScript/ONNX files `./outputs_transfer_71/` ‡∂≠‡∑î‡∑Ö.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
