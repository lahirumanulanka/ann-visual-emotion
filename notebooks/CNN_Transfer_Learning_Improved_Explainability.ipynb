{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Emotion Classification with Transfer Learning (Improved)\n",
        "\n",
        "This notebook improves your previous pipeline with:\n",
        "- Two-stage fine-tuning (head-only warmup, then full unfreeze)\n",
        "- Better learning rate scheduling (Linear warmup + Cosine)\n",
        "- Stable mixed precision (no deprecation warnings)\n",
        "- Early stopping + EMA (Exponential Moving Average)\n",
        "- Clear, per-epoch logging including LR, losses, accuracy, macro F1\n",
        "- Robust evaluation: Confusion Matrix + Classification Report (Validation & Test)\n",
        "- Explainability: Grad-CAM/Grad-CAM++ (updated API), SHAP (DeepExplainer), LIME\n",
        "- Safe checkpoint loading for PyTorch 2.6 (weights_only change)\n",
        "- ONNX export and quick verification via ONNX Runtime\n",
        "\n",
        "Tips:\n",
        "- Confirm data CSV paths in the config cell.\n",
        "- If you have a larger GPU, you can raise `batch_size`.\n",
        "- You can switch backbones (resnet50 by default) to ConvNeXt/EfficientNet/ViT (requires `timm`).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If running on a fresh Colab, uncomment to install packages\n",
        "# %pip install -U timm pytorch-grad-cam shap lime onnx onnxruntime opencv-python seaborn scikit-learn"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import os, gc, json, math, time, random, pathlib\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    import timm\n", 
        "    TIMM_AVAILABLE = True\n",
        "except Exception:\n",
        "    TIMM_AVAILABLE = False\n",
        "\n",
        "# Explainability & Export\n",
        "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import shap\n",
        "from lime import lime_image\n",
        "import onnx, onnxruntime as ort\n",
        "import cv2"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config\n",
        "- Update the paths below to point to your CSVs.\n",
        "- `model_type`: choose 'resnet50' (default) or any timm model like 'convnext_base', 'tf_efficientnet_b3_ns', 'vit_base_patch16_224'.\n",
        "- `freeze_backbone_epochs`: head-only warmup epochs, then full unfreeze."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class CFG:\n",
        "    # Paths (update as needed)\n",
        "    project_root: Path = Path('/content/ann-visual-emotion')\n",
        "    csv_train: Path = Path('/content/ann-visual-emotion/data/processed/EmoSet_splits/train.csv')\n",
        "    csv_val:   Path = Path('/content/ann-visual-emotion/data/processed/EmoSet_splits/val.csv')\n",
        "    csv_test:  Path = Path('/content/ann-visual-emotion/data/processed/EmoSet_splits/test.csv')\n",
        "    label_map_json: Path = Path('/content/ann-visual-emotion/data/processed/EmoSet_splits/label_map.json')\n",
        "    out_dir: Path = Path('/content/ann-visual-emotion/artifacts_explainability')\n",
        "\n",
        "    image_col: str = 'path'\n",
        "    label_col: str = 'label'\n",
        "\n",
        "    # Backbone\n",
        "    model_type: str = 'resnet50'  # try: 'convnext_base', 'tf_efficientnet_b3_ns', 'vit_base_patch16_224'\n",
        "    pretrained: bool = True\n",
        "    img_size: int = 224\n",
        "\n",
        "    # Optimization\n",
        "    batch_size: int = 32\n",
        "    epochs: int = 60\n",
        "    freeze_backbone_epochs: int = 3  # head-only warmup\n",
        "    lr_backbone: float = 1e-4\n",
        "    lr_head: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    label_smoothing: float = 0.05\n",
        "    use_amp: bool = True\n",
        "    grad_clip_norm: float = 1.0\n",
        "\n",        
        "    # Regularization\n",
        "    use_mixup: bool = True\n",
        "    mixup_alpha: float = 0.4\n",
        "    use_cutmix: bool = False\n",
        "    cutmix_alpha: float = 1.0\n",
        "\n",
        "    # Early stopping\n",
        "    patience: int = 12\n",
        "    monitor: str = 'macro_f1'  # or 'acc'\n",
        "\n",
        "    # EMA\n",
        "    use_ema: bool = True\n",
        "    ema_decay: float = 0.999\n",
        "\n",
        "    # Class weighting / sampler\n",
        "    use_class_weights: bool = True\n",
        "    use_weighted_sampler: bool = False\n",
        "    class_weight_alpha: float = 0.6\n",
        "\n",
        "    # Explainability toggles\n",
        "    run_gradcam: bool = True\n",
        "    run_gradcam_pp: bool = True\n",
        "    run_shap: bool = True\n",
        "    run_lime: bool = True\n",
        "    run_vit_attn: bool = True  # only if model_type startswith('vit')\n",
        "\n",
        "    # ONNX export\n",
        "    export_onnx: bool = True\n",
        "    onnx_filename: str = 'best_model.onnx'\n",
        "\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "print(cfg)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils & setup\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(cfg.seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "def check_paths():\n",
        "    print('Project root:', cfg.project_root)\n",
        "    for p in [cfg.csv_train, cfg.csv_val, cfg.csv_test, cfg.label_map_json]:\n",
        "        print(f'{p}: {\"OK\" if Path(p).exists() else \"MISSING\"}')\n",
        "\n",
        "check_paths()\n",
        "\n",
        "train_df = pd.read_csv(cfg.csv_train)\n",
        "val_df   = pd.read_csv(cfg.csv_val)\n",
        "test_df  = pd.read_csv(cfg.csv_test)\n",
        "\n",
        "def load_label_map(path: Path, df: pd.DataFrame, label_col: str) -> Tuple[List[str], Dict[str,int]]:\n",
        "    if path.exists():\n",
        "        with open(path, 'r') as f:\n",
        "            lm = json.load(f)\n",
        "        label_to_id = {k: int(v) for k,v in lm.items()}\n",
        "    else:\n",
        "        unique = sorted(df[label_col].unique().tolist())\n",
        "        label_to_id = {lbl: i for i, lbl in enumerate(unique)}\n",
        "    classes = [lbl for lbl, _id in sorted(label_to_id.items(), key=lambda x: x[1])]\n",
        "    return classes, label_to_id\n",
        "\n",
        "CLASSES, class_to_idx = load_label_map(cfg.label_map_json, train_df, cfg.label_col)\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "print('Classes:', CLASSES)\n",
        "print('Train counts:', train_df[cfg.label_col].value_counts().to_dict())"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforms & Dataset\n",
        "- Strong but safe augmentations for train, deterministic center crop for val/test.\n",
        "- Optionally enable weighted sampler (dataset is near-balanced)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "def build_transforms(img_size):\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size, scale=(0.75, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(0.25, 0.25, 0.25, 0.10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "        transforms.RandomErasing(p=0.25, scale=(0.02, 0.12), value='random')\n",
        "    ])\n",
        "    val_tf = transforms.Compose([\n",
        "        transforms.Resize(int(img_size*1.14)),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    ])\n",
        "    return train_tf, val_tf\n",
        "\n",
        "train_tf, val_tf = build_transforms(cfg.img_size)\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, image_col, label_col, class_to_idx, transform=None, root: Optional[Path]=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_col = image_col\n",
        "        self.label_col = label_col\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.root = Path(root) if root else cfg.project_root\n",
        "\n",
        "    def resolve_path(self, raw):\n",
        "        p = Path(raw)\n",
        "        if p.is_absolute() and p.exists(): return p\n",
        "        try:\n",
        "            candidate = self.root / p.relative_to('/')\n",
        "        except Exception:\n",
        "            candidate = self.root / p\n",
        "        if candidate.exists(): return candidate\n",
        "        if p.exists(): return p\n",
        "        raise FileNotFoundError(f'Image not found: {raw}')\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.loc[idx]\n",
        "        img_path = self.resolve_path(row[self.image_col])\n",
        "        label_name = row[self.label_col]\n",
        "        label = self.class_to_idx[label_name]\n",
        "        with Image.open(img_path) as img:\n",
        "            img = img.convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "        return img, label, str(img_path)\n",
        "\n",
        "train_dataset = EmotionDataset(train_df, cfg.image_col, cfg.label_col, class_to_idx, transform=train_tf)\n",
        "val_dataset   = EmotionDataset(val_df,   cfg.image_col, cfg.label_col, class_to_idx, transform=val_tf)\n",
        "test_dataset  = EmotionDataset(test_df,  cfg.image_col, cfg.label_col, class_to_idx, transform=val_tf)\n",
        "\n",
        "# Class weights\n",
        "counts = train_df[cfg.label_col].value_counts().reindex(CLASSES).values.astype(float)\n",
        "if cfg.use_class_weights:\n",
        "    inv = 1.0 / (counts ** cfg.class_weight_alpha)\n",
        "    class_weights = inv / inv.sum() * len(CLASSES)\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "else:\n",
        "    class_weights_tensor = torch.ones(NUM_CLASSES, dtype=torch.float32).to(device)\n",
        "\n",
        "if cfg.use_weighted_sampler:\n",
        "    sample_weights = train_df[cfg.label_col].map({c: class_weights[i] for i,c in enumerate(CLASSES)}).values\n",
        "    sampler = WeightedRandomSampler(torch.DoubleTensor(sample_weights), num_samples=len(sample_weights), replacement=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "else:\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model\n",
        "- Choose the backbone via `cfg.model_type`.\n",
        "- Two param-groups: backbone LR vs head LR for discriminative fine-tuning."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def build_resnet50(num_classes, pretrained=True):\n",
        "    m = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
        "    feat = m.fc.in_features\n",
        "    m.fc = nn.Sequential(\n",
        "        nn.Linear(feat, 512), nn.ReLU(inplace=True), nn.LayerNorm(512), nn.Dropout(0.25),\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "    return m\n",
        "\n",
        "def build_timm(model_name: str, num_classes: int, pretrained=True):\n",
        "    assert TIMM_AVAILABLE, 'timm is required for this model.'\n",
        "    return timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
        "\n",
        "if cfg.model_type == 'resnet50':\n",
        "    model = build_resnet50(NUM_CLASSES, pretrained=cfg.pretrained)\n",
        "elif TIMM_AVAILABLE:\n",
        "    model = build_timm(cfg.model_type, NUM_CLASSES, pretrained=cfg.pretrained)\n",
        "else:\n",
        "    raise ValueError('Unsupported model type without timm installed.')\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "def make_param_groups(model, lr_backbone, lr_head):\n",
        "    backbone_params, head_params = [], []\n",
        "    # Heuristic head module names:\n",
        "    head_names = ['fc', 'classifier', 'head', 'heads', 'last_layer', 'pre_logits', 'head_drop']\n",
        "    head_modules = [getattr(model, hn) for hn in head_names if hasattr(model, hn)]\n",
        "    head_param_ids = set()\n",
        "    for hm in head_modules:\n",
        "        for p in hm.parameters(): head_param_ids.add(id(p))\n",
        "    for p in model.parameters():\n",
        "        if id(p) in head_param_ids: head_params.append(p)\n",
        "        else: backbone_params.append(p)\n",
        "    return [\n",
        "        {'params': backbone_params, 'lr': lr_backbone, 'base_lr': lr_backbone},\n",
        "        {'params': head_params,     'lr': lr_head,     'base_lr': lr_head},\n",
        "    ]\n",
        "\n",
        "optimizer = torch.optim.AdamW(make_param_groups(model, cfg.lr_backbone, cfg.lr_head), weight_decay=cfg.weight_decay)\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=cfg.use_amp)\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n] = p.data.clone()\n",
        "    def update(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n].mul_(self.decay).add_(p.data, alpha=1 - self.decay)\n",
        "    def apply(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad and n in self.shadow:\n",
        "                p.data.copy_(self.shadow[n])\n",
        "\n",
        "ema = EMA(model, decay=cfg.ema_decay) if cfg.use_ema else None\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=cfg.label_smoothing)\n",
        "\n",
        "# LR schedulers: Linear warmup then Cosine decay\n",
        "warmup_epochs = cfg.freeze_backbone_epochs\n",
        "main_epochs = max(1, cfg.epochs - warmup_epochs)\n",
        "warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs) if warmup_epochs>0 else None\n",
        "cosine = CosineAnnealingLR(optimizer, T_max=main_epochs)\n",
        "if warmup:\n",
        "    scheduler = SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
        "else:\n",
        "    scheduler = cosine\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Utilities"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_mixup(x, y, alpha=0.4):\n",
        "    if alpha <= 0: return x, (y, y, 1.0), False\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    mixed = lam * x + (1 - lam) * x[idx]\n",
        "    return mixed, (y, y[idx], lam), True\n",
        "\n",
        "def apply_cutmix(x, y, alpha=1.0):\n",
        "    if alpha <= 0: return x, (y, y, 1.0), False\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    B, C, H, W = x.size()\n",
        "    idx = torch.randperm(B, device=x.device)\n",
        "    rx, ry = np.random.randint(W), np.random.randint(H)\n",
        "    rw = int(W * math.sqrt(1 - lam))\n",
        "    rh = int(H * math.sqrt(1 - lam))\n",
        "    x1, y1 = np.clip(rx - rw//2, 0, W), np.clip(ry - rh//2, 0, H)\n",
        "    x2, y2 = np.clip(rx + rw//2, 0, W), np.clip(ry + rh//2, 0, H)\n",
        "    x[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n",
        "    lam_adjust = 1 - ((x2 - x1)*(y2 - y1)/(W*H))\n",
        "    return x, (y, y[idx], lam_adjust), True\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device, criterion, scaler=None):\n",
        "    model.train()\n",
        "    total_loss, total_correct, total = 0.0, 0.0, 0\n",
        "    for imgs, labels, _ in loader:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        mixed = False\n",
        "        if cfg.use_mixup and cfg.use_cutmix:\n",
        "            if random.random() < 0.5:\n",
        "                imgs, (ya, yb, lam), mixed = apply_mixup(imgs, labels, cfg.mixup_alpha)\n",
        "            else:\n",
        "                imgs, (ya, yb, lam), mixed = apply_cutmix(imgs, labels, cfg.cutmix_alpha)\n",
        "        elif cfg.use_mixup:\n",
        "            imgs, (ya, yb, lam), mixed = apply_mixup(imgs, labels, cfg.mixup_alpha)\n",
        "        elif cfg.use_cutmix:\n",
        "            imgs, (ya, yb, lam), mixed = apply_cutmix(imgs, labels, cfg.cutmix_alpha)\n",
        "\n",
        "        with torch.amp.autocast('cuda', enabled=cfg.use_amp):\n",
        "            logits = model(imgs)\n",
        "            if mixed:\n",
        "                loss = lam * criterion(logits, ya) + (1 - lam) * criterion(logits, yb)\n",
        "            else:\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            if cfg.grad_clip_norm is not None:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            if cfg.grad_clip_norm is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        if ema: ema.update(model)\n",
        "\n",
        "        bs = imgs.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        preds = logits.argmax(1)\n",
        "        if mixed:\n",
        "            total_correct += (lam * preds.eq(ya).sum().item() + (1 - lam) * preds.eq(yb).sum().item())\n",
        "        else:\n",
        "            total_correct += preds.eq(labels).sum().item()\n",
        "        total += bs\n",
        "\n",
        "    return total_loss / total, total_correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total = 0.0, 0.0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    for imgs, labels, _ in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        bs = imgs.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        preds = logits.argmax(1)\n",
        "        total_correct += preds.eq(labels).sum().item()\n",
        "        total += bs\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    return total_loss / total, acc, macro_f1, all_preds, all_labels\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, mode='max', min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.best = None\n",
        "        self.bad = 0\n",
        "        self.stop = False\n",
        "    def step(self, val):\n",
        "        if self.best is None:\n",
        "            self.best = val\n",
        "            return False\n",
        "        improve = (val - self.best) if self.mode == 'max' else (self.best - val)\n",
        "        if improve > self.min_delta:\n",
        "            self.best = val; self.bad = 0\n",
        "        else:\n",
        "            self.bad += 1\n",
        "        if self.bad >= self.patience:\n",
        "            self.stop = True\n",
        "        return self.stop\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train (Two-Stage Fine-Tuning)\n",
        "Stage 1: Train only the classification head (`freeze_backbone_epochs`).\n",
        "Stage 2: Unfreeze all layers and continue training with discriminative LRs.\n",
        "\n",
        "We print details per epoch: LRs of groups, Train/Val Loss, Acc, Macro F1. Best model by validation metric is saved."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training {cfg.model_type} for {cfg.epochs} epochs...')\n",
        "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[], 'val_macro_f1':[], 'lr_backbone':[], 'lr_head':[]}\n",
        "early_stopper = EarlyStopper(patience=cfg.patience, mode='max')\n",
        "best_metric, best_state = -1.0, None\n",
        "\n",
        "# Stage 1: freeze backbone\n",
        "if cfg.freeze_backbone_epochs > 0:\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'fc' in name or 'classifier' in name or 'head' in name or 'heads' in name:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "    print(f'Backbone frozen for {cfg.freeze_backbone_epochs} warmup epochs.')\n",
        "else:\n",
        "    print('No backbone freeze; training all layers from the start.')\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    # Unfreeze all after warmup\n",
        "    if epoch == cfg.freeze_backbone_epochs:\n",
        "        for p in model.parameters(): p.requires_grad = True\n",
        "        print('Unfroze backbone layers.')\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device, criterion, scaler)\n",
        "    if ema: ema.apply(model)\n",
        "    val_loss, val_acc, val_macro_f1, val_preds, val_labels = evaluate(model, val_loader, device, criterion)\n",
        "\n",
        "    # Step LR\n",
        "    scheduler.step()\n",
        "\n",
        "    # Log\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_macro_f1'].append(val_macro_f1)\n",
        "    # capture group LRs\n",
        "    if len(optimizer.param_groups) >= 2:\n",
        "        history['lr_backbone'].append(optimizer.param_groups[0]['lr'])\n",
        "        history['lr_head'].append(optimizer.param_groups[1]['lr'])\n",
        "    else:\n",
        "        history['lr_backbone'].append(optimizer.param_groups[0]['lr'])\n",
        "        history['lr_head'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    metric_now = val_macro_f1 if cfg.monitor == 'macro_f1' else val_acc\n",
        "    if metric_now > best_metric:\n",
        "        best_metric = metric_now\n",
        "        best_state = {'model': model.state_dict(), 'epoch': epoch, 'val_metric': best_metric, 'config': asdict(cfg)}\n",
        "        torch.save(best_state, cfg.out_dir / 'best_model.pth')\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{cfg.epochs} | LRs: bb={history['lr_backbone'][-1]:.6f}, head={history['lr_head'][-1]:.6f} | \"\n",
        "          f\"TrainLoss {train_loss:.4f} Acc {train_acc:.4f} | ValLoss {val_loss:.4f} Acc {val_acc:.4f} MacroF1 {val_macro_f1:.4f}\")\n",
        "\n",
        "    if early_stopper.step(metric_now):\n",
        "        print('Early stopping triggered.')\n",
        "        break\n",
        "\n",
        "print(f'Best {cfg.monitor} = {best_metric:.6f}')\n",
        "\n",
        "# Curves\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,3,1); plt.plot(history['train_loss'], label='TrainLoss'); plt.plot(history['val_loss'], label='ValLoss'); plt.legend(); plt.title('Loss')\n",
        "plt.subplot(1,3,2); plt.plot(history['train_acc'], label='TrainAcc'); plt.plot(history['val_acc'], label='ValAcc'); plt.plot(history['val_macro_f1'], label='ValMacroF1'); plt.legend(); plt.title('Acc & MacroF1')\n",
        "plt.subplot(1,3,3); plt.plot(history['lr_backbone'], label='LR_backbone'); plt.plot(history['lr_head'], label='LR_head'); plt.legend(); plt.title('LRs')\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation: Confusion Matrix + Classification Report (Best Checkpoint)\n",
        "PyTorch 2.6 note: `torch.load` default `weights_only=True`. We explicitly set `weights_only=False` to load our dict with metadata safely (we created it in this notebook). Only do this for checkpoints you trust."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Safe load (we created this checkpoint)\n",
        "ckpt = torch.load(cfg.out_dir / 'best_model.pth', map_location=device, weights_only=False)\n",
        "model.load_state_dict(ckpt['model'])\n",
        "if ema: ema.apply(model)\n",
        "model.eval()\n",
        "\n",
        "val_loss, val_acc, val_macro_f1, val_preds, val_labels = evaluate(model, val_loader, device, criterion)\n",
        "print(f'Validation (Best) Loss {val_loss:.4f} Acc {val_acc:.4f} MacroF1 {val_macro_f1:.4f}')\n",
        "print('\\nValidation Classification Report:\\n', classification_report(val_labels, val_preds, target_names=CLASSES, digits=4))\n",
        "\n",
        "cm_v = confusion_matrix(val_labels, val_preds, labels=list(range(NUM_CLASSES)))\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm_v, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "plt.title('Validation Confusion Matrix'); plt.xlabel('Predicted'); plt.ylabel('True'); plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Evaluation: Confusion Matrix + Classification Report"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, test_macro_f1, test_preds, test_labels = evaluate(model, test_loader, device, criterion)\n",
        "print(f'Test Loss {test_loss:.4f} Acc {test_acc:.4f} MacroF1 {test_macro_f1:.4f}')\n",
        "print('\\nTest Classification Report:\\n', classification_report(test_labels, test_preds, target_names=CLASSES, digits=4))\n",
        "\n",
        "cm_t = confusion_matrix(test_labels, test_preds, labels=list(range(NUM_CLASSES)))\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm_t, annot=True, fmt='d', cmap='Greens', xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "plt.title('Test Confusion Matrix'); plt.xlabel('Predicted'); plt.ylabel('True'); plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grad-CAM / Grad-CAM++\n",
        "Updated for current `pytorch-grad-cam` API (no `use_cuda` arg)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_rgb(img_t: torch.Tensor):\n",
        "    # img_t: 1x3xHxW normalized tensor\n",
        "    x = img_t.squeeze(0).detach().cpu().numpy()\n",
        "    x = np.transpose(x, (1,2,0))\n",
        "    x = (x * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN))\n",
        "    x = np.clip(x, 0, 1)\n",
        "    return x\n",
        "\n",
        "def select_target_layers_for_cam(model):\n",
        "    if cfg.model_type == 'resnet50':\n",
        "        return [model.layer4[-1]]\n",
        "    elif cfg.model_type.startswith('vit') and hasattr(model, 'blocks'):\n",
        "        return [model.blocks[-1].norm1] if hasattr(model.blocks[-1], 'norm1') else [model.blocks[-1]]\n",
        "    else:\n",
        "        # try to find last conv\n",
        "        for name, module in reversed(list(model.named_modules())):\n",
        "            if isinstance(module, nn.Conv2d): return [module]\n",
        "        raise ValueError('No suitable CAM layer found.')\n",
        "\n",
        "if cfg.run_gradcam or cfg.run_gradcam_pp:\n",
        "    imgs_vis, labels_vis = [], []\n",
        "    for i in range(min(6, len(test_dataset))):\n",
        "        img, lab, _ = test_dataset[i]\n",
        "        imgs_vis.append(img); labels_vis.append(lab)\n",
        "    imgs_vis_t = torch.stack(imgs_vis).to(device)\n",
        "    target_layers = select_target_layers_for_cam(model)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(imgs_vis_t)\n",
        "        preds = logits.argmax(1).cpu().numpy()\n",
        "\n",        
        "    cam_methods = []\n",
        "    if cfg.run_gradcam: cam_methods.append(('Grad-CAM', GradCAM))\n",
        "    if cfg.run_gradcam_pp: cam_methods.append(('Grad-CAM++', GradCAMPlusPlus))\n",
        "\n",
        "    for cam_name, CamClass in cam_methods:\n",
        "        cam = CamClass(model=model, target_layers=target_layers)\n",
        "        grayscale_cam = cam(input_tensor=imgs_vis_t)  # [N, H, W]\n",
        "        print(f'\\n{cam_name} visualizations:')\n",
        "        plt.figure(figsize=(14,7))\n",
        "        for i in range(len(imgs_vis)):\n",
        "            rgb = tensor_to_rgb(imgs_vis[i].unsqueeze(0))\n",
        "            grad_cam_map = grayscale_cam[i]\n",
        "            vis = show_cam_on_image(rgb, grad_cam_map, use_rgb=True)\n",
        "            plt.subplot(2, 3, i+1)\n",
        "            plt.imshow(vis)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Pred:{CLASSES[preds[i]]} | True:{CLASSES[labels_vis[i]]}')\n",
        "        plt.suptitle(cam_name)\n",
        "        plt.tight_layout(); plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SHAP (DeepExplainer)\n",
        "We use a small background set for speed. DeepExplainer supports PyTorch models directly."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg.run_shap:\n",
        "    print('Running SHAP (small subset for speed)...')\n",
        "    # background tensors (normalized already)\n",
        "    bg_idx = np.random.choice(len(train_dataset), size=min(16, len(train_dataset)), replace=False)\n",
        "    bg_tensors = torch.stack([train_dataset[i][0] for i in bg_idx], dim=0).to(device)\n",
        "\n",
        "    # small test batch\n",
        "    test_idx = np.random.choice(len(test_dataset), size=min(4, len(test_dataset)), replace=False)\n",
        "    test_tensors = torch.stack([test_dataset[i][0] for i in test_idx], dim=0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    explainer = shap.DeepExplainer(model, bg_tensors)\n",
        "    shap_vals = explainer.shap_values(test_tensors)  # list of [C x N x 3 x H x W]\n",
        "\n",
        "    # For each sample, pick predicted class and show mean |SHAP| overlay\n",
        "    with torch.no_grad():\n",
        "        logits = model(test_tensors)\n",
        "        preds = logits.argmax(1).cpu().numpy()\n",
        "    test_np = test_tensors.detach().cpu().numpy()\n",
        "\n",
        "    for i in range(test_np.shape[0]):\n",
        "        pc = preds[i]\n",
        "        sv_c = shap_vals[pc][i]  # [3, H, W]\n",
        "        sv_gray = np.mean(np.abs(sv_c), axis=0)\n",
        "        img = np.transpose(test_np[i], (1,2,0))\n",
        "        img = (img * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN))\n",
        "        img = np.clip(img, 0, 1)\n",
        "        plt.figure(figsize=(8,3))\n",
        "        plt.subplot(1,2,1); plt.imshow(img); plt.axis('off'); plt.title(f'Pred: {CLASSES[pc]}')\n",
        "        plt.subplot(1,2,2); plt.imshow(sv_gray, cmap='seismic'); plt.colorbar(); plt.axis('off'); plt.title('SHAP |abs|')\n",
        "        plt.tight_layout(); plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIME (small subset)\n",
        "Model-agnostic local explanations on a few samples."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg.run_lime:\n",
        "    print('Running LIME (small subset for speed)...')\n",
        "    explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "    def predict_fn_for_explain(x: np.ndarray) -> np.ndarray:\n",
        "        # x: N,H,W,3 uint8 or float [0..255]\n",
        "        xs = []\n",
        "        for i in range(x.shape[0]):\n",
        "            img = x[i].astype(np.float32)\n",
        "            if img.max() > 1.5: img = img / 255.0\n",
        "            img_t = torch.tensor(np.transpose(img, (2,0,1)), dtype=torch.float32)\n",
        "            # normalize\n",
        "            for c in range(3):\n",
        "                img_t[c] = (img_t[c] - IMAGENET_MEAN[c]) / IMAGENET_STD[c]\n",
        "            xs.append(img_t)\n",
        "        xs = torch.stack(xs).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(xs)\n",
        "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "        return probs\n",
        "\n",
        "    lime_idx = np.random.choice(len(test_dataset), size=min(4, len(test_dataset)), replace=False)\n",
        "    for idx in lime_idx:\n",
        "        img_t, _, _ = test_dataset[idx]\n",
        "        img = np.transpose(img_t.numpy(), (1,2,0))\n",
        "        img_rgb = (np.clip((img * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)), 0, 1) * 255).astype(np.uint8)\n",
        "        explanation = explainer.explain_instance(\n",
        "            image=img_rgb,\n",
        "            classifier_fn=predict_fn_for_explain,\n",
        "            top_labels=1,\n",
        "            hide_color=0,\n",
        "            num_samples=600\n",
        "        )\n",
        "        top = explanation.top_labels[0]\n",
        "        temp, mask = explanation.get_image_and_mask(label=top, positive_only=True, num_features=6, hide_rest=False)\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.subplot(1,2,1); plt.imshow(img_rgb); plt.axis('off'); plt.title('Original')\n",
        "        plt.subplot(1,2,2); plt.imshow(temp); plt.axis('off'); plt.title(f'LIME Top: {CLASSES[top]}')\n",
        "        plt.tight_layout(); plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ONNX Export + Runtime Check"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg.export_onnx:\n",
        "    model.eval()\n",
        "    onnx_path = cfg.out_dir / cfg.onnx_filename\n",
        "    dummy = torch.randn(1, 3, cfg.img_size, cfg.img_size, device=device)\n",
        "    dynamic_axes = {'input': {0: 'batch'}, 'logits': {0: 'batch'}}\n",
        "    torch.onnx.export(\n",
        "        model, dummy, str(onnx_path), input_names=['input'], output_names=['logits'],\n",
        "        dynamic_axes=dynamic_axes, opset_version=17, do_constant_folding=True\n",
        "    )\n",
        "    print('Saved ONNX to:', onnx_path)\n",
        "    sess = ort.InferenceSession(str(onnx_path), providers=['CPUExecutionProvider'])\n",
        "    out = sess.run(None, {'input': dummy.detach().cpu().numpy().astype(np.float32)})\n",
        "    print('ONNXRuntime output shape:', out[0].shape)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}