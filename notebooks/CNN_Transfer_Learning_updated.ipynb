{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpOTlhgalZoU",
        "outputId": "9f8f87c6-9e0a-495f-f683-3e0cc658c278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ann-visual-emotion' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b reduce_features https://github.com/lahirumanulanka/ann-visual-emotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNYScK2vlU3V"
      },
      "source": [
        "# CNN Transfer Learning for Visual Emotion Recognition\n",
        "\n",
        "This notebook demonstrates how to implement transfer learning with CNN networks for visual emotion recognition. We'll use a pre-trained VGG16 model and fine-tune it for our emotion classification task.\n",
        "\n",
        "## What is Transfer Learning?\n",
        "Transfer learning is a machine learning technique where we use a model that has been trained on one task and adapt it for a related task. In our case, we'll use a CNN pre-trained on ImageNet and adapt it for emotion recognition.\n",
        "\n",
        "## Benefits of Transfer Learning:\n",
        "1. **Faster training**: We start with pre-trained weights instead of random initialization\n",
        "2. **Better performance**: Especially when we have limited training data\n",
        "3. **Lower computational requirements**: Less training time needed\n",
        "4. **Proven feature extractors**: Pre-trained networks have learned robust low-level features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPhiOh1PlU3W"
      },
      "source": [
        "## Step 1: Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_HTJV22lU3W",
        "outputId": "7b1d40fd-c89d-4c1c-ff01-9c330c685e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# Sklearn for metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM3vMzXQlU3X"
      },
      "source": [
        "## Step 2: Data Preparation\n",
        "\n",
        "First, let's set up our data paths and load the dataset splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ouHDnCxlU3X",
        "outputId": "dc828b77-222e-4097-ad91-2463abc1c5ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking data files...\n",
            "✓ Found: /content/ann-visual-emotion/data/processed/EmoSet_splits/train.csv\n",
            "✓ Found: /content/ann-visual-emotion/data/processed/EmoSet_splits/val.csv\n",
            "✓ Found: /content/ann-visual-emotion/data/processed/EmoSet_splits/test.csv\n",
            "✓ Found: /content/ann-visual-emotion/data/processed/EmoSet_splits/label_map.json\n",
            "Number of emotion classes: 5\n",
            "Emotion classes: ['angry', 'happy', 'neutral', 'sad', 'surprise']\n"
          ]
        }
      ],
      "source": [
        "# Data paths (modify these if running locally)\n",
        "\n",
        "PROJECT_ROOT = Path('/content/ann-visual-emotion')\n",
        "CSV_TRAIN = PROJECT_ROOT / 'data/processed/EmoSet_splits/train.csv'\n",
        "CSV_VAL = PROJECT_ROOT / 'data/processed/EmoSet_splits/val.csv'\n",
        "CSV_TEST = PROJECT_ROOT / 'data/processed/EmoSet_splits/test.csv'\n",
        "LABEL_MAP_PATH = PROJECT_ROOT / 'data/processed/EmoSet_splits/label_map.json'\n",
        "DATA_DIR = PROJECT_ROOT / 'data/raw/EmoSet'\n",
        "\n",
        "# Check if files exist\n",
        "print(\"Checking data files...\")\n",
        "for path in [CSV_TRAIN, CSV_VAL, CSV_TEST, LABEL_MAP_PATH]:\n",
        "    if path.exists():\n",
        "        print(f\"✓ Found: {path}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {path}\")\n",
        "\n",
        "# Load label map if it exists\n",
        "if LABEL_MAP_PATH.exists():\n",
        "    with open(LABEL_MAP_PATH, 'r') as f:\n",
        "        label_map = json.load(f)\n",
        "    num_classes = len(label_map)\n",
        "    print(f'Number of emotion classes: {num_classes}')\n",
        "    print(f'Emotion classes: {list(label_map.keys())}')\n",
        "else:\n",
        "    # Create a dummy label map for testing\n",
        "    label_map = {'anger': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, 'sadness': 4, 'surprise': 5, 'neutral': 6}\n",
        "    num_classes = len(label_map)\n",
        "    print(f'Using dummy label map with {num_classes} classes: {list(label_map.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-tciV8VA-qw"
      },
      "source": [
        "## (Optional) Step 2b: Generate 224×224 Resized Dataset and Updated CSV Splits\n",
        "\n",
        "In case your original FER-style images are 48×48 (grayscale) and you want to:\n",
        "\n",
        "1. Upscale them to 224×224 (required for many ImageNet pre-trained backbones).\n",
        "2. Store the resized copies in a separate folder structure (`class` subfolders).\n",
        "3. Regenerate new CSV split files (`train_224.csv`, `val_224.csv`, `test_224.csv`) that point directly to the resized images.\n",
        "4. Produce an updated stats file (`stats_resized_224.json`) summarizing counts per class.\n",
        "\n",
        "You can run the following cells once. They are resume-friendly (will skip already processed files by default). Set `FORCE_OVERWRITE = True` to re-create images.\n",
        "\n",
        "If you already have resized images / CSVs, you can skip this section and set `USE_RESIZED_SPLITS = True` later to automatically pick them up for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "1atI0g4CA-qw"
      },
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "# import pandas as pd\n",
        "# from PIL import Image\n",
        "# from tqdm import tqdm\n",
        "# import json\n",
        "\n",
        "# # Configuration for resizing\n",
        "# RESIZE_ROOT = PROJECT_ROOT  # base project root (already defined earlier)\n",
        "# RAW_IMG_ROOT = PROJECT_ROOT / 'data/raw/EmoSet'\n",
        "# PROCESSED_SPLITS_DIR = PROJECT_ROOT / 'data/processed/EmoSet_splits'\n",
        "# RESIZED_OUT_DIR = PROJECT_ROOT / 'data/processed/EmoSet_resized_224'\n",
        "# RESIZED_IMG_DIR = RESIZED_OUT_DIR / 'images_224'\n",
        "# RESIZED_SPLIT_DIR = RESIZED_OUT_DIR / 'splits'\n",
        "# RESIZED_STATS_PATH = RESIZED_OUT_DIR / 'stats_resized_224.json'\n",
        "\n",
        "# # Control flags\n",
        "# SKIP_IF_EXISTS = True       # Don't re-generate images if target file exists\n",
        "# FORCE_OVERWRITE = False     # If True, ignore SKIP_IF_EXISTS\n",
        "# USE_RESIZED_SPLITS = True   # Later, if True and splits exist, training section will use them\n",
        "# GRAYSCALE_SOURCE = True     # If original images are grayscale 48x48\n",
        "# TARGET_SIZE = (224, 224)\n",
        "\n",
        "# RESIZED_IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "# RESIZED_SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# SUPPORTED_PATH_COLS  = [\"path\", \"filepath\", \"file_path\", \"image_path\", \"img_path\"]\n",
        "# SUPPORTED_LABEL_COLS = [\"label\", \"class\", \"emotion\", \"target\", \"y\", \"label_id\"]\n",
        "\n",
        "\n",
        "# def robust_read_csv(csv_path: Path) -> pd.DataFrame:\n",
        "#     df = pd.read_csv(csv_path)\n",
        "#     df.rename(columns={c: c.strip().lower() for c in df.columns}, inplace=True)\n",
        "#     path_col = next((c for c in SUPPORTED_PATH_COLS if c in df.columns), None)\n",
        "#     label_col = next((c for c in SUPPORTED_LABEL_COLS if c in df.columns), None)\n",
        "#     if path_col is None or label_col is None:\n",
        "#         raise AssertionError(f\"CSV must contain a path+label column. Found columns: {list(df.columns)}\")\n",
        "#     out = df[[path_col, label_col]].copy()\n",
        "#     out.columns = [\"path\", \"label\"]\n",
        "#     out[\"label\"] = out[\"label\"].astype(str)\n",
        "#     return out\n",
        "\n",
        "\n",
        "# def resolve_path(p: str) -> Path:\n",
        "#     pth = Path(p)\n",
        "#     if pth.exists():\n",
        "#         return pth\n",
        "#     alt = RAW_IMG_ROOT / pth\n",
        "#     return alt if alt.exists() else alt  # return alt even if missing for error reporting\n",
        "\n",
        "\n",
        "# def out_path_for(in_path: Path, label: str) -> Path:\n",
        "#     out_dir = RESIZED_IMG_DIR / label\n",
        "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
        "#     return out_dir / (in_path.stem + '.jpg')\n",
        "\n",
        "\n",
        "# def resize_to_target(img: Image.Image) -> Image.Image:\n",
        "#     if GRAYSCALE_SOURCE:\n",
        "#         img = img.convert('L')  # ensure grayscale\n",
        "#         # convert to 3-channel RGB for pre-trained models after resizing\n",
        "#         img = img.resize(TARGET_SIZE, resample=Image.BICUBIC).convert('RGB')\n",
        "#     else:\n",
        "#         img = img.convert('RGB').resize(TARGET_SIZE, resample=Image.BICUBIC)\n",
        "#     return img\n",
        "\n",
        "\n",
        "# def process_split(csv_path: Path, out_csv_path: Path):\n",
        "#     if not csv_path.exists():\n",
        "#         print(f\"[skip] Missing split file: {csv_path}\")\n",
        "#         return None\n",
        "\n",
        "#     df = robust_read_csv(csv_path)\n",
        "#     rows = []\n",
        "#     print(f\"Processing {csv_path.name} -> {out_csv_path.name}  (n={len(df)})\")\n",
        "\n",
        "#     for r in tqdm(df.itertuples(index=False), total=len(df)):\n",
        "#         in_path = resolve_path(r.path)\n",
        "#         label = str(r.label)\n",
        "#         out_img_path = out_path_for(in_path, label)\n",
        "\n",
        "#         if FORCE_OVERWRITE is False and SKIP_IF_EXISTS and out_img_path.exists():\n",
        "#             rows.append({\"path\": str(out_img_path.relative_to(PROJECT_ROOT)), \"label\": label})\n",
        "#             continue\n",
        "\n",
        "#         try:\n",
        "#             img = Image.open(in_path)\n",
        "#             img224 = resize_to_target(img)\n",
        "#             out_img_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "#             img224.save(out_img_path, format='JPEG', quality=95)\n",
        "#             rows.append({\"path\": str(out_img_path.relative_to(PROJECT_ROOT)), \"label\": label})\n",
        "#         except Exception as e:\n",
        "#             print(f\"[error] {in_path}: {e}\")\n",
        "\n",
        "#     if rows:\n",
        "#         out_df = pd.DataFrame(rows)\n",
        "#         out_df.to_csv(out_csv_path, index=False)\n",
        "#         print(f\"Wrote {out_csv_path} (rows={len(out_df)})\")\n",
        "#     else:\n",
        "#         print(f\"No rows written for {csv_path}\")\n",
        "#     return rows\n",
        "\n",
        "# # Run resizing for splits if original CSVs exist\n",
        "# resized_train_csv = RESIZED_SPLIT_DIR / 'train_224.csv'\n",
        "# resized_val_csv = RESIZED_SPLIT_DIR / 'val_224.csv'\n",
        "# resized_test_csv = RESIZED_SPLIT_DIR / 'test_224.csv'\n",
        "\n",
        "# if CSV_TRAIN.exists():\n",
        "#     process_split(CSV_TRAIN, resized_train_csv)\n",
        "# if CSV_VAL.exists():\n",
        "#     process_split(CSV_VAL, resized_val_csv)\n",
        "# if CSV_TEST.exists():\n",
        "#     process_split(CSV_TEST, resized_test_csv)\n",
        "\n",
        "# # Aggregate stats\n",
        "# if resized_train_csv.exists():\n",
        "#     train_df_r = pd.read_csv(resized_train_csv)\n",
        "#     cls_counts = train_df_r['label'].value_counts().to_dict()\n",
        "#     stats = {\n",
        "#         'total_train_images': int(len(train_df_r)),\n",
        "#         'classes': cls_counts,\n",
        "#         'num_classes': int(len(cls_counts)),\n",
        "#         'target_size': list(TARGET_SIZE),\n",
        "#         'grayscale_source': GRAYSCALE_SOURCE,\n",
        "#     }\n",
        "#     with open(RESIZED_STATS_PATH, 'w') as f:\n",
        "#         json.dump(stats, f, indent=2)\n",
        "#     print(f\"Wrote stats: {RESIZED_STATS_PATH}\")\n",
        "# else:\n",
        "#     print(\"Resized training CSV not found; skipping stats generation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8difZpzHlU3Y"
      },
      "source": [
        "## Step 3: Custom Dataset Class\n",
        "\n",
        "We'll create a custom dataset class that handles both grayscale and RGB images for transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "HoonO1_0lU3Y"
      },
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for emotion recognition.\n",
        "    Supports both grayscale and RGB images for transfer learning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, root_dir, transform=None, label_map=None, rgb=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe: pandas DataFrame with image paths and labels\n",
        "            root_dir: Root directory containing images\n",
        "            transform: Optional transform to be applied on images\n",
        "            label_map: Dictionary mapping emotion names to indices\n",
        "            rgb: If True, convert grayscale images to RGB for pre-trained models\n",
        "        \"\"\"\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.label_map = label_map\n",
        "        self.rgb = rgb\n",
        "\n",
        "        # Auto-detect column names\n",
        "        possible_path_cols = [c for c in self.df.columns if 'path' in c.lower() or 'file' in c.lower() or 'image' in c.lower()]\n",
        "        self.path_col = possible_path_cols[0] if possible_path_cols else self.df.columns[0]\n",
        "\n",
        "        possible_label_cols = [c for c in self.df.columns if 'label' in c.lower() or 'class' in c.lower() or 'emotion' in c.lower()]\n",
        "        self.label_col = possible_label_cols[0] if possible_label_cols else self.df.columns[1]\n",
        "\n",
        "        print(f\"Using columns - Path: '{self.path_col}', Label: '{self.label_col}'\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        rel_path = row[self.path_col]\n",
        "        label = row[self.label_col]\n",
        "\n",
        "        # Convert label to index if needed\n",
        "        if self.label_map and isinstance(label, str):\n",
        "            label_idx = self.label_map[label]\n",
        "        else:\n",
        "            label_idx = int(label)\n",
        "\n",
        "        # Load image\n",
        "        img_path = self.root_dir / rel_path\n",
        "\n",
        "        try:\n",
        "            if self.rgb:\n",
        "                # Load as RGB for pre-trained models\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "            else:\n",
        "                # Load as grayscale\n",
        "                image = Image.open(img_path).convert('L')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a dummy image in case of error\n",
        "            if self.rgb:\n",
        "                image = Image.new('RGB', (48, 48), color='black')\n",
        "            else:\n",
        "                image = Image.new('L', (48, 48), color='black')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m_AdbRMlU3Y"
      },
      "source": [
        "## Step 4: Data Transforms for Transfer Learning\n",
        "\n",
        "For transfer learning with pre-trained models, we need to:\n",
        "1. Resize images to the expected input size (224x224 for VGG)\n",
        "2. Normalize with ImageNet statistics\n",
        "3. Apply data augmentation for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D1tn-VGlU3Y",
        "outputId": "f6a8c1b9-62c2-4e53-8a0a-8c12d9991fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data transforms created (grayscale mode):\n",
            "- Input size: 48x48\n",
            "- Grayscale normalization: mean=[0.5025611264067582], std=[0.13170107979280243]\n"
          ]
        }
      ],
      "source": [
        "# Grayscale normalization values derived from global_pixel_stats.json\n",
        "# global_mean_mean ~127.03, global_mean_std ~34.50 (values in [0,255]) so scale to [0,1]\n",
        "GRAYSCALE_MEAN = [128.15308723372334 / 255.0]\n",
        "GRAYSCALE_STD = [33.58377534716462 / 255.0]\n",
        "\n",
        "INPUT_SIZE = 48  # Already resized dataset\n",
        "USE_GRAYSCALE = True\n",
        "\n",
        "if USE_GRAYSCALE:\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ToTensor(),  # Produces 1xHxW for L mode\n",
        "        transforms.Normalize(mean=GRAYSCALE_MEAN, std=GRAYSCALE_STD)\n",
        "    ])\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=GRAYSCALE_MEAN, std=GRAYSCALE_STD)\n",
        "    ])\n",
        "else:\n",
        "    # ImageNet normalization (fallback if switching back to RGB)\n",
        "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "    ])\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "    ])\n",
        "\n",
        "print(\"Data transforms created (grayscale mode):\" if USE_GRAYSCALE else \"Data transforms created (RGB mode):\")\n",
        "print(f\"- Input size: {INPUT_SIZE}x{INPUT_SIZE}\")\n",
        "if USE_GRAYSCALE:\n",
        "    print(f\"- Grayscale normalization: mean={GRAYSCALE_MEAN}, std={GRAYSCALE_STD}\")\n",
        "else:\n",
        "    print(f\"- ImageNet normalization: mean={IMAGENET_MEAN}, std={IMAGENET_STD}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUUmG1bnlU3Z"
      },
      "source": [
        "## Step 5: Create Datasets and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNVXKgpalU3Z",
        "outputId": "a516f9ab-0ca7-444e-c892-c48f5acc188d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "✓ Using resized 224×224 split CSVs\n",
            "DATA_DIR adjusted to project root for resized relative paths.\n",
            "Dataset sizes:\n",
            "- Training: 20150\n",
            "- Validation: 4030\n",
            "- Testing: 1005\n",
            "Using columns - Path: 'path', Label: 'label'\n",
            "Using columns - Path: 'path', Label: 'label'\n",
            "Using columns - Path: 'path', Label: 'label'\n",
            "DataLoaders created with batch size: 32\n"
          ]
        }
      ],
      "source": [
        "# Create dummy datasets if actual data files don't exist\n",
        "if not all(path.exists() for path in [CSV_TRAIN, CSV_VAL, CSV_TEST]):\n",
        "    print(\"Creating dummy datasets for demonstration...\")\n",
        "\n",
        "    # Create dummy data\n",
        "    dummy_data = {\n",
        "        'image_path': [f'dummy_{i}.jpg' for i in range(100)],\n",
        "        'emotion': np.random.choice(list(label_map.keys()), 100)\n",
        "    }\n",
        "\n",
        "    train_df = pd.DataFrame(dummy_data)\n",
        "    val_df = pd.DataFrame(dummy_data)\n",
        "    test_df = pd.DataFrame(dummy_data)\n",
        "\n",
        "    print(\"Using dummy datasets (replace with actual data loading when available)\")\n",
        "else:\n",
        "    # Load actual datasets, choose resized if requested\n",
        "    print(\"Loading datasets...\")\n",
        "    use_resized = False\n",
        "    if 'USE_RESIZED_SPLITS' in globals() and USE_RESIZED_SPLITS:\n",
        "        candidate_train = PROJECT_ROOT / 'data/processed/EmoSet_resized_224/splits/train_224.csv'\n",
        "        candidate_val = PROJECT_ROOT / 'data/processed/EmoSet_resized_224/splits/val_224.csv'\n",
        "        candidate_test = PROJECT_ROOT / 'data/processed/EmoSet_resized_224/splits/test_224.csv'\n",
        "        if candidate_train.exists():\n",
        "            print(\"✓ Using resized 224×224 split CSVs\")\n",
        "            CSV_TRAIN_ACTIVE = candidate_train\n",
        "            CSV_VAL_ACTIVE = candidate_val if candidate_val.exists() else CSV_VAL\n",
        "            CSV_TEST_ACTIVE = candidate_test if candidate_test.exists() else CSV_TEST\n",
        "            use_resized = True\n",
        "        else:\n",
        "            print(\"Resized splits not found; falling back to original CSVs\")\n",
        "            CSV_TRAIN_ACTIVE, CSV_VAL_ACTIVE, CSV_TEST_ACTIVE = CSV_TRAIN, CSV_VAL, CSV_TEST\n",
        "    else:\n",
        "        CSV_TRAIN_ACTIVE, CSV_VAL_ACTIVE, CSV_TEST_ACTIVE = CSV_TRAIN, CSV_VAL, CSV_TEST\n",
        "\n",
        "    train_df = pd.read_csv(CSV_TRAIN_ACTIVE)\n",
        "    val_df = pd.read_csv(CSV_VAL_ACTIVE) if CSV_VAL_ACTIVE.exists() else pd.read_csv(CSV_TRAIN_ACTIVE).sample(frac=0.2, random_state=42)\n",
        "    test_df = pd.read_csv(CSV_TEST_ACTIVE) if CSV_TEST_ACTIVE.exists() else pd.read_csv(CSV_TRAIN_ACTIVE).sample(frac=0.2, random_state=123)\n",
        "\n",
        "    if use_resized:\n",
        "        # Paths in resized CSVs are relative to project root already (we made them relative earlier)\n",
        "        DATA_DIR = PROJECT_ROOT  # so concatenation below works\n",
        "        print(\"DATA_DIR adjusted to project root for resized relative paths.\")\n",
        "\n",
        "print(f\"Dataset sizes:\")\n",
        "print(f\"- Training: {len(train_df)}\")\n",
        "print(f\"- Validation: {len(val_df)}\")\n",
        "print(f\"- Testing: {len(test_df)}\")\n",
        "\n",
        "# Create datasets (using RGB for transfer learning)\n",
        "train_dataset = EmotionDataset(train_df, DATA_DIR, transform=train_transform,\n",
        "                              label_map=label_map, rgb=True)\n",
        "val_dataset = EmotionDataset(val_df, DATA_DIR, transform=val_transform,\n",
        "                            label_map=label_map, rgb=True)\n",
        "test_dataset = EmotionDataset(test_df, DATA_DIR, transform=val_transform,\n",
        "                             label_map=label_map, rgb=True)\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"DataLoaders created with batch size: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7XGIvVA-qw"
      },
      "source": [
        "### Note on Resized Dataset Usage\n",
        "If `USE_RESIZED_SPLITS = True` and the resized CSVs were found, training now uses the 224×224 images stored under `data/processed/EmoSet_resized_224/images_224/`. Otherwise, it falls back to the original split CSVs. Adjust flags in the resizing section if you need to regenerate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsORwIjUlU3Z"
      },
      "source": [
        "## Step 6: CNN Transfer Learning Model\n",
        "\n",
        "Now we'll create our transfer learning model using a pre-trained VGG16 network.\n",
        "\n",
        "### Transfer Learning Strategies:\n",
        "1. **Feature Extraction**: Freeze pre-trained layers, only train classifier\n",
        "2. **Fine-tuning**: Train all layers with very small learning rate\n",
        "3. **Gradual unfreezing**: Start with frozen layers, gradually unfreeze\n",
        "\n",
        "We'll implement strategy #2 (fine-tuning) as it typically gives the best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTJwwTkelU3Z",
        "outputId": "fa7ce06f-29f8-4639-9e31-ba2f296d69f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Freeze] backbone PARTIAL: last block unfrozen.\n",
            "Model: backbone=vgg16 pretrained=True freeze_mode=partial feature_dim=512 classes=7\n",
            "Trainable params: 4,985,863\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "from torchvision.models import (\n",
        "    VGG16_Weights, VGG19_Weights, AlexNet_Weights,\n",
        "    ResNet18_Weights\n",
        ")\n",
        "\n",
        "class CNNTransferLearning(nn.Module):\n",
        "    \"\"\"\n",
        "    Transfer Learning head over a pretrained backbone.\n",
        "    - Supports: vgg16, vgg19, alexnet, resnet18\n",
        "    - Uses AdaptiveAvgPool2d((1,1)) so output dim = last conv channels (robust to input size)\n",
        "    - Freeze strategy: 'all' (features frozen), 'none' (full finetune), 'partial' (unfreeze last block only for VGG/ResNet)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes: int,\n",
        "                 backbone: str = 'vgg16',\n",
        "                 pretrained: bool = True,\n",
        "                 freeze_mode: str = 'none',   # 'all' | 'none' | 'partial'\n",
        "                 classifier_hidden: int = 512 # compact head since 1x1 pooling\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.backbone_name = backbone.lower()\n",
        "        self.num_classes = num_classes\n",
        "        self.freeze_mode = freeze_mode\n",
        "\n",
        "        # ---- Load backbone with new Weights API\n",
        "        if self.backbone_name == 'vgg16':\n",
        "            weights = VGG16_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            net = models.vgg16(weights=weights)\n",
        "            self.features = net.features                  # conv stack\n",
        "            last_channels = 512                           # VGG16 last conv planes\n",
        "        elif self.backbone_name == 'vgg19':\n",
        "            weights = VGG19_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            net = models.vgg19(weights=weights)\n",
        "            self.features = net.features\n",
        "            last_channels = 512\n",
        "        elif self.backbone_name == 'alexnet':\n",
        "            weights = AlexNet_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            net = models.alexnet(weights=weights)\n",
        "            self.features = net.features\n",
        "            last_channels = 256                           # AlexNet last conv planes\n",
        "        elif self.backbone_name == 'resnet18':\n",
        "            weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            net = models.resnet18(weights=weights)\n",
        "            # build a conv feature extractor (everything except avgpool & fc)\n",
        "            self.features = nn.Sequential(\n",
        "                net.conv1, net.bn1, net.relu, net.maxpool,\n",
        "                net.layer1, net.layer2, net.layer3, net.layer4\n",
        "            )\n",
        "            last_channels = 512\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        # ---- Robust pooling to handle arbitrary input sizes\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))   # -> [B, C, 1, 1]\n",
        "        feature_dim = last_channels                    # flatten -> [B, C]\n",
        "\n",
        "        # ---- Freeze strategy\n",
        "        if freeze_mode not in ('all', 'none', 'partial'):\n",
        "            raise ValueError(\"freeze_mode must be 'all' | 'none' | 'partial'\")\n",
        "\n",
        "        if freeze_mode == 'all':\n",
        "            for p in self.features.parameters():\n",
        "                p.requires_grad = False\n",
        "            print(f\"[Freeze] backbone ALL layers frozen.\")\n",
        "        elif freeze_mode == 'partial':\n",
        "            # Freeze all first\n",
        "            for p in self.features.parameters():\n",
        "                p.requires_grad = False\n",
        "            # Unfreeze the last block only (heuristics per backbone)\n",
        "            self._unfreeze_last_block()\n",
        "            print(f\"[Freeze] backbone PARTIAL: last block unfrozen.\")\n",
        "        else:\n",
        "            print(f\"[Freeze] backbone NONE: full finetuning.\")\n",
        "\n",
        "        # ---- Compact classifier (since feature_dim is small)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(feature_dim, classifier_hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(classifier_hidden, num_classes)\n",
        "        )\n",
        "        self._initialize_classifier()\n",
        "\n",
        "        print(f\"Model: backbone={self.backbone_name} pretrained={pretrained} \"\n",
        "              f\"freeze_mode={freeze_mode} feature_dim={feature_dim} classes={num_classes}\")\n",
        "\n",
        "    # === helpers ===\n",
        "    def _initialize_classifier(self):\n",
        "        for m in self.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _unfreeze_last_block(self):\n",
        "        \"\"\"\n",
        "        Unfreezes only the last stage of the backbone (good for small datasets).\n",
        "        Heuristics per-architecture to keep code simple.\n",
        "        \"\"\"\n",
        "        if self.backbone_name in ('vgg16', 'vgg19', 'alexnet'):\n",
        "            # features is a Sequential of conv/pool layers; unfreeze last ~5 layers\n",
        "            # (VGG block5 roughly ~ last 5 layers with conv+relu)\n",
        "            k = 5\n",
        "            for m in list(self.features.children())[-k:]:\n",
        "                for p in m.parameters():\n",
        "                    p.requires_grad = True\n",
        "        elif self.backbone_name == 'resnet18':\n",
        "            # Unfreeze only layer4 (last residual stage)\n",
        "            for m in self.features[-1].modules():  # self.features[-1] == layer4\n",
        "                for p in m.parameters():\n",
        "                    p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)        # [B,C,H,W]\n",
        "        x = self.avgpool(x)         # [B,C,1,1]\n",
        "        x = torch.flatten(x, 1)     # [B,C]\n",
        "        return self.classifier(x)\n",
        "\n",
        "    # Optional: parameter groups for different LRs\n",
        "    def param_groups(self):\n",
        "        backbone_params = [p for p in self.features.parameters() if p.requires_grad]\n",
        "        head_params     = list(self.classifier.parameters())\n",
        "        return {'backbone': backbone_params, 'head': head_params}\n",
        "\n",
        "    def get_num_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = 7\n",
        "\n",
        "model = CNNTransferLearning(\n",
        "    num_classes=num_classes,\n",
        "    backbone='vgg16',          # 'vgg16' | 'vgg19' | 'alexnet' | 'resnet18'\n",
        "    pretrained=True,\n",
        "    freeze_mode='partial',      # 'all' (smallest data) | 'partial' (small data) | 'none' (big data)\n",
        "    classifier_hidden=512\n",
        ").to(device)\n",
        "\n",
        "print(f\"Trainable params: {model.get_num_params():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Ij7fvJlU3a"
      },
      "source": [
        "## Step 7: Training Setup\n",
        "\n",
        "For transfer learning, we need to use different learning rates:\n",
        "- Smaller learning rate for pre-trained layers (if unfrozen)\n",
        "- Regular learning rate for new classifier layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrKWFy2YlU3a",
        "outputId": "aa8b57d1-bddc-43cc-c635-d90d9cac26cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3270649273.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n"
          ]
        }
      ],
      "source": [
        "# Loss function\n",
        "# Assume: model, train_loader, val_loader already created\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# (Optional) class weights for imbalance: tensor of shape [num_classes]\n",
        "# class_weights = torch.tensor([...], dtype=torch.float, device=device)\n",
        "# criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "criterion = nn.CrossEntropyLoss()  # default\n",
        "\n",
        "# Two-LR setup (backbone small, head larger)\n",
        "pg = model.param_groups()  # from your model class\n",
        "optimizer = torch.optim.Adam(\n",
        "    [\n",
        "        {'params': pg['backbone'], 'lr': 1e-5},\n",
        "        {'params': pg['head'],     'lr': 1e-3},\n",
        "    ],\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Epoch-level scheduler (step each epoch, not each batch)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Mixed precision scaler (only if CUDA)\n",
        "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAq30I9alU3a"
      },
      "source": [
        "## Step 8: Training Functions\n",
        "\n",
        "Let's create training and validation functions with proper progress tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "kQ6lFvc5lU3a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    train_loader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    scaler: Optional[torch.cuda.amp.GradScaler] = None,\n",
        "    progress_every: int = 100,\n",
        "    max_grad_norm: Optional[float] = None,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Train the model for one epoch with mixed precision, optional grad clipping, and progress logging.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    use_amp = (scaler is not None)\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if use_amp:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "            scaler.scale(loss).backward()\n",
        "            if max_grad_norm is not None:\n",
        "                # Unscale first, then clip\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            if max_grad_norm is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += float(loss.detach())\n",
        "        pred = output.argmax(dim=1)\n",
        "        total += target.size(0)\n",
        "        correct += (pred == target).sum().item()\n",
        "\n",
        "        if progress_every and (batch_idx % progress_every == 0) and (batch_idx > 0):\n",
        "            print(f'    [Epoch {epoch}] Batch {batch_idx}/{len(train_loader)} '\n",
        "                  f'Loss={running_loss/(batch_idx+1):.4f} '\n",
        "                  f'Acc={100.0*correct/total:.2f}%')\n",
        "\n",
        "    epoch_loss = running_loss / max(1, len(train_loader))\n",
        "    epoch_acc  = 100.0 * correct / max(1, total)\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate_epoch(\n",
        "    model: nn.Module,\n",
        "    val_loader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        "    use_amp: bool = True\n",
        ") -> Tuple[float, float, List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Validate the model with no grad. Uses autocast by default for speed.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions: List[int] = []\n",
        "    all_targets: List[int] = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        amp_ctx = torch.cuda.amp.autocast() if (use_amp and device.type == 'cuda') else torch.cuda.amp.autocast(enabled=False)\n",
        "        with amp_ctx:\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                running_loss += float(loss)\n",
        "                pred = output.argmax(dim=1)\n",
        "                total += target.size(0)\n",
        "                correct += (pred == target).sum().item()\n",
        "\n",
        "                all_predictions.extend(pred.detach().cpu().tolist())\n",
        "                all_targets.extend(target.detach().cpu().tolist())\n",
        "\n",
        "    epoch_loss = running_loss / max(1, len(val_loader))\n",
        "    epoch_acc  = 100.0 * correct / max(1, total)\n",
        "    return epoch_loss, epoch_acc, all_predictions, all_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvLiBVvTlU3a"
      },
      "source": [
        "## Step 9: Training Loop with Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi7fx8aslU3a",
        "outputId": "9d27f0bb-cfac-49f9-b998-c7c9df2e456c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training function defined. Ready to start training!\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                device, num_epochs=30, patience=5, save_path='best_cnn_transfer_model.pth'):\n",
        "    \"\"\"\n",
        "    Train the model with early stopping.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"STARTING TRAINING - CNN TRANSFER LEARNING\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Epochs: {num_epochs}, Patience: {patience}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print()\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Training phase\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
        "\n",
        "        # Validation phase\n",
        "        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e} (backbone), {optimizer.param_groups[1]['lr']:.2e} (classifier)\")\n",
        "\n",
        "        # Check for improvement\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'train_accuracies': train_accuracies,\n",
        "                'val_accuracies': val_accuracies\n",
        "            }, save_path)\n",
        "            print(f\"✓ New best model saved! Val Acc: {best_val_acc:.2f}%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_accuracies': val_accuracies,\n",
        "        'best_val_acc': best_val_acc\n",
        "    }\n",
        "\n",
        "print(\"Training function defined. Ready to start training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6y_axPdlU3b"
      },
      "source": [
        "## Step 10: Start Training\n",
        "\n",
        "Note: If running with dummy data, this will not produce meaningful results. Replace with actual data for real training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ9AWK__lU3b",
        "outputId": "b3d268e5-9722-4f4c-fbbb-7c94c8e78519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with actual data...\n",
            "\n",
            "============================================================\n",
            "STARTING TRAINING - CNN TRANSFER LEARNING\n",
            "============================================================\n",
            "Epochs: 50, Patience: 5\n",
            "Device: cuda\n",
            "\n",
            "Epoch 1/50\n",
            "----------------------------------------\n",
            "    [Epoch 0] Batch 100/630 Loss=2.3135 Acc=26.36%\n",
            "    [Epoch 0] Batch 200/630 Loss=2.0298 Acc=27.05%\n",
            "    [Epoch 0] Batch 300/630 Loss=1.8981 Acc=27.56%\n",
            "    [Epoch 0] Batch 400/630 Loss=1.8236 Acc=28.04%\n",
            "    [Epoch 0] Batch 500/630 Loss=1.7744 Acc=28.75%\n",
            "    [Epoch 0] Batch 600/630 Loss=1.7351 Acc=29.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1831838612.py:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  amp_ctx = torch.cuda.amp.autocast() if (use_amp and device.type == 'cuda') else torch.cuda.amp.autocast(enabled=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7256, Train Acc: 29.46%\n",
            "Val Loss: 1.4577, Val Acc: 35.56%\n",
            "Current LR: 1.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "✓ New best model saved! Val Acc: 35.56%\n",
            "\n",
            "Epoch 2/50\n",
            "----------------------------------------\n",
            "    [Epoch 1] Batch 100/630 Loss=1.4937 Acc=34.22%\n",
            "    [Epoch 1] Batch 200/630 Loss=1.4942 Acc=34.70%\n",
            "    [Epoch 1] Batch 300/630 Loss=1.5000 Acc=34.49%\n",
            "    [Epoch 1] Batch 400/630 Loss=1.4997 Acc=34.55%\n",
            "    [Epoch 1] Batch 500/630 Loss=1.4919 Acc=34.91%\n",
            "    [Epoch 1] Batch 600/630 Loss=1.4898 Acc=35.01%\n",
            "Train Loss: 1.4882, Train Acc: 35.17%\n",
            "Val Loss: 1.4127, Val Acc: 40.69%\n",
            "Current LR: 1.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "✓ New best model saved! Val Acc: 40.69%\n",
            "\n",
            "Epoch 3/50\n",
            "----------------------------------------\n",
            "    [Epoch 2] Batch 100/630 Loss=1.4478 Acc=38.43%\n",
            "    [Epoch 2] Batch 200/630 Loss=1.4571 Acc=37.45%\n",
            "    [Epoch 2] Batch 300/630 Loss=1.4492 Acc=37.62%\n",
            "    [Epoch 2] Batch 400/630 Loss=1.4445 Acc=37.45%\n",
            "    [Epoch 2] Batch 500/630 Loss=1.4439 Acc=37.49%\n",
            "    [Epoch 2] Batch 600/630 Loss=1.4408 Acc=37.45%\n",
            "Train Loss: 1.4422, Train Acc: 37.39%\n",
            "Val Loss: 1.3827, Val Acc: 43.30%\n",
            "Current LR: 1.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "✓ New best model saved! Val Acc: 43.30%\n",
            "\n",
            "Epoch 4/50\n",
            "----------------------------------------\n",
            "    [Epoch 3] Batch 100/630 Loss=1.4233 Acc=38.68%\n",
            "    [Epoch 3] Batch 200/630 Loss=1.4241 Acc=38.15%\n",
            "    [Epoch 3] Batch 300/630 Loss=1.4242 Acc=38.10%\n",
            "    [Epoch 3] Batch 400/630 Loss=1.4198 Acc=38.47%\n",
            "    [Epoch 3] Batch 500/630 Loss=1.4159 Acc=38.85%\n",
            "    [Epoch 3] Batch 600/630 Loss=1.4152 Acc=38.91%\n",
            "Train Loss: 1.4167, Train Acc: 38.90%\n",
            "Val Loss: 1.3833, Val Acc: 42.85%\n",
            "Current LR: 1.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "No improvement. Patience: 1/5\n",
            "\n",
            "Epoch 5/50\n",
            "----------------------------------------\n",
            "    [Epoch 4] Batch 100/630 Loss=1.4153 Acc=39.36%\n",
            "    [Epoch 4] Batch 200/630 Loss=1.4140 Acc=39.37%\n",
            "    [Epoch 4] Batch 300/630 Loss=1.4092 Acc=39.88%\n",
            "    [Epoch 4] Batch 400/630 Loss=1.3979 Acc=40.51%\n",
            "    [Epoch 4] Batch 500/630 Loss=1.3945 Acc=40.52%\n",
            "    [Epoch 4] Batch 600/630 Loss=1.3905 Acc=40.62%\n",
            "Train Loss: 1.3899, Train Acc: 40.68%\n",
            "Val Loss: 1.3479, Val Acc: 44.39%\n",
            "Current LR: 1.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "✓ New best model saved! Val Acc: 44.39%\n",
            "\n",
            "Epoch 6/50\n",
            "----------------------------------------\n",
            "    [Epoch 5] Batch 100/630 Loss=1.3613 Acc=42.98%\n",
            "    [Epoch 5] Batch 200/630 Loss=1.3676 Acc=42.46%\n",
            "    [Epoch 5] Batch 300/630 Loss=1.3737 Acc=41.87%\n",
            "    [Epoch 5] Batch 400/630 Loss=1.3743 Acc=42.12%\n",
            "    [Epoch 5] Batch 500/630 Loss=1.3748 Acc=41.90%\n",
            "    [Epoch 5] Batch 600/630 Loss=1.3716 Acc=42.07%\n",
            "Train Loss: 1.3713, Train Acc: 41.98%\n",
            "Val Loss: 1.3235, Val Acc: 46.18%\n",
            "Current LR: 1.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "✓ New best model saved! Val Acc: 46.18%\n",
            "\n",
            "Epoch 7/50\n",
            "----------------------------------------\n",
            "    [Epoch 6] Batch 100/630 Loss=1.3607 Acc=42.79%\n",
            "    [Epoch 6] Batch 200/630 Loss=1.3651 Acc=42.62%\n",
            "    [Epoch 6] Batch 300/630 Loss=1.3583 Acc=42.97%\n",
            "    [Epoch 6] Batch 400/630 Loss=1.3583 Acc=42.87%\n",
            "    [Epoch 6] Batch 500/630 Loss=1.3590 Acc=42.76%\n",
            "    [Epoch 6] Batch 600/630 Loss=1.3572 Acc=42.88%\n",
            "Train Loss: 1.3571, Train Acc: 42.92%\n",
            "Val Loss: 1.3144, Val Acc: 45.93%\n",
            "Current LR: 1.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "No improvement. Patience: 1/5\n",
            "\n",
            "Epoch 8/50\n",
            "----------------------------------------\n",
            "    [Epoch 7] Batch 100/630 Loss=1.3367 Acc=44.55%\n",
            "    [Epoch 7] Batch 200/630 Loss=1.3333 Acc=44.22%\n",
            "    [Epoch 7] Batch 300/630 Loss=1.3392 Acc=43.42%\n",
            "    [Epoch 7] Batch 400/630 Loss=1.3404 Acc=43.24%\n",
            "    [Epoch 7] Batch 500/630 Loss=1.3395 Acc=43.24%\n",
            "    [Epoch 7] Batch 600/630 Loss=1.3372 Acc=43.46%\n",
            "Train Loss: 1.3354, Train Acc: 43.56%\n",
            "Val Loss: 1.2966, Val Acc: 47.02%\n",
            "Current LR: 1.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "✓ New best model saved! Val Acc: 47.02%\n",
            "\n",
            "Epoch 9/50\n",
            "----------------------------------------\n",
            "    [Epoch 8] Batch 100/630 Loss=1.3166 Acc=45.88%\n",
            "    [Epoch 8] Batch 200/630 Loss=1.3177 Acc=45.23%\n",
            "    [Epoch 8] Batch 300/630 Loss=1.3273 Acc=45.01%\n",
            "    [Epoch 8] Batch 400/630 Loss=1.3314 Acc=44.62%\n",
            "    [Epoch 8] Batch 500/630 Loss=1.3291 Acc=44.85%\n",
            "    [Epoch 8] Batch 600/630 Loss=1.3266 Acc=44.87%\n",
            "Train Loss: 1.3261, Train Acc: 44.75%\n",
            "Val Loss: 1.2925, Val Acc: 47.94%\n",
            "Current LR: 1.00e-05 (backbone), 1.00e-03 (classifier)\n",
            "✓ New best model saved! Val Acc: 47.94%\n",
            "\n",
            "Epoch 10/50\n",
            "----------------------------------------\n",
            "    [Epoch 9] Batch 100/630 Loss=1.2964 Acc=45.82%\n",
            "    [Epoch 9] Batch 200/630 Loss=1.3162 Acc=45.21%\n",
            "    [Epoch 9] Batch 300/630 Loss=1.3167 Acc=45.02%\n",
            "    [Epoch 9] Batch 400/630 Loss=1.3164 Acc=44.76%\n",
            "    [Epoch 9] Batch 500/630 Loss=1.3096 Acc=45.35%\n",
            "    [Epoch 9] Batch 600/630 Loss=1.3108 Acc=45.24%\n",
            "Train Loss: 1.3107, Train Acc: 45.31%\n",
            "Val Loss: 1.2840, Val Acc: 48.83%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "✓ New best model saved! Val Acc: 48.83%\n",
            "\n",
            "Epoch 11/50\n",
            "----------------------------------------\n",
            "    [Epoch 10] Batch 100/630 Loss=1.2939 Acc=46.78%\n",
            "    [Epoch 10] Batch 200/630 Loss=1.2957 Acc=46.24%\n",
            "    [Epoch 10] Batch 300/630 Loss=1.2985 Acc=45.77%\n",
            "    [Epoch 10] Batch 400/630 Loss=1.2945 Acc=46.10%\n",
            "    [Epoch 10] Batch 500/630 Loss=1.2954 Acc=46.00%\n",
            "    [Epoch 10] Batch 600/630 Loss=1.2969 Acc=45.98%\n",
            "Train Loss: 1.2979, Train Acc: 45.98%\n",
            "Val Loss: 1.2783, Val Acc: 49.35%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "✓ New best model saved! Val Acc: 49.35%\n",
            "\n",
            "Epoch 12/50\n",
            "----------------------------------------\n",
            "    [Epoch 11] Batch 100/630 Loss=1.2850 Acc=46.60%\n",
            "    [Epoch 11] Batch 200/630 Loss=1.2829 Acc=47.12%\n",
            "    [Epoch 11] Batch 300/630 Loss=1.2782 Acc=46.92%\n",
            "    [Epoch 11] Batch 400/630 Loss=1.2797 Acc=46.55%\n",
            "    [Epoch 11] Batch 500/630 Loss=1.2764 Acc=46.64%\n",
            "    [Epoch 11] Batch 600/630 Loss=1.2787 Acc=46.57%\n",
            "Train Loss: 1.2790, Train Acc: 46.58%\n",
            "Val Loss: 1.2612, Val Acc: 49.98%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "✓ New best model saved! Val Acc: 49.98%\n",
            "\n",
            "Epoch 13/50\n",
            "----------------------------------------\n",
            "    [Epoch 12] Batch 100/630 Loss=1.2917 Acc=47.34%\n",
            "    [Epoch 12] Batch 200/630 Loss=1.2830 Acc=47.09%\n",
            "    [Epoch 12] Batch 300/630 Loss=1.2830 Acc=46.67%\n",
            "    [Epoch 12] Batch 400/630 Loss=1.2761 Acc=47.21%\n",
            "    [Epoch 12] Batch 500/630 Loss=1.2771 Acc=47.06%\n",
            "    [Epoch 12] Batch 600/630 Loss=1.2749 Acc=47.10%\n",
            "Train Loss: 1.2739, Train Acc: 47.12%\n",
            "Val Loss: 1.2484, Val Acc: 50.45%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "✓ New best model saved! Val Acc: 50.45%\n",
            "\n",
            "Epoch 14/50\n",
            "----------------------------------------\n",
            "    [Epoch 13] Batch 100/630 Loss=1.2714 Acc=47.09%\n",
            "    [Epoch 13] Batch 200/630 Loss=1.2702 Acc=46.97%\n",
            "    [Epoch 13] Batch 300/630 Loss=1.2592 Acc=47.82%\n",
            "    [Epoch 13] Batch 400/630 Loss=1.2670 Acc=47.51%\n",
            "    [Epoch 13] Batch 500/630 Loss=1.2670 Acc=47.43%\n",
            "    [Epoch 13] Batch 600/630 Loss=1.2668 Acc=47.35%\n",
            "Train Loss: 1.2683, Train Acc: 47.30%\n",
            "Val Loss: 1.2717, Val Acc: 49.83%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "No improvement. Patience: 1/5\n",
            "\n",
            "Epoch 15/50\n",
            "----------------------------------------\n",
            "    [Epoch 14] Batch 100/630 Loss=1.2430 Acc=49.26%\n",
            "    [Epoch 14] Batch 200/630 Loss=1.2492 Acc=48.62%\n",
            "    [Epoch 14] Batch 300/630 Loss=1.2531 Acc=48.70%\n",
            "    [Epoch 14] Batch 400/630 Loss=1.2585 Acc=48.27%\n",
            "    [Epoch 14] Batch 500/630 Loss=1.2593 Acc=48.07%\n",
            "    [Epoch 14] Batch 600/630 Loss=1.2583 Acc=47.95%\n",
            "Train Loss: 1.2606, Train Acc: 47.88%\n",
            "Val Loss: 1.2510, Val Acc: 50.22%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "No improvement. Patience: 2/5\n",
            "\n",
            "Epoch 16/50\n",
            "----------------------------------------\n",
            "    [Epoch 15] Batch 100/630 Loss=1.2339 Acc=47.80%\n",
            "    [Epoch 15] Batch 200/630 Loss=1.2422 Acc=48.45%\n",
            "    [Epoch 15] Batch 300/630 Loss=1.2419 Acc=48.60%\n",
            "    [Epoch 15] Batch 400/630 Loss=1.2462 Acc=48.53%\n",
            "    [Epoch 15] Batch 500/630 Loss=1.2498 Acc=48.48%\n",
            "    [Epoch 15] Batch 600/630 Loss=1.2518 Acc=48.27%\n",
            "Train Loss: 1.2541, Train Acc: 48.20%\n",
            "Val Loss: 1.2474, Val Acc: 50.42%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "No improvement. Patience: 3/5\n",
            "\n",
            "Epoch 17/50\n",
            "----------------------------------------\n",
            "    [Epoch 16] Batch 100/630 Loss=1.2550 Acc=47.80%\n",
            "    [Epoch 16] Batch 200/630 Loss=1.2559 Acc=47.95%\n",
            "    [Epoch 16] Batch 300/630 Loss=1.2593 Acc=47.65%\n",
            "    [Epoch 16] Batch 400/630 Loss=1.2568 Acc=47.69%\n",
            "    [Epoch 16] Batch 500/630 Loss=1.2530 Acc=47.83%\n",
            "    [Epoch 16] Batch 600/630 Loss=1.2526 Acc=47.82%\n",
            "Train Loss: 1.2536, Train Acc: 47.86%\n",
            "Val Loss: 1.2349, Val Acc: 51.24%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "✓ New best model saved! Val Acc: 51.24%\n",
            "\n",
            "Epoch 18/50\n",
            "----------------------------------------\n",
            "    [Epoch 17] Batch 100/630 Loss=1.2361 Acc=48.64%\n",
            "    [Epoch 17] Batch 200/630 Loss=1.2438 Acc=48.96%\n",
            "    [Epoch 17] Batch 300/630 Loss=1.2414 Acc=48.94%\n",
            "    [Epoch 17] Batch 400/630 Loss=1.2429 Acc=49.20%\n",
            "    [Epoch 17] Batch 500/630 Loss=1.2475 Acc=48.85%\n",
            "    [Epoch 17] Batch 600/630 Loss=1.2481 Acc=48.79%\n",
            "Train Loss: 1.2481, Train Acc: 48.74%\n",
            "Val Loss: 1.2316, Val Acc: 51.34%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "✓ New best model saved! Val Acc: 51.34%\n",
            "\n",
            "Epoch 19/50\n",
            "----------------------------------------\n",
            "    [Epoch 18] Batch 100/630 Loss=1.2399 Acc=49.41%\n",
            "    [Epoch 18] Batch 200/630 Loss=1.2410 Acc=49.27%\n",
            "    [Epoch 18] Batch 300/630 Loss=1.2516 Acc=48.86%\n",
            "    [Epoch 18] Batch 400/630 Loss=1.2471 Acc=48.78%\n",
            "    [Epoch 18] Batch 500/630 Loss=1.2465 Acc=48.65%\n",
            "    [Epoch 18] Batch 600/630 Loss=1.2405 Acc=48.88%\n",
            "Train Loss: 1.2411, Train Acc: 48.83%\n",
            "Val Loss: 1.2226, Val Acc: 50.40%\n",
            "Current LR: 5.00e-06 (backbone), 5.00e-04 (classifier)\n",
            "No improvement. Patience: 1/5\n",
            "\n",
            "Epoch 20/50\n",
            "----------------------------------------\n",
            "    [Epoch 19] Batch 100/630 Loss=1.2366 Acc=49.47%\n",
            "    [Epoch 19] Batch 200/630 Loss=1.2420 Acc=49.11%\n",
            "    [Epoch 19] Batch 300/630 Loss=1.2385 Acc=48.84%\n",
            "    [Epoch 19] Batch 400/630 Loss=1.2349 Acc=49.08%\n",
            "    [Epoch 19] Batch 500/630 Loss=1.2385 Acc=48.83%\n",
            "    [Epoch 19] Batch 600/630 Loss=1.2354 Acc=49.03%\n",
            "Train Loss: 1.2348, Train Acc: 49.08%\n",
            "Val Loss: 1.2267, Val Acc: 50.22%\n",
            "Current LR: 2.50e-06 (backbone), 2.50e-04 (classifier)\n",
            "No improvement. Patience: 2/5\n",
            "\n",
            "Epoch 21/50\n",
            "----------------------------------------\n",
            "    [Epoch 20] Batch 100/630 Loss=1.2272 Acc=50.09%\n",
            "    [Epoch 20] Batch 200/630 Loss=1.2302 Acc=49.41%\n",
            "    [Epoch 20] Batch 300/630 Loss=1.2226 Acc=49.48%\n"
          ]
        }
      ],
      "source": [
        "# Check if we have actual data or dummy data\n",
        "if DATA_DIR.exists() and any(DATA_DIR.iterdir()):\n",
        "    print(\"Starting training with actual data...\")\n",
        "    EPOCHS = 50\n",
        "else:\n",
        "    print(\"Using dummy data - training for demonstration only...\")\n",
        "    EPOCHS = 3  # Shorter training for demo\n",
        "\n",
        "# Start training\n",
        "training_history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    num_epochs=EPOCHS,\n",
        "    patience=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D4lQdfhlU3b"
      },
      "source": [
        "## Step 11: Plot Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlGv9al2lU3b"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    ax1.plot(history['train_losses'], label='Training Loss', marker='o')\n",
        "    ax1.plot(history['val_losses'], label='Validation Loss', marker='s')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracies\n",
        "    ax2.plot(history['train_accuracies'], label='Training Accuracy', marker='o')\n",
        "    ax2.plot(history['val_accuracies'], label='Validation Accuracy', marker='s')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Training Summary:\")\n",
        "    print(f\"- Final Training Loss: {history['train_losses'][-1]:.4f}\")\n",
        "    print(f\"- Final Validation Loss: {history['val_losses'][-1]:.4f}\")\n",
        "    print(f\"- Final Training Accuracy: {history['train_accuracies'][-1]:.2f}%\")\n",
        "    print(f\"- Final Validation Accuracy: {history['val_accuracies'][-1]:.2f}%\")\n",
        "    print(f\"- Best Validation Accuracy: {history['best_val_acc']:.2f}%\")\n",
        "\n",
        "# Plot the training history\n",
        "plot_training_history(training_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwbSC9lzlU3b"
      },
      "source": [
        "## Step 12: Load Best Model and Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaG8dwKXlU3b"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "if Path('best_cnn_transfer_model.pth').exists():\n",
        "    checkpoint = torch.load('best_cnn_transfer_model.pth', map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model from epoch {checkpoint['epoch']} with val acc: {checkpoint['best_val_acc']:.2f}%\")\n",
        "else:\n",
        "    print(\"No saved model found, using current model state\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_loss, test_acc, test_predictions, test_targets = validate_epoch(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Results:\")\n",
        "print(f\"- Test Loss: {test_loss:.4f}\")\n",
        "print(f\"- Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "# Detailed classification report\n",
        "if len(set(test_targets)) > 1:  # Only if we have multiple classes\n",
        "    print(\"\\nClassification Report:\")\n",
        "    emotion_names = list(label_map.keys())\n",
        "    report = classification_report(test_targets, test_predictions,\n",
        "                                 target_names=emotion_names,\n",
        "                                 zero_division=0)\n",
        "    print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBjcRVXwlU3b"
      },
      "source": [
        "## Step 13: Visualize Results - Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJknCL2OlU3b"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, class_names, title='Confusion Matrix'):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Emotion')\n",
        "    plt.ylabel('True Emotion')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print accuracy per class\n",
        "    print(\"\\nPer-class Accuracy:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_correct = cm[i, i]\n",
        "        class_total = cm[i, :].sum()\n",
        "        if class_total > 0:\n",
        "            acc = 100 * class_correct / class_total\n",
        "            print(f\"- {class_name}: {acc:.2f}% ({class_correct}/{class_total})\")\n",
        "\n",
        "# Plot confusion matrix if we have predictions\n",
        "if len(set(test_targets)) > 1:\n",
        "    emotion_names = list(label_map.keys())\n",
        "    plot_confusion_matrix(test_targets, test_predictions, emotion_names,\n",
        "                         'CNN Transfer Learning - Test Set Confusion Matrix')\n",
        "else:\n",
        "    print(\"Skipping confusion matrix (insufficient data/classes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU98Le33lU3b"
      },
      "source": [
        "## Step 14: Model Comparison and Analysis\n",
        "\n",
        "Let's compare our transfer learning CNN with the original baseline CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og55HZRjlU3b"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL ANALYSIS AND COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Model parameters\n",
        "print(f\"\\n1. MODEL ARCHITECTURE:\")\n",
        "print(f\"   - Transfer Learning CNN (VGG16 backbone)\")\n",
        "print(f\"   - Total parameters: {model.get_num_params():,}\")\n",
        "print(f\"   - Input size: {INPUT_SIZE}x{INPUT_SIZE}x3 (RGB)\")\n",
        "print(f\"   - Output classes: {num_classes}\")\n",
        "\n",
        "# Performance summary\n",
        "print(f\"\\n2. PERFORMANCE SUMMARY:\")\n",
        "if 'training_history' in locals():\n",
        "    print(f\"   - Best Validation Accuracy: {training_history['best_val_acc']:.2f}%\")\n",
        "    print(f\"   - Final Test Accuracy: {test_acc:.2f}%\")\n",
        "    print(f\"   - Training Epochs: {len(training_history['train_losses'])}\")\n",
        "\n",
        "# Transfer Learning Benefits\n",
        "print(f\"\\n3. TRANSFER LEARNING BENEFITS:\")\n",
        "print(f\"   ✓ Pre-trained features: Learned from ImageNet (1.2M images)\")\n",
        "print(f\"   ✓ Faster convergence: Starts with meaningful weights\")\n",
        "print(f\"   ✓ Better generalization: Robust low-level feature extraction\")\n",
        "print(f\"   ✓ Less overfitting: Pre-trained features are well-regularized\")\n",
        "\n",
        "print(f\"\\n4. KEY DIFFERENCES FROM BASELINE CNN:\")\n",
        "print(f\"   - Uses pre-trained VGG16 backbone vs. random initialization\")\n",
        "print(f\"   - RGB input (224x224) vs. Grayscale (48x48)\")\n",
        "print(f\"   - ImageNet normalization vs. simple normalization\")\n",
        "print(f\"   - Transfer learning strategy vs. training from scratch\")\n",
        "print(f\"   - Different learning rates for backbone vs. classifier\")\n",
        "\n",
        "print(f\"\\n5. TRAINING STRATEGY USED:\")\n",
        "print(f\"   - Fine-tuning: All layers trainable\")\n",
        "print(f\"   - Backbone LR: {backbone_lr} (very small)\")\n",
        "print(f\"   - Classifier LR: {classifier_lr} (regular)\")\n",
        "print(f\"   - Data augmentation: Rotation, flip, color jitter, affine\")\n",
        "print(f\"   - Early stopping with patience={5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWY7HvpSlU3b"
      },
      "source": [
        "## Step 15: Save Final Model for Production\n",
        "\n",
        "Let's save our model in a format that can be easily loaded for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXGjsqdhlU3b"
      },
      "outputs": [],
      "source": [
        "# Save complete model information\n",
        "final_model_info = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_config': {\n",
        "        'num_classes': num_classes,\n",
        "        'backbone': 'vgg16',\n",
        "        'input_size': INPUT_SIZE,\n",
        "        'pretrained': True,\n",
        "        'freeze_backbone': False\n",
        "    },\n",
        "    'label_map': label_map,\n",
        "    'transforms': {\n",
        "        'mean': IMAGENET_MEAN,\n",
        "        'std': IMAGENET_STD,\n",
        "        'input_size': INPUT_SIZE\n",
        "    },\n",
        "    'training_info': {\n",
        "        'final_test_acc': test_acc,\n",
        "        'backbone_lr': backbone_lr,\n",
        "        'classifier_lr': classifier_lr\n",
        "    }\n",
        "}\n",
        "\n",
        "torch.save(final_model_info, 'cnn_transfer_learning_final.pth')\n",
        "print(\"✓ Final model saved as 'cnn_transfer_learning_final.pth'\")\n",
        "\n",
        "# Create a simple inference function\n",
        "def create_inference_function():\n",
        "    \"\"\"\n",
        "    Create a simple inference function that can be used in production.\n",
        "    \"\"\"\n",
        "    inference_code = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "def load_emotion_model(model_path):\n",
        "    \"\"\"Load the trained emotion recognition model.\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "    # Recreate model architecture\n",
        "    model = CNNTransferLearning(\n",
        "        num_classes=checkpoint['model_config']['num_classes'],\n",
        "        backbone=checkpoint['model_config']['backbone'],\n",
        "        pretrained=False  # We're loading trained weights\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Create transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((checkpoint['transforms']['input_size'],\n",
        "                          checkpoint['transforms']['input_size'])),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=checkpoint['transforms']['mean'],\n",
        "                           std=checkpoint['transforms']['std'])\n",
        "    ])\n",
        "\n",
        "    return model, transform, checkpoint['label_map']\n",
        "\n",
        "def predict_emotion(model, transform, label_map, image_path):\n",
        "    \"\"\"Predict emotion from image.\"\"\"\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    # Convert to emotion name\n",
        "    emotion_names = {v: k for k, v in label_map.items()}\n",
        "    predicted_emotion = emotion_names[predicted_class]\n",
        "    confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "    return predicted_emotion, confidence, probabilities[0].numpy()\n",
        "'''\n",
        "\n",
        "    with open('emotion_inference.py', 'w') as f:\n",
        "        f.write(inference_code)\n",
        "\n",
        "    print(\"✓ Inference code saved as 'emotion_inference.py'\")\n",
        "\n",
        "create_inference_function()\n",
        "\n",
        "print(\"\\n✓ Model deployment ready!\")\n",
        "print(\"Files created:\")\n",
        "print(\"- cnn_transfer_learning_final.pth (complete model)\")\n",
        "print(\"- emotion_inference.py (inference functions)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqx43b9blU3c"
      },
      "source": [
        "## Summary: CNN Transfer Learning Implementation\n",
        "\n",
        "### What We've Accomplished:\n",
        "\n",
        "1. **Transfer Learning Setup**: Implemented CNN transfer learning using VGG16 backbone pre-trained on ImageNet\n",
        "2. **Optional Resizing Pipeline**: Added automated 48×48 → 224×224 upscaling with per-class folder structure and regenerated split CSVs (`train_224.csv`, `val_224.csv`, `test_224.csv`) plus stats file.\n",
        "3. **Architecture**:\n",
        "   - Pre-trained VGG16 feature extractor\n",
        "   - Custom classifier head for emotion recognition\n",
        "   - Support for RGB images (224×224)\n",
        "4. **Training Strategy**:\n",
        "   - Fine-tuning approach with different learning rates\n",
        "   - ImageNet normalization for compatibility\n",
        "   - Data augmentation for better generalization\n",
        "   - Early stopping to prevent overfitting\n",
        "5. **Key Benefits Over Baseline CNN**:\n",
        "   - ✅ **Faster Convergence**: Pre-trained weights provide good starting point\n",
        "   - ✅ **Better Feature Learning**: Robust low-level features from ImageNet\n",
        "   - ✅ **Improved Generalization**: Less prone to overfitting\n",
        "   - ✅ **State-of-the-art Architecture**: Proven CNN design\n",
        "6. **Resized Dataset Artifacts** (if generated):\n",
        "   - Images: `data/processed/EmoSet_resized_224/images_224/<class>/*.jpg`\n",
        "   - Splits: `data/processed/EmoSet_resized_224/splits/train_224.csv` (and val/test)\n",
        "   - Stats:  `data/processed/EmoSet_resized_224/stats_resized_224.json`\n",
        "   - Toggle usage via `USE_RESIZED_SPLITS = True` in the notebook.\n",
        "\n",
        "### Transfer Learning Strategies Implemented:\n",
        "1. **Feature Extraction** (freezing backbone)\n",
        "2. **Fine-tuning** (training all layers with different LRs)\n",
        "3. **Gradual unfreezing** (implemented as methods)\n",
        "\n",
        "### Production-Ready Features:\n",
        "- Complete model serialization\n",
        "- Inference functions\n",
        "- Proper preprocessing pipeline\n",
        "- Model configuration storage\n",
        "- Optional resized dataset workflow\n",
        "\n",
        "### Next Steps:\n",
        "1. **Experiment with different backbones** (VGG19, ResNet, EfficientNet)\n",
        "2. **Implement ensemble methods** combining multiple models\n",
        "3. **Add model interpretability** (Grad-CAM, attention maps)\n",
        "4. **Optimize for deployment** (model quantization, ONNX export)\n",
        "5. **Augment data** further with synthetic generation or advanced augmentation policies\n",
        "\n",
        "This implementation provides a solid foundation for visual emotion recognition using modern transfer learning techniques and a flexible resizing pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 16: Model Interpretability (Grad-CAM heatmaps)\n",
        "\n",
        "We’ll generate Grad-CAM heatmaps to understand which image regions drive the model’s predictions. This helps validate that the model focuses on relevant facial areas.\n",
        "\n",
        "What we’ll do:\n",
        "- Hook the last convolutional layer in the backbone\n",
        "- Compute class-specific gradients and weights\n",
        "- Produce heatmaps and overlay them on the original images"
      ],
      "metadata": {
        "id": "pWECNFA0NXqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 16 (run): Grad-CAM utilities and visualization helpers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.target_layer = target_layer\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "        self.hook_handles = []\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def fwd_hook(module, inp, out):\n",
        "            self.activations = out.detach()\n",
        "        def bwd_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0].detach()\n",
        "        self.hook_handles.append(self.target_layer.register_forward_hook(fwd_hook))\n",
        "        self.hook_handles.append(self.target_layer.register_backward_hook(bwd_hook))\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for h in self.hook_handles:\n",
        "            h.remove()\n",
        "\n",
        "    def __call__(self, input_tensor, class_idx=None):\n",
        "        input_tensor = input_tensor.requires_grad_(True)\n",
        "        logits = self.model(input_tensor)\n",
        "        if class_idx is None:\n",
        "            class_idx = logits.argmax(dim=1).item()\n",
        "        score = logits[:, class_idx]\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "\n",
        "        # activations: [B, C, H, W]; gradients: [B, C, H, W]\n",
        "        weights = self.gradients.mean(dim=(2,3), keepdim=True)  # [B, C, 1, 1]\n",
        "        cam = (weights * self.activations).sum(dim=1, keepdim=True)  # [B,1,H,W]\n",
        "        cam = F.relu(cam)\n",
        "        # Normalize to [0,1]\n",
        "        cam_min, cam_max = cam.min(), cam.max()\n",
        "        cam = (cam - cam_min) / (cam_max - cam_min + 1e-6)\n",
        "        return cam.squeeze(0).squeeze(0).cpu().numpy(), class_idx\n",
        "\n",
        "# Helper to overlay heatmap\n",
        "def overlay_cam_on_image(img_pil, cam, alpha=0.4):\n",
        "    img = np.array(img_pil.convert('RGB'))\n",
        "    h, w = img.shape[:2]\n",
        "    cam_resized = cv2.resize(cam, (w, h))\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255*cam_resized), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    overlay = np.uint8(alpha*heatmap + (1-alpha)*img)\n",
        "    return img, heatmap, overlay\n",
        "\n",
        "# Pick the last conv block for the chosen backbone (VGG16/AlexNet/ResNet18 supported)\n",
        "last_conv = None\n",
        "if hasattr(model, 'features'):\n",
        "    for m in reversed(model.features):\n",
        "        if isinstance(m, torch.nn.Conv2d):\n",
        "            last_conv = m\n",
        "            break\n",
        "else:\n",
        "    # ResNet18 path: find last Conv2d under features (layer4)\n",
        "    for m in reversed(list(model.features.modules())):\n",
        "        if isinstance(m, torch.nn.Conv2d):\n",
        "            last_conv = m\n",
        "            break\n",
        "assert last_conv is not None, \"Could not find a Conv2d layer for Grad-CAM\"\n",
        "\n",
        "cam_explainer = GradCAM(model, last_conv)\n",
        "\n",
        "# Visualize Grad-CAM on a few validation samples\n",
        "def show_gradcam_for_samples(df, root, transform, k=4):\n",
        "    samples = df.sample(n=min(k, len(df)), random_state=42)\n",
        "    plt.figure(figsize=(12, 3*k))\n",
        "    for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
        "        img_path = Path(root) / row[train_dataset.path_col]\n",
        "        img_pil = Image.open(img_path).convert('L')\n",
        "        inp = transform(img_pil)\n",
        "        if inp.shape[0]==1:\n",
        "            inp = inp.repeat(3,1,1)  # expand to 3 channels for CNN backbones\n",
        "        cam, pred_idx = cam_explainer(inp.unsqueeze(0).to(device))\n",
        "        img, heatmap, overlay = overlay_cam_on_image(img_pil, cam)\n",
        "        pred_name = list(label_map.keys())[pred_idx]\n",
        "\n",
        "        plt.subplot(k, 3, 3*(i-1)+1); plt.imshow(img); plt.axis('off'); plt.title('Input')\n",
        "        plt.subplot(k, 3, 3*(i-1)+2); plt.imshow(heatmap); plt.axis('off'); plt.title('Grad-CAM')\n",
        "        plt.subplot(k, 3, 3*(i-1)+3); plt.imshow(overlay); plt.axis('off'); plt.title(f'Overlay (pred={pred_name})')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "print(\"Grad-CAM ready. Run the next cell to visualize on validation samples.\")"
      ],
      "metadata": {
        "id": "yfL2hyc-NPKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Visualize Grad-CAM on a few validation images\n",
        "try:\n",
        "    show_gradcam_for_samples(val_df, DATA_DIR, val_transform, k=4)\n",
        "except Exception as e:\n",
        "    print('Grad-CAM visualization error:', e)"
      ],
      "metadata": {
        "id": "Xe-lgdMsNb6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 17: Evaluation Plan — Overview\n",
        "\n",
        "Primary metrics:\n",
        "- Macro-F1 (treats all classes equally)\n",
        "- Balanced Accuracy (mean recall across classes)\n",
        "\n",
        "Secondary metrics:\n",
        "- Per-class precision/recall/F1\n",
        "- ROC-AUC (macro and micro, OvR)\n",
        "\n",
        "Tools:\n",
        "- Confusion matrix and per-class accuracy\n",
        "- Error analysis (high-confidence mistakes)\n",
        "- Cross-validation (K-fold skeleton)"
      ],
      "metadata": {
        "id": "7HaWcdEtNg4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 18: Compute Metrics — Macro-F1, Balanced Accuracy, ROC-AUC, and Report\n",
        "from sklearn.metrics import f1_score, balanced_accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_probs(model, loader, device):\n",
        "    model.eval()\n",
        "    all_probs, all_preds, all_targets = [], [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model(xb)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        preds = probs.argmax(dim=1)\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(yb.cpu().numpy())\n",
        "    y_true = np.concatenate(all_targets)\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    y_proba = np.concatenate(all_probs)\n",
        "    return y_true, y_pred, y_proba\n",
        "\n",
        "# Evaluate on test set\n",
        "y_true, y_pred, y_proba = evaluate_with_probs(model, test_loader, device)\n",
        "\n",
        "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "print(f\"Primary metrics:\\n- Macro-F1: {macro_f1:.4f}\\n- Balanced Accuracy: {bal_acc:.4f}\")\n",
        "\n",
        "print(\"\\nPer-class metrics (precision/recall/F1):\")\n",
        "print(classification_report(y_true, y_pred, target_names=list(label_map.keys()), zero_division=0))\n",
        "\n",
        "# ROC-AUC (multi-class OvR)\n",
        "try:\n",
        "    y_true_1h = label_binarize(y_true, classes=list(range(len(label_map))))\n",
        "    roc_auc_macro = roc_auc_score(y_true_1h, y_proba, average='macro', multi_class='ovr')\n",
        "    roc_auc_micro = roc_auc_score(y_true_1h, y_proba, average='micro', multi_class='ovr')\n",
        "    print(f\"ROC-AUC (macro): {roc_auc_macro:.4f}\\nROC-AUC (micro): {roc_auc_micro:.4f}\")\n",
        "except Exception as e:\n",
        "    print(\"ROC-AUC could not be computed:\", e)"
      ],
      "metadata": {
        "id": "k57j2NNVNfCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 19: Error Analysis — High-confidence Misclassifications\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure y_true/y_pred/y_proba are available; else recompute\n",
        "y_true, y_pred, y_proba = (y_true if 'y_true' in globals() else None,\n",
        "                           y_pred if 'y_pred' in globals() else None,\n",
        "                           y_proba if 'y_proba' in globals() else None)\n",
        "if y_true is None or y_pred is None or y_proba is None:\n",
        "    y_true, y_pred, y_proba = evaluate_with_probs(model, test_loader, device)\n",
        "\n",
        "# Paths in test order\n",
        "paths_in_order = test_dataset.df[test_dataset.path_col].tolist()\n",
        "abs_paths = [str(Path(DATA_DIR) / p) for p in paths_in_order]\n",
        "\n",
        "mis_idx = np.where(y_pred != y_true)[0]\n",
        "if mis_idx.size == 0:\n",
        "    print(\"No misclassifications found on the test set.\")\n",
        "else:\n",
        "    top2_idx = np.argsort(-y_proba, axis=1)[:, :2]\n",
        "    top1_prob = y_proba[np.arange(len(y_proba)), top2_idx[:, 0]]\n",
        "    top2_prob = y_proba[np.arange(len(y_proba)), top2_idx[:, 1]]\n",
        "\n",
        "    inv_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    rows = []\n",
        "    for i in mis_idx:\n",
        "        rows.append({\n",
        "            'index': int(i),\n",
        "            'image_path': abs_paths[i] if i < len(abs_paths) else None,\n",
        "            'true_idx': int(y_true[i]),\n",
        "            'true_label': inv_label_map.get(int(y_true[i]), str(y_true[i])),\n",
        "            'pred_idx': int(top2_idx[i, 0]),\n",
        "            'pred_label': inv_label_map.get(int(top2_idx[i, 0]), str(top2_idx[i, 0])),\n",
        "            'pred_prob': float(top1_prob[i]),\n",
        "            'second_idx': int(top2_idx[i, 1]),\n",
        "            'second_label': inv_label_map.get(int(top2_idx[i, 1]), str(top2_idx[i, 1])),\n",
        "            'second_prob': float(top2_prob[i])\n",
        "        })\n",
        "\n",
        "    mis_df = pd.DataFrame(rows).sort_values(by='pred_prob', ascending=False).reset_index(drop=True)\n",
        "    display(mis_df.head(20))\n",
        "\n",
        "    out_dir = Path(PROJECT_ROOT) / 'results' / 'error_analysis'\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_csv = out_dir / 'misclassifications.csv'\n",
        "    mis_df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved misclassifications to {out_csv}\")"
      ],
      "metadata": {
        "id": "qLe-7fTCNkqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 20: Explainable AI (XAI) — Why and How\n",
        "\n",
        "Why XAI?\n",
        "- Trust & Adoption: Understand predictions.\n",
        "- Debugging & Improvement: Find weaknesses or biases.\n",
        "- Compliance & Ethics: Ensure fairness and explainability.\n",
        "\n",
        "Techniques in this notebook:\n",
        "- Grad-CAM (already implemented) and Grad-CAM++ (optional)\n",
        "- LIME (local explanations)\n",
        "- SHAP (Shapley values; optional and compute-heavy)\n",
        "- Attention visualization (for Transformers; not applicable here)"
      ],
      "metadata": {
        "id": "2H5fDrPgNoKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "ccPI39FjNm_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 21: LIME — Local Explanations for Individual Predictions\n",
        "try:\n",
        "    from lime import lime_image\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    def lime_predict(images: np.ndarray):\n",
        "        tensors = []\n",
        "        for img in images:\n",
        "            pil = Image.fromarray((img * 255).astype(np.uint8)) if img.max() <= 1.0 else Image.fromarray(img.astype(np.uint8))\n",
        "            pil = pil.convert('L')\n",
        "            t = val_transform(pil)\n",
        "            if t.shape[0] == 1:\n",
        "                t = t.repeat(3, 1, 1)\n",
        "            tensors.append(t)\n",
        "        batch = torch.stack(tensors, dim=0)\n",
        "        with torch.no_grad():\n",
        "            logits = model(batch.to(device))\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "        return probs\n",
        "\n",
        "    sample_idx = 0\n",
        "    img_path = Path(DATA_DIR) / test_dataset.df.iloc[sample_idx][test_dataset.path_col]\n",
        "    img_rgb = Image.open(img_path).convert('RGB')\n",
        "    img_np = np.asarray(img_rgb) / 255.0\n",
        "\n",
        "    explainer = lime_image.LimeImageExplainer()\n",
        "    explanation = explainer.explain_instance(\n",
        "        img_np,\n",
        "        classifier_fn=lime_predict,\n",
        "        top_labels=1,\n",
        "        hide_color=0,\n",
        "        num_samples=500\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pil_gray = Image.open(img_path).convert('L')\n",
        "        t = val_transform(pil_gray)\n",
        "        if t.shape[0] == 1:\n",
        "            t = t.repeat(3, 1, 1)\n",
        "        pred_idx = int(torch.softmax(model(t.unsqueeze(0).to(device)), dim=1).argmax().item())\n",
        "\n",
        "    lime_img, mask = explanation.get_image_and_mask(\n",
        "        label=pred_idx,\n",
        "        positive_only=True,\n",
        "        num_features=5,\n",
        "        hide_rest=False\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1); plt.imshow(img_rgb); plt.axis('off'); plt.title('Original')\n",
        "    plt.subplot(1, 3, 2); plt.imshow(mask, cmap='gray'); plt.axis('off'); plt.title('LIME Mask')\n",
        "    plt.subplot(1, 3, 3); plt.imshow(lime_img); plt.axis('off'); plt.title('LIME Overlay')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    print(\"LIME explanation generated.\")\n",
        "except Exception as e:\n",
        "    print(\"LIME explanation skipped. Install with: pip install lime. Error:\", e)"
      ],
      "metadata": {
        "id": "DrRi1h3sNq5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 20 (run): Install optional XAI packages (safe with fallbacks)\n",
        "import importlib, sys, subprocess\n",
        "\n",
        "pkgs = [\n",
        "    { 'pip': 'pytorch-grad-cam', 'import': 'pytorch_grad_cam', 'alt': 'git+https://github.com/jacobgil/pytorch-grad-cam@master' },\n",
        "    { 'pip': 'lime',              'import': 'lime',             'alt': None },\n",
        "    { 'pip': 'shap',              'import': 'shap',             'alt': None },\n",
        "]\n",
        "\n",
        "def ensure_package(pip_name: str, import_name: str, alt: str | None = None):\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        print(f\"✓ {pip_name} already available as '{import_name}'\")\n",
        "        return\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pip_name}...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools', 'wheel'])\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_name])\n",
        "            importlib.import_module(import_name)\n",
        "            print(f\"✓ Installed {pip_name}\")\n",
        "            return\n",
        "        except Exception as e1:\n",
        "            if alt:\n",
        "                print(f\"Primary install failed for {pip_name}. Trying fallback: {alt}\")\n",
        "                try:\n",
        "                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', alt])\n",
        "                    importlib.import_module(import_name)\n",
        "                    print(f\"✓ Installed via fallback: {alt}\")\n",
        "                    return\n",
        "                except Exception as e2:\n",
        "                    print(f\"✗ Failed to install {pip_name} via pip and fallback. Error: {e2}\")\n",
        "            else:\n",
        "                print(f\"✗ Failed to install {pip_name}. Error: {e1}\")\n",
        "\n",
        "for p in pkgs:\n",
        "    ensure_package(p['pip'], p['import'], p['alt'])\n",
        "\n",
        "print(\"Optional XAI packages installation attempt complete. Proceed to Steps 21–23; cells will skip gracefully if a package is still unavailable.\")"
      ],
      "metadata": {
        "id": "gUXm1tUMNuCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 22: Grad-CAM++ (optional via pytorch-grad-cam)\n",
        "try:\n",
        "    from pytorch_grad_cam import GradCAMPlusPlus\n",
        "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "\n",
        "    target_conv = None\n",
        "    if hasattr(model, 'features'):\n",
        "        for m in reversed(model.features):\n",
        "            if isinstance(m, torch.nn.Conv2d):\n",
        "                target_conv = m\n",
        "                break\n",
        "    else:\n",
        "        for m in reversed(list(model.features.modules())):\n",
        "            if isinstance(m, torch.nn.Conv2d):\n",
        "                target_conv = m\n",
        "                break\n",
        "    assert target_conv is not None\n",
        "\n",
        "    campp = GradCAMPlusPlus(model=model, target_layers=[target_conv]) # Removed use_cuda\n",
        "\n",
        "    img_path = Path(DATA_DIR) / val_dataset.df.iloc[0][val_dataset.path_col]\n",
        "    img_pil = Image.open(img_path).convert('RGB')\n",
        "    img_np = np.array(img_pil).astype(np.float32) / 255.0\n",
        "\n",
        "    img_gray = Image.open(img_path).convert('L')\n",
        "    inp = val_transform(img_gray)\n",
        "    if inp.shape[0] == 1:\n",
        "        inp = inp.repeat(3, 1, 1)\n",
        "    input_tensor = inp.unsqueeze(0)\n",
        "\n",
        "    grayscale_cam = campp(input_tensor=input_tensor.to(device))  # [1,H,W]\n",
        "    grayscale_cam = grayscale_cam[0]\n",
        "\n",
        "    visualization = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1); plt.imshow(img_np); plt.axis('off'); plt.title('Input')\n",
        "    plt.subplot(1,2,2); plt.imshow(visualization); plt.axis('off'); plt.title('Grad-CAM++')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    print(\"Grad-CAM++ visualization ready.\")\n",
        "except Exception as e:\n",
        "    print(\"Grad-CAM++ skipped. Install with: pip install pytorch-grad-cam. Error:\", e)"
      ],
      "metadata": {
        "id": "oYd6Zw9JNvzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 23: SHAP (optional) — DeepExplainer for CNNs\n",
        "try:\n",
        "    import shap\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Background for SHAP (small)\n",
        "    N_BG = 8\n",
        "    bg_tensors = []\n",
        "    for i in range(min(N_BG, len(val_dataset))):\n",
        "        p = Path(DATA_DIR) / val_dataset.df.iloc[i][val_dataset.path_col]\n",
        "        img_gray = Image.open(p).convert('L')\n",
        "        t = val_transform(img_gray)\n",
        "        if t.shape[0] == 1:\n",
        "            t = t.repeat(3, 1, 1)\n",
        "        bg_tensors.append(t)\n",
        "\n",
        "    if len(bg_tensors) < 1:\n",
        "         raise ValueError(\"Not enough background samples for SHAP.\")\n",
        "\n",
        "    background = torch.stack(bg_tensors, dim=0)\n",
        "\n",
        "    def f_predict(x: np.ndarray):\n",
        "        # Input to f_predict is expected to be 2D (n_samples, n_features)\n",
        "        # Reshape back to (n_samples, C, H, W)\n",
        "        x_reshaped = x.reshape(-1, 3, INPUT_SIZE, INPUT_SIZE) # Assuming 3 channels, 224x224\n",
        "        with torch.no_grad():\n",
        "            xb = torch.from_numpy(x_reshaped).float().to(device)\n",
        "            logits = model(xb)\n",
        "            probs = torch.softmax(logits, dim=1).detach().cpu().numpy()\n",
        "        return probs\n",
        "\n",
        "    idx = 0\n",
        "    p = Path(DATA_DIR) / val_dataset.df.iloc[idx][val_dataset.path_col]\n",
        "    img_gray = Image.open(p).convert('L')\n",
        "    x = val_transform(img_gray)\n",
        "    if x.shape[0] == 1:\n",
        "        x = x.repeat(3, 1, 1)\n",
        "    x = x.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # Flatten the input tensor for KernelExplainer\n",
        "    x_flat = x.cpu().numpy().reshape(1, -1)\n",
        "    background_flat = background.cpu().numpy().reshape(background.shape[0], -1)\n",
        "\n",
        "\n",
        "    try:\n",
        "        explainer = shap.DeepExplainer(model.to(device), background.to(device))\n",
        "        shap_values = explainer.shap_values(x.to(device))\n",
        "    except Exception:\n",
        "        print(\"Falling back to KernelExplainer (slower)...\")\n",
        "        explainer = shap.KernelExplainer(f_predict, background_flat[:min(50, background_flat.shape[0])]) # Use a subset of background if large\n",
        "        shap_values = explainer.shap_values(x_flat, nsamples=100)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_idx = int(torch.softmax(model(x.to(device)), dim=1).argmax().item())\n",
        "\n",
        "    try:\n",
        "        # Reshape SHAP values back to image dimensions for plotting\n",
        "        if isinstance(shap_values, list): # Output of multi-output models\n",
        "             shap_values_reshaped = [np.reshape(v, (INPUT_SIZE, INPUT_SIZE, 3)) for v in shap_values] # Assuming 3 channels\n",
        "        else: # Single output\n",
        "             shap_values_reshaped = np.reshape(shap_values, (INPUT_SIZE, INPUT_SIZE, 3)) # Assuming 3 channels\n",
        "\n",
        "        shap.image_plot(shap_values_reshaped[pred_idx] if isinstance(shap_values_reshaped, list) else shap_values_reshaped,\n",
        "                         np.transpose(x.cpu().numpy()[0], (1, 2, 0))) # Transpose (C, H, W) to (H, W, C)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"SHAP image plot skipped:\", e)\n",
        "\n",
        "    print(\"SHAP explanation generated for one validation sample.\")\n",
        "except Exception as e:\n",
        "    print(\"SHAP skipped. Install with: pip install shap. Error:\", e)"
      ],
      "metadata": {
        "id": "kaWAjTSnNxkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 24: Cross-validation (skeleton)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "k = 3\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "labels_series = train_df[train_dataset.label_col]\n",
        "\n",
        "print(f\"Preparing {k}-fold CV on training set (size={len(train_df)})...\")\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, labels_series), start=1):\n",
        "    print(f\"Fold {fold}: train={len(train_idx)}, val={len(val_idx)}\")\n",
        "\n",
        "# To fully run CV per fold:\n",
        "# 1) subset train_df/val_df to train_idx/val_idx\n",
        "# 2) create new Datasets/DataLoaders\n",
        "# 3) re-initialize model/optimizer\n",
        "# 4) train and record metrics, then aggregate"
      ],
      "metadata": {
        "id": "1DGH6pMLNzhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 25: Optimize for Deployment — Quantization + ONNX\n",
        "\n",
        "We’ll export two artifacts:\n",
        "- Dynamically-quantized PyTorch checkpoint (smaller and faster on CPU).\n",
        "- ONNX model (dynamic batch axis) for ONNX Runtime and cross-platform use.\n",
        "\n",
        "The next cell performs both and optionally verifies ONNX forward with onnxruntime if available."
      ],
      "metadata": {
        "id": "jOm7aZO0N1Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 25 (run): Export quantized PyTorch and ONNX models\n",
        "import torch\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "\n",
        "# 25a. Dynamic quantization (CPU inference)\n",
        "cpu_model = deepcopy(model).to('cpu').eval()\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    cpu_model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "quant_ckpt = {\n",
        "    'model_state_dict': quantized_model.state_dict(),\n",
        "    'model_config': {\n",
        "        'num_classes': num_classes,\n",
        "        'backbone': 'vgg16',\n",
        "        'input_size': INPUT_SIZE,\n",
        "        'pretrained': False,\n",
        "        'quantized': True,\n",
        "        'grayscale': True,\n",
        "        'channel_repeat': True\n",
        "    },\n",
        "    'label_map': label_map,\n",
        "}\n",
        "torch.save(quant_ckpt, 'cnn_transfer_learning_quantized.pth')\n",
        "print('✓ Saved dynamic-quantized model: cnn_transfer_learning_quantized.pth')\n",
        "\n",
        "# 25b. ONNX export\n",
        "dummy = torch.randn(1, 3, INPUT_SIZE, INPUT_SIZE, dtype=torch.float32)\n",
        "model_cpu = model.to('cpu').eval()\n",
        "onnx_path = 'cnn_transfer_learning.onnx'\n",
        "torch.onnx.export(\n",
        "    model_cpu, dummy, onnx_path,\n",
        "    input_names=['input'], output_names=['logits'],\n",
        "    dynamic_axes={'input': {0: 'batch'}, 'logits': {0: 'batch'}},\n",
        "    opset_version=13\n",
        ")\n",
        "print(f'✓ Exported ONNX: {onnx_path}')\n",
        "\n",
        "try:\n",
        "    import onnxruntime as ort\n",
        "    sess = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
        "    ort_out = sess.run(['logits'], { 'input': dummy.numpy() })[0]\n",
        "    print('ONNX forward ok. Output shape:', ort_out.shape)\n",
        "except Exception as e:\n",
        "    print('ONNXRuntime check skipped or failed:', e)\n",
        "\n",
        "print('Deployment artifacts ready.')"
      ],
      "metadata": {
        "id": "Bfzjoy-2N1ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brbijW0jN7Vt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}